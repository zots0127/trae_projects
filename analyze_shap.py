#!/usr/bin/env python3
"""
SHAPå¯è§£é‡Šæ€§åˆ†æå·¥å…· - åå¤„ç†è„šæœ¬
ç”¨äºåˆ†æå·²è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä¸ä¼šç ´ååŸæœ‰é¡¹ç›®

ä½¿ç”¨æ–¹æ³•:
    python analyze_shap.py Paper_0930_222051
    python analyze_shap.py Paper_0930_222051 --models xgboost lightgbm
    python analyze_shap.py Paper_0930_222051 --sample-size 200
"""

import sys
import os
from pathlib import Path
import argparse
import joblib
import pandas as pd
import numpy as np
import json
from datetime import datetime
import shap
import matplotlib.pyplot as plt
import warnings

warnings.filterwarnings('ignore')

# æ·»åŠ è·¯å¾„ï¼ˆç”¨äºå¯¼å…¥RDKitç›¸å…³ï¼‰
sys.path.insert(0, str(Path(__file__).parent.parent / 'ir2025'))

# SHAPåº“ç›´æ¥ä½¿ç”¨ï¼Œä¸ä¾èµ–é¡¹ç›®å†…éƒ¨æ¨¡å—
print("âœ… SHAPåˆ†æå·¥å…·å·²åŠ è½½")


class ModelShapAnalyzer:
    """å·²è®­ç»ƒæ¨¡å‹çš„SHAPåˆ†æå™¨"""

    def __init__(self, paper_dir):
        self.paper_dir = Path(paper_dir)
        self.models_dir = self.paper_dir / 'all_models' / 'automl_train'
        self.output_dir = self.paper_dir / 'shap_analysis'
        self.output_dir.mkdir(exist_ok=True)

        # KernelExplainer åŠ é€Ÿå‚æ•°ï¼ˆå¯åœ¨ main ä¸­è¦†ç›–ï¼‰
        self.kernel_k = 20           # èƒŒæ™¯èšç±»æ•°
        self.kernel_nsamples = 200   # æ¯ä¸ªè§£é‡Šçš„é‡‡æ ·ä¸Šé™
        self.kernel_max_samples = 40 # kernelç±»å‹çš„æœ€å¤§æ ·æœ¬æ•°

        # è¯»å–æ•°æ®
        self.load_data()

    def load_data(self):
        """åŠ è½½è®­ç»ƒæ•°æ®ç”¨äºSHAPèƒŒæ™¯"""
        print("\nğŸ“‚ åŠ è½½æ•°æ®...")

        # å°è¯•ä»å¤šä¸ªä½ç½®åŠ è½½æ•°æ®
        data_paths = [
            # ä¸è®ºæ–‡ç›®å½•åŒçº§/å†…éƒ¨çš„dataè·¯å¾„
            self.paper_dir / 'data' / 'Database_normalized.csv',
            Path('/Users/kanshan/IR/ir2025/data/Database_normalized.csv'),
            Path('../ir2025/data/Database_normalized.csv'),
            Path('data/Database_normalized.csv')
        ]

        for data_path in data_paths:
            if data_path.exists():
                self.df = pd.read_csv(data_path)
                print(f"  âœ… æ•°æ®åŠ è½½æˆåŠŸ: {data_path}")
                print(f"  ğŸ“Š æ•°æ®ç»´åº¦: {self.df.shape}")
                return

        raise FileNotFoundError("âŒ æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶")

    def extract_features(self, smiles_list):
        """æå–åˆ†å­ç‰¹å¾"""
        # ä¸è®­ç»ƒç®¡çº¿ä¿æŒä¸€è‡´ï¼ˆ1024-bit Morgan + 85ä¸ªæè¿°ç¬¦ï¼‰
        from core.feature_extractor import FeatureExtractor
        extractor = FeatureExtractor(feature_type="combined", morgan_bits=1024, morgan_radius=2)

        features_list = []
        for smiles in smiles_list:
            if smiles is None or (isinstance(smiles, float) and np.isnan(smiles)):
                continue
            feat = extractor.extract_from_smiles(smiles, feature_type="combined")
            features_list.append(feat)

        if not features_list:
            return np.array([])

        return np.vstack(features_list)

    def get_feature_names(self):
        from core.feature_extractor import DESCRIPTOR_NAMES
        fp_names = [f'Morgan_{i}' for i in range(1024)]
        desc_names = DESCRIPTOR_NAMES if DESCRIPTOR_NAMES else []
        return fp_names + desc_names

    def find_models(self, model_filter=None):
        """æŸ¥æ‰¾æ‰€æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print("\nğŸ” æœç´¢å·²è®­ç»ƒæ¨¡å‹...")

        models_info = []

        if not self.models_dir.exists():
            print(f"  âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {self.models_dir}")
            return models_info

        # éå†æ‰€æœ‰æ¨¡å‹ç›®å½•
        for model_dir in self.models_dir.iterdir():
            if not model_dir.is_dir():
                continue

            model_name = model_dir.name

            # è¿‡æ»¤æ¨¡å‹
            if model_filter and model_name not in model_filter:
                continue

            # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
            model_files_dir = model_dir / 'models'
            if not model_files_dir.exists():
                continue

            for model_file in model_files_dir.glob('*.joblib'):
                # è§£æç›®æ ‡åç§°
                filename = model_file.stem
                if 'Max_wavelength' in filename:
                    target = 'Max_wavelength(nm)'
                    target_clean = 'Wavelength'
                elif 'PLQY' in filename:
                    target = 'PLQY'
                    target_clean = 'PLQY'
                else:
                    continue

                models_info.append({
                    'model_name': model_name,
                    'model_path': model_file,
                    'target': target,
                    'target_clean': target_clean
                })

        print(f"  âœ… æ‰¾åˆ° {len(models_info)} ä¸ªæ¨¡å‹")
        for info in models_info:
            print(f"     - {info['model_name']:20s} | {info['target_clean']}")

        return models_info

    def _resolve_predictor(self, loaded_model):
        """è§£ææ¨¡å‹å¯¹è±¡ï¼Œè¿”å›å¯ç”¨äºSHAPçš„é¢„æµ‹å‡½æ•°ä¸åº•å±‚æ¨¡å‹å¯¹è±¡"""
        # ç›´æ¥ä½¿ç”¨å¯é¢„æµ‹çš„æ¨¡å‹
        if hasattr(loaded_model, 'predict'):
            return loaded_model.predict, loaded_model

        # å­—å…¸å°è£…ï¼ˆåŒ…å«scaler/target_scalerç­‰ï¼‰
        if isinstance(loaded_model, dict):
            inner = loaded_model.get('model', None)
            scaler = loaded_model.get('scaler', None)
            target_scaler = loaded_model.get('target_scaler', None)

            if inner is None:
                raise ValueError('å­—å…¸æ¨¡å‹ç¼ºå°‘ "model" é”®')

            # ç»„è£…å¸¦é¢„å¤„ç†çš„é¢„æµ‹å‡½æ•°
            def predict_fn(X):
                X_in = X
                if scaler is not None:
                    X_in = scaler.transform(X_in)
                y_pred = inner.predict(X_in)
                if target_scaler is not None:
                    y_pred = target_scaler.inverse_transform(np.array(y_pred).reshape(-1, 1)).ravel()
                return y_pred

            return predict_fn, inner

        # å…¶ä»–åŒ…è£…ç±»å‹ï¼ˆå¦‚pipelineï¼‰
        try:
            predict = getattr(loaded_model, 'predict')
            return predict, loaded_model
        except Exception:
            raise AttributeError('æ— æ³•è§£æé¢„æµ‹å‡½æ•°ï¼Œæ¨¡å‹å¯¹è±¡ä¸æ”¯æŒ predict')

    def analyze_model(self, model_info, sample_size=100):
        """åˆ†æå•ä¸ªæ¨¡å‹"""
        model_name = model_info['model_name']
        target = model_info['target']
        target_clean = model_info['target_clean']

        print(f"\n{'='*70}")
        print(f"ğŸ”¬ åˆ†ææ¨¡å‹: {model_name} - {target_clean}")
        print(f"{'='*70}")

        # åŠ è½½æ¨¡å‹
        try:
            model = joblib.load(model_info['model_path'])
            print(f"  âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"  âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return None

        # å‡†å¤‡æ•°æ®
        print(f"  ğŸ“Š å‡†å¤‡ç‰¹å¾æ•°æ®...")
        valid_df = self.df.dropna(subset=[target])
        smiles_cols = ['L1', 'L2', 'L3']
        print(f"     æ­£åœ¨æå–åˆ†å­ç‰¹å¾...")
        from core.feature_extractor import FeatureExtractor
        extractor = FeatureExtractor(feature_type="combined", morgan_bits=1024, morgan_radius=2)
        X = extractor.extract_from_dataframe(valid_df, smiles_columns=smiles_cols, feature_type="combined")

        if len(X) == 0:
            print(f"  âŒ ç‰¹å¾æå–å¤±è´¥")
            return None

        print(f"     âœ… ç‰¹å¾ç»´åº¦: {X.shape}")

        # é‡‡æ ·ï¼ˆSHAPè®¡ç®—è¾ƒæ…¢ï¼‰
        if len(X) > sample_size:
            print(f"  âš¡ é‡‡æ · {sample_size} ä¸ªæ ·æœ¬è¿›è¡ŒSHAPåˆ†æ")
            sample_idx = np.random.choice(len(X), sample_size, replace=False)
            X_sample = X[sample_idx]
        else:
            X_sample = X

        # åˆ›å»ºSHAPåˆ†æå™¨
        print(f"  ğŸ§® è®¡ç®—SHAPå€¼...")
        try:
            # ç¡®å®šæ¨¡å‹ç±»å‹
            model_type_map = {
                'xgboost': 'tree',
                'lightgbm': 'tree',
                'catboost': 'tree',
                'random_forest': 'tree',
                'gradient_boosting': 'tree',
                'decision_tree': 'tree',
                'adaboost': 'tree',
                'ridge': 'linear',
                'lasso': 'linear',
                'elastic_net': 'linear'
            }

            shap_model_type = model_type_map.get(model_name, 'kernel')

            # åˆ›å»ºexplainer
            if shap_model_type == 'tree':
                explainer = shap.TreeExplainer(model)
            elif shap_model_type == 'linear':
                explainer = shap.LinearExplainer(model, X_sample)
            else:
                # Kernelç±»å‹ï¼šå¼ºåˆ¶å¿«é€Ÿæ¨¡å¼ï¼ˆKMeansèƒŒæ™¯ + é™åˆ¶æ ·æœ¬æ•° + é™åˆ¶nsamplesï¼‰
                print(f"     âš¡ Kernelå¿«é€Ÿæ¨¡å¼ï¼škmeans={self.kernel_k}, nsamples={self.kernel_nsamples}")
                predict_fn, _ = self._resolve_predictor(model)
                # é™åˆ¶æ ·æœ¬é‡
                if len(X_sample) > self.kernel_max_samples:
                    sample_idx = np.random.choice(len(X_sample), self.kernel_max_samples, replace=False)
                    X_sample = X_sample[sample_idx]
                    print(f"     âš ï¸ å·²å°†æ ·æœ¬æ•°é™åˆ¶ä¸º {len(X_sample)} ç”¨äºKernelè§£é‡Š")
                # ä½¿ç”¨kmeansæ‘˜è¦ä½œä¸ºèƒŒæ™¯
                try:
                    background = shap.kmeans(X_sample, self.kernel_k)
                except Exception:
                    # å›é€€åˆ°éšæœºé‡‡æ ·
                    k = min(self.kernel_k, len(X_sample))
                    background = shap.sample(X_sample, k)
                explainer = shap.KernelExplainer(predict_fn, background)

            # è®¡ç®—SHAPå€¼
            # Kernelåˆ†æ”¯å·²åœ¨ä¸Šæ–¹è¿›å…¥ï¼Œç›´æ¥é™åˆ¶nsamplesæé€Ÿï¼›å…¶å®ƒè§£é‡Šå™¨æ­£å¸¸è®¡ç®—
            if shap_model_type == 'kernel':
                shap_values = explainer.shap_values(X_sample, nsamples=self.kernel_nsamples)
            else:
                shap_values = explainer.shap_values(X_sample)

            print(f"     âœ… SHAPå€¼è®¡ç®—å®Œæˆ")

        except Exception as e:
            print(f"  âŒ SHAPåˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

        # åˆ†æç‰¹å¾é‡è¦æ€§
        feature_names = self.get_feature_names()

        # ç¡®ä¿ç‰¹å¾ç»´åº¦åŒ¹é…
        if len(feature_names) > X_sample.shape[1]:
            feature_names = feature_names[:X_sample.shape[1]]
        elif len(feature_names) < X_sample.shape[1]:
            feature_names = feature_names + [f'Feature_{i}' for i in range(len(feature_names), X_sample.shape[1])]

        # è®¡ç®—ç‰¹å¾é‡è¦æ€§
        importance = np.abs(shap_values).mean(axis=0)
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)

        # ä¿å­˜ç»“æœ
        output_subdir = self.output_dir / model_name / target_clean
        output_subdir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜ç‰¹å¾é‡è¦æ€§CSV
        csv_path = output_subdir / 'shap_feature_importance.csv'
        importance_df.to_csv(csv_path, index=False)
        print(f"  ğŸ’¾ ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜: {csv_path}")

        # ä¿å­˜Top 30ç‰¹å¾
        top30 = importance_df.head(30)

        # åˆ›å»ºå¯è§†åŒ–
        print(f"  ğŸ“Š ç”Ÿæˆå¯è§†åŒ–...")

        # 1. ç‰¹å¾é‡è¦æ€§æ¡å½¢å›¾
        plt.figure(figsize=(12, 8))
        plt.barh(range(len(top30)), top30['importance'].values)
        plt.yticks(range(len(top30)), top30['feature'].values)
        plt.xlabel('Mean |SHAP value|')
        plt.title(f'Top 30 Feature Importance - {model_name} ({target_clean})')
        plt.gca().invert_yaxis()
        plt.tight_layout()

        fig_path = output_subdir / 'feature_importance_bar.png'
        plt.savefig(fig_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"     âœ… æ¡å½¢å›¾: {fig_path.name}")

        # 2. SHAP summary plot
        try:
            plt.figure(figsize=(12, 8))
            shap.summary_plot(
                shap_values,
                X_sample,
                feature_names=feature_names,
                max_display=30,
                show=False
            )
            summary_path = output_subdir / 'shap_summary_plot.png'
            plt.savefig(summary_path, dpi=150, bbox_inches='tight')
            plt.close()
            print(f"     âœ… Summaryå›¾: {summary_path.name}")
        except Exception as e:
            print(f"     âš ï¸ Summaryå›¾ç”Ÿæˆå¤±è´¥: {e}")

        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'model_name': model_name,
            'target': target_clean,
            'sample_size': len(X_sample),
            'n_features': X_sample.shape[1],
            'top_10_features': top30.head(10).to_dict('records'),
            'analysis_time': datetime.now().isoformat()
        }

        json_path = output_subdir / 'shap_metadata.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

        print(f"  âœ… åˆ†æå®Œæˆ")

        return {
            'model': model_name,
            'target': target_clean,
            'top_features': top30.head(10)
        }

    def generate_summary_report(self, results):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        print(f"\n{'='*70}")
        print(f"ğŸ“ ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š")
        print(f"{'='*70}")

        html = f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>SHAPå¯è§£é‡Šæ€§åˆ†ææŠ¥å‘Š</title>
    <style>
        body {{
            font-family: 'Segoe UI', Arial, sans-serif;
            margin: 40px;
            background: #f5f5f5;
        }}
        .container {{
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        h1 {{
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }}
        h2 {{
            color: #34495e;
            margin-top: 30px;
            padding-left: 10px;
            border-left: 4px solid #3498db;
        }}
        .model-section {{
            margin: 30px 0;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 5px;
            border-left: 4px solid #3498db;
        }}
        .model-title {{
            font-size: 20px;
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 15px;
        }}
        .feature-table {{
            width: 100%;
            border-collapse: collapse;
            margin-top: 15px;
        }}
        .feature-table th {{
            background: #3498db;
            color: white;
            padding: 12px;
            text-align: left;
        }}
        .feature-table td {{
            padding: 10px;
            border-bottom: 1px solid #ddd;
        }}
        .feature-table tr:hover {{
            background: #f0f0f0;
        }}
        .images {{
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin-top: 20px;
        }}
        .images img {{
            width: 100%;
            border: 1px solid #ddd;
            border-radius: 5px;
        }}
        .summary-box {{
            background: #e8f4f8;
            padding: 20px;
            border-radius: 5px;
            margin: 20px 0;
        }}
        .file-link {{
            color: #3498db;
            text-decoration: none;
            font-weight: 500;
        }}
        .file-link:hover {{
            text-decoration: underline;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ¯ SHAPå¯è§£é‡Šæ€§åˆ†ææŠ¥å‘Š</h1>

        <div class="summary-box">
            <p><strong>ğŸ“… åˆ†ææ—¶é—´:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            <p><strong>ğŸ“‚ é¡¹ç›®ç›®å½•:</strong> {self.paper_dir.name}</p>
            <p><strong>ğŸ”¬ åˆ†ææ¨¡å‹æ•°:</strong> {len(results)}</p>
            <p><strong>ğŸ’¡ è¯´æ˜:</strong> SHAPå€¼è¡¨ç¤ºæ¯ä¸ªç‰¹å¾å¯¹æ¨¡å‹é¢„æµ‹çš„è´¡çŒ®åº¦ã€‚å€¼è¶Šå¤§è¡¨ç¤ºè¯¥ç‰¹å¾å¯¹é¢„æµ‹ç»“æœå½±å“è¶Šå¤§ã€‚</p>
        </div>

        <h2>ğŸ“Š åˆ†æç»“æœ</h2>
"""

        for result in results:
            if result is None:
                continue

            model = result['model']
            target = result['target']
            top_features = result['top_features']

            html += f"""
        <div class="model-section">
            <div class="model-title">ğŸ”¹ {model} - {target}</div>

            <h3>Top 10 é‡è¦ç‰¹å¾</h3>
            <table class="feature-table">
                <thead>
                    <tr>
                        <th>æ’å</th>
                        <th>ç‰¹å¾åç§°</th>
                        <th>é‡è¦æ€§ (Mean |SHAP|)</th>
                    </tr>
                </thead>
                <tbody>
"""

            for idx, row in enumerate(top_features.iterrows(), 1):
                feature = row[1]['feature']
                importance = row[1]['importance']
                html += f"""
                    <tr>
                        <td>{idx}</td>
                        <td>{feature}</td>
                        <td>{importance:.6f}</td>
                    </tr>
"""

            html += """
                </tbody>
            </table>

            <h3>å¯è§†åŒ–ç»“æœ</h3>
            <div class="images">
"""

            # æ·»åŠ å›¾ç‰‡é“¾æ¥
            img_dir = f"{model}/{target}"
            html += f"""
                <div>
                    <p><strong>ç‰¹å¾é‡è¦æ€§æ¡å½¢å›¾</strong></p>
                    <a href="{img_dir}/feature_importance_bar.png" target="_blank">
                        <img src="{img_dir}/feature_importance_bar.png" alt="Feature Importance">
                    </a>
                </div>
                <div>
                    <p><strong>SHAP Summary Plot</strong></p>
                    <a href="{img_dir}/shap_summary_plot.png" target="_blank">
                        <img src="{img_dir}/shap_summary_plot.png" alt="SHAP Summary">
                    </a>
                </div>
"""

            html += """
            </div>

            <p style="margin-top: 15px;">
                ğŸ“„ <a class="file-link" href="{}/{}/shap_feature_importance.csv">ä¸‹è½½å®Œæ•´ç‰¹å¾é‡è¦æ€§æ•°æ® (CSV)</a> |
                ğŸ“„ <a class="file-link" href="{}/{}/shap_metadata.json">å…ƒæ•°æ® (JSON)</a>
            </p>
        </div>
""".format(model, target, model, target)

        html += """
        <div class="summary-box" style="margin-top: 40px;">
            <h3>ğŸ“– å¦‚ä½•ä½¿ç”¨è¿™äº›ç»“æœ</h3>
            <ol>
                <li><strong>è¯†åˆ«å…³é”®ç‰¹å¾:</strong> Topç‰¹å¾è¡¨æ˜å“ªäº›åˆ†å­æ€§è´¨å¯¹é¢„æµ‹æœ€é‡è¦</li>
                <li><strong>æŒ‡å¯¼åˆ†å­è®¾è®¡:</strong> å…³æ³¨é‡è¦ç‰¹å¾æ¥ä¼˜åŒ–åˆ†å­ç»“æ„</li>
                <li><strong>æ¨¡å‹è¯Šæ–­:</strong> æ£€æŸ¥æ¨¡å‹æ˜¯å¦å…³æ³¨åˆç†çš„åŒ–å­¦ç‰¹å¾</li>
                <li><strong>è®ºæ–‡å†™ä½œ:</strong> åœ¨è®¨è®ºéƒ¨åˆ†è§£é‡Šæ¨¡å‹çš„é¢„æµ‹ä¾æ®</li>
            </ol>
        </div>
    </div>
</body>
</html>
"""

        report_path = self.output_dir / 'shap_report.html'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(html)

        print(f"  âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        print(f"  ğŸŒ ç”¨æµè§ˆå™¨æ‰“å¼€æŸ¥çœ‹: file://{report_path.absolute()}")

        return report_path


def main():
    parser = argparse.ArgumentParser(
        description='SHAPå¯è§£é‡Šæ€§åˆ†æå·¥å…· - åˆ†æå·²è®­ç»ƒçš„æ¨¡å‹',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
    # åˆ†ææ‰€æœ‰æ¨¡å‹
    python analyze_shap.py Paper_0930_222051

    # åªåˆ†æXGBoostå’ŒLightGBM
    python analyze_shap.py Paper_0930_222051 --models xgboost lightgbm

    # ä½¿ç”¨æ›´å¤§çš„æ ·æœ¬é‡
    python analyze_shap.py Paper_0930_222051 --sample-size 200
        """
    )

    parser.add_argument('paper_dir', help='è®ºæ–‡è¾“å‡ºç›®å½• (å¦‚: Paper_0930_222051)')
    parser.add_argument('--models', nargs='+', help='æŒ‡å®šè¦åˆ†æçš„æ¨¡å‹ (å¦‚: xgboost lightgbm)')
    parser.add_argument('--sample-size', type=int, default=100, help='SHAPåˆ†æçš„æ ·æœ¬æ•°é‡ (é»˜è®¤: 100)')
    parser.add_argument('--kernel-k', type=int, default=20, help='KernelExplainerçš„èƒŒæ™¯kmeansèšç±»æ•° (é»˜è®¤: 20)')
    parser.add_argument('--kernel-nsamples', type=int, default=200, help='KernelExplaineræ¯æ¬¡è§£é‡Šçš„é‡‡æ ·æ¬¡æ•°ä¸Šé™ (é»˜è®¤: 200)')
    parser.add_argument('--kernel-max-samples', type=int, default=40, help='Kernelæ¨¡å‹å‚ä¸è§£é‡Šçš„æœ€å¤§æ ·æœ¬æ•° (é»˜è®¤: 40)')

    args = parser.parse_args()

    print("="*70)
    print("ğŸ¯ SHAPå¯è§£é‡Šæ€§åˆ†æå·¥å…·")
    print("="*70)

    # åˆ›å»ºåˆ†æå™¨
    analyzer = ModelShapAnalyzer(args.paper_dir)
    # è¦†ç›–Kernelå¿«é€Ÿå‚æ•°
    analyzer.kernel_k = max(5, int(args.kernel_k))
    analyzer.kernel_nsamples = max(50, int(args.kernel_nsamples))
    analyzer.kernel_max_samples = max(10, int(args.kernel_max_samples))

    # æŸ¥æ‰¾æ¨¡å‹
    models = analyzer.find_models(model_filter=args.models)

    if not models:
        print("\nâŒ æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹æ–‡ä»¶")
        return 1

    # åˆ†ææ¯ä¸ªæ¨¡å‹
    results = []
    for model_info in models:
        result = analyzer.analyze_model(model_info, sample_size=args.sample_size)
        results.append(result)

    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    analyzer.generate_summary_report(results)

    print("\n" + "="*70)
    print("âœ… æ‰€æœ‰åˆ†æå®Œæˆ!")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {analyzer.output_dir}")
    print("="*70)

    return 0


if __name__ == '__main__':
    sys.exit(main())
