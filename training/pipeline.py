#!/usr/bin/env python3
"""
åŸºäºé…ç½®çš„è®­ç»ƒç®¡é“
ç±»ä¼¼YOLOçš„ä¸€é”®è®­ç»ƒç³»ç»Ÿ
"""

import os
import sys
import argparse
import warnings
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Optional, Dict, List, Tuple
import traceback
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from config.system import (
    ExperimentConfig, ConfigManager, ConfigValidator,
    BatchExperimentConfig, load_config
)
from core.feature_extractor import FeatureExtractor
from models.base import ModelFactory, evaluate_model, generate_model_filename
from training.logger import TrainingLogger
from sklearn.model_selection import KFold
import joblib
from utils.timing import TimingTracker
from utils.file_feature_cache import FileFeatureCache

warnings.filterwarnings('ignore')


# ========================================
#           è®­ç»ƒç®¡é“
# ========================================

class TrainingPipeline:
    """åŸºäºé…ç½®çš„è®­ç»ƒç®¡é“"""
    
    def __init__(self, config: ExperimentConfig):
        """
        åˆå§‹åŒ–è®­ç»ƒç®¡é“
        
        Args:
            config: å®éªŒé…ç½®
        """
        self.config = config
        self.logger = None
        self.data = None
        self.features = None
        self.targets = None
        
        # éªŒè¯é…ç½®
        if not ConfigValidator.validate_all(config):
            raise ValueError("é…ç½®éªŒè¯å¤±è´¥")
        
        print("\n" + "="*60)
        print(f"ğŸš€ è®­ç»ƒç®¡é“åˆå§‹åŒ–: {config.name}")
        print("="*60)
        print(f"æ¨¡å‹: {config.model.model_type}")
        print(f"ç‰¹å¾: {config.feature.feature_type}")
        print(f"äº¤å‰éªŒè¯: {config.training.n_folds}æŠ˜")
        # åˆå§‹åŒ–ç»†ç²’åº¦è®¡æ—¶å™¨
        self.timing = TimingTracker()
        
    def load_data(self, target_col: str = None) -> pd.DataFrame:
        """
        åŠ è½½æ•°æ®
        
        Args:
            target_col: å¦‚æœæŒ‡å®šï¼Œåªä¸ºè¯¥ç›®æ ‡åˆ—è¿‡æ»¤æ•°æ®ï¼›å¦åˆ™åŠ è½½æ‰€æœ‰æ•°æ®
        """
        if target_col is None:
            # å…¼å®¹æ€§ï¼šå¦‚æœæ²¡æœ‰æŒ‡å®šç›®æ ‡ï¼ŒåŠ è½½æ‰€æœ‰æ•°æ®ï¼ˆç”¨äºåˆå§‹æ£€æŸ¥ï¼‰
            print(f"\nğŸ“Š åŠ è½½æ•°æ®: {self.config.data.data_path}")
            with self.timing.measure('data_load_train'):
                df = pd.read_csv(self.config.data.data_path)
            print(f"   åŸå§‹æ•°æ®: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
            
            # æ£€æŸ¥å¯ç”¨çš„ç›®æ ‡åˆ—
            available_targets = []
            target_stats = {}
            for target in self.config.data.target_columns:
                if target in df.columns:
                    available_targets.append(target)
                    n_valid = df[target].notna().sum()
                    target_stats[target] = n_valid
                    print(f"   {target}: {n_valid} ä¸ªæœ‰æ•ˆå€¼")
            
            if not available_targets:
                raise ValueError(f"æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç›®æ ‡åˆ—: {self.config.data.target_columns}")
            
            # æ ¹æ®å¤šç›®æ ‡ç­–ç•¥æ˜¾ç¤ºæ•°æ®é€‰æ‹©ä¿¡æ¯
            if not hasattr(self.config.data, 'multi_target_strategy'):
                self.config.data.multi_target_strategy = 'independent'
            
            if self.config.data.multi_target_strategy == 'intersection':
                # è®¡ç®—äº¤é›†æ•°æ®é‡
                valid_mask = pd.Series([True] * len(df))
                for target in available_targets:
                    valid_mask &= df[target].notna()
                n_intersection = valid_mask.sum()
                print(f"\n   ğŸ“Š å¤šç›®æ ‡ç­–ç•¥: äº¤é›†æ¨¡å¼")
                print(f"      æ‰€æœ‰ç›®æ ‡éƒ½æœ‰å€¼çš„æ•°æ®: {n_intersection} è¡Œ")
                print(f"      æ•°æ®åˆ©ç”¨ç‡: {n_intersection/len(df)*100:.1f}%")
            elif self.config.data.multi_target_strategy == 'independent':
                print(f"\n   ğŸ“Š å¤šç›®æ ‡ç­–ç•¥: ç‹¬ç«‹æ¨¡å¼")
                print(f"      æ¯ä¸ªç›®æ ‡ç‹¬ç«‹ä½¿ç”¨å…¶æœ‰æ•ˆæ•°æ®")
            elif self.config.data.multi_target_strategy == 'union':
                print(f"\n   ğŸ“Š å¤šç›®æ ‡ç­–ç•¥: å¹¶é›†æ¨¡å¼")
                print(f"      ä½¿ç”¨æ‰€æœ‰æ•°æ®ï¼Œç¼ºå¤±å€¼å°†è¢«å¡«å……")
            
            self.available_targets = available_targets
            self.target_stats = target_stats
            self.raw_data = df  # ä¿å­˜åŸå§‹æ•°æ®
            return df
        else:
            # ä¸ºç‰¹å®šç›®æ ‡åŠ è½½å’Œè¿‡æ»¤æ•°æ®
            if not hasattr(self, 'raw_data'):
                with self.timing.measure('data_load_train'):
                    df = pd.read_csv(self.config.data.data_path)
                self.raw_data = df
            else:
                df = self.raw_data.copy()
            
            # å¤„ç†ç¼ºå¤±å€¼
            if not hasattr(self.config.data, 'nan_handling'):
                self.config.data.nan_handling = 'skip'  # é»˜è®¤å€¼
            if not hasattr(self.config.data, 'multi_target_strategy'):
                self.config.data.multi_target_strategy = 'independent'  # é»˜è®¤å€¼
            
            # æ ¹æ®å¤šç›®æ ‡ç­–ç•¥å’Œç¼ºå¤±å€¼å¤„ç†ç­–ç•¥å¤„ç†æ•°æ®
            if self.config.data.multi_target_strategy == 'intersection':
                # äº¤é›†æ¨¡å¼ï¼šåªä½¿ç”¨æ‰€æœ‰ç›®æ ‡éƒ½æœ‰å€¼çš„æ•°æ®
                print(f"\n   ğŸ“Œ ä½¿ç”¨äº¤é›†æ¨¡å¼å¤„ç† {target_col}")
                valid_mask = pd.Series([True] * len(df))
                for target in self.config.data.target_columns:
                    if target in df.columns:
                        valid_mask &= df[target].notna()
                
                # æ£€æŸ¥SMILESåˆ—
                if self.config.feature.feature_type in ['morgan', 'descriptors', 'combined']:
                    for col in self.config.data.smiles_columns:
                        if col in df.columns:
                            valid_mask &= df[col].notna()
                
                df_valid = df[valid_mask].copy()
                n_dropped = len(df) - len(df_valid)
                print(f"   {target_col} çš„æœ‰æ•ˆæ•°æ®: {len(df_valid)} è¡Œ (äº¤é›†æ¨¡å¼)")
                # ç›´æ¥è®¾ç½®æ•°æ®å¹¶è¿”å›
                self.data = df_valid
                return df_valid
                
            elif self.config.data.multi_target_strategy == 'union':
                # å¹¶é›†æ¨¡å¼ï¼šä½¿ç”¨æ‰€æœ‰æ•°æ®ï¼Œé…åˆnan_handlingç­–ç•¥
                print(f"\n   ğŸ“Œ ä½¿ç”¨å¹¶é›†æ¨¡å¼å¤„ç† {target_col}")
                df_valid = df.copy()
                
                # æ ¹æ®nan_handlingç­–ç•¥å¡«å……ç¼ºå¤±å€¼
                if self.config.data.nan_handling != 'skip':
                    # å¤„ç†ç›®æ ‡åˆ—ç¼ºå¤±å€¼
                    if target_col in df_valid.columns:
                        n_missing = df_valid[target_col].isna().sum()
                        if n_missing > 0:
                            if self.config.data.nan_handling == 'mean':
                                mean_val = df_valid[target_col].mean()
                                df_valid[target_col].fillna(mean_val, inplace=True)
                                print(f"   âœ… ä½¿ç”¨å‡å€¼ {mean_val:.4f} å¡«å……äº† {n_missing} ä¸ªç¼ºå¤±å€¼")
                            elif self.config.data.nan_handling == 'median':
                                median_val = df_valid[target_col].median()
                                df_valid[target_col].fillna(median_val, inplace=True)
                                print(f"   âœ… ä½¿ç”¨ä¸­ä½æ•° {median_val:.4f} å¡«å……äº† {n_missing} ä¸ªç¼ºå¤±å€¼")
                            elif self.config.data.nan_handling == 'zero':
                                df_valid[target_col].fillna(0, inplace=True)
                                print(f"   âœ… ä½¿ç”¨0å¡«å……äº† {n_missing} ä¸ªç¼ºå¤±å€¼")
                
                # SMILESç¼ºå¤±ä»éœ€è·³è¿‡
                if self.config.feature.feature_type in ['morgan', 'descriptors', 'combined']:
                    for col in self.config.data.smiles_columns:
                        if col in df_valid.columns:
                            mask = df_valid[col].notna()
                            n_missing = (~mask).sum()
                            if n_missing > 0:
                                df_valid = df_valid[mask]
                                print(f"   âš ï¸ è·³è¿‡äº† {n_missing} è¡ŒSMILESç¼ºå¤±çš„æ•°æ®")
                
                print(f"   {target_col} çš„æœ‰æ•ˆæ•°æ®: {len(df_valid)} è¡Œ (å¹¶é›†æ¨¡å¼)")
                # ç›´æ¥è®¾ç½®æ•°æ®å¹¶è¿”å›
                self.data = df_valid
                return df_valid
                
            elif self.config.data.multi_target_strategy == 'independent':
                # ç‹¬ç«‹æ¨¡å¼ï¼šæ¯ä¸ªç›®æ ‡ç‹¬ç«‹å¤„ç†ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
                pass
                
            # æ ¹æ®ä¸åŒç­–ç•¥å¤„ç†ç¼ºå¤±å€¼ï¼ˆç‹¬ç«‹æ¨¡å¼çš„åŸæœ‰é€»è¾‘ï¼‰
            if self.config.data.multi_target_strategy == 'independent' and self.config.data.nan_handling == 'skip':
                # ç­›é€‰æœ‰æ•ˆæ•°æ®ï¼šåªæ£€æŸ¥å½“å‰ç›®æ ‡åˆ—å’ŒSMILESåˆ—
                valid_mask = pd.Series([True] * len(df))
                
                # æ£€æŸ¥ç›®æ ‡åˆ—
                if target_col in df.columns:
                    valid_mask &= df[target_col].notna()
                else:
                    raise ValueError(f"ç›®æ ‡åˆ—ä¸å­˜åœ¨: {target_col}")
                
                # æ£€æŸ¥SMILESåˆ—ï¼ˆä»…å½“ä½¿ç”¨åˆ†å­ç‰¹å¾æ—¶ï¼‰
                if self.config.feature.feature_type in ['morgan', 'descriptors', 'combined']:
                    for col in self.config.data.smiles_columns:
                        if col in df.columns:
                            valid_mask &= df[col].notna()
                        else:
                            print(f"   âš ï¸ SMILESåˆ—ä¸å­˜åœ¨: {col}")
                elif self.config.feature.feature_type == 'tabular':
                    # å¯¹äºè¡¨æ ¼æ•°æ®ï¼Œä¸éœ€è¦SMILESåˆ—
                    pass
                
                df_valid = df[valid_mask].copy()
                n_dropped = len(df) - len(df_valid)
                if n_dropped > 0:
                    print(f"   {target_col} çš„æœ‰æ•ˆæ•°æ®: {len(df_valid)} è¡Œ (è·³è¿‡äº† {n_dropped} è¡Œå«ç¼ºå¤±å€¼çš„æ•°æ®)")
                else:
                    print(f"   {target_col} çš„æœ‰æ•ˆæ•°æ®: {len(df_valid)} è¡Œ")
                    
            else:
                # å…¶ä»–ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥
                df_valid = df.copy()
                
                # å¤„ç†ç›®æ ‡åˆ—ç¼ºå¤±å€¼
                if target_col in df_valid.columns:
                    n_missing = df_valid[target_col].isna().sum()
                    if n_missing > 0:
                        if self.config.data.nan_handling == 'mean':
                            mean_val = df_valid[target_col].mean()
                            df_valid[target_col].fillna(mean_val, inplace=True)
                            print(f"   âœ… ä½¿ç”¨å‡å€¼ {mean_val:.4f} å¡«å……äº† {n_missing} ä¸ªç›®æ ‡ç¼ºå¤±å€¼")
                        elif self.config.data.nan_handling == 'median':
                            median_val = df_valid[target_col].median()
                            df_valid[target_col].fillna(median_val, inplace=True)
                            print(f"   âœ… ä½¿ç”¨ä¸­ä½æ•° {median_val:.4f} å¡«å……äº† {n_missing} ä¸ªç›®æ ‡ç¼ºå¤±å€¼")
                        elif self.config.data.nan_handling == 'zero':
                            df_valid[target_col].fillna(0, inplace=True)
                            print(f"   âœ… ä½¿ç”¨0å¡«å……äº† {n_missing} ä¸ªç›®æ ‡ç¼ºå¤±å€¼")
                        elif self.config.data.nan_handling == 'forward':
                            df_valid[target_col].fillna(method='ffill', inplace=True)
                            df_valid[target_col].fillna(method='bfill', inplace=True)
                            print(f"   âœ… ä½¿ç”¨å‰å‘å¡«å……å¤„ç†äº† {n_missing} ä¸ªç›®æ ‡ç¼ºå¤±å€¼")
                        elif self.config.data.nan_handling == 'interpolate':
                            df_valid[target_col] = df_valid[target_col].interpolate()
                            df_valid[target_col].fillna(method='bfill', inplace=True)
                            df_valid[target_col].fillna(method='ffill', inplace=True)
                            print(f"   âœ… ä½¿ç”¨æ’å€¼å¤„ç†äº† {n_missing} ä¸ªç›®æ ‡ç¼ºå¤±å€¼")
                
                # SMILESåˆ—ç¼ºå¤±å¿…é¡»è·³è¿‡
                if self.config.feature.feature_type in ['morgan', 'descriptors', 'combined']:
                    for col in self.config.data.smiles_columns:
                        if col in df_valid.columns:
                            mask = df_valid[col].notna()
                            n_missing = (~mask).sum()
                            if n_missing > 0:
                                df_valid = df_valid[mask]
                                print(f"   âš ï¸ è·³è¿‡äº† {n_missing} è¡ŒSMILESç¼ºå¤±çš„æ•°æ® (åˆ—: {col})")
                
                print(f"   {target_col} çš„æœ‰æ•ˆæ•°æ®: {len(df_valid)} è¡Œ")
            
            self.data = df_valid
            return df_valid
    
    def extract_features(self) -> np.ndarray:
        """æå–ç‰¹å¾"""
        if self.data is None:
            self.load_data()
        
        print(f"\nğŸ”§ æå–{self.config.feature.feature_type}ç‰¹å¾...")
        
        # å¼€å§‹ç‰¹å¾æå–è®¡æ—¶
        with self.timing.measure('feature_extraction', {'type': self.config.feature.feature_type}):
            self._extract_features_internal()
        
        # è®¡ç®—ååé‡
        if self.features is not None:
            self.timing.calculate_throughput('feature_extraction', len(self.features))
            
        return self.features
    
    def _extract_features_internal(self) -> np.ndarray:
        """å†…éƒ¨ç‰¹å¾æå–å®ç°"""
        # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
        feature_extractor = FeatureExtractor(
            use_cache=self.config.feature.use_cache,
            cache_dir=self.config.feature.cache_dir,
            morgan_bits=self.config.feature.morgan_bits if hasattr(self.config.feature, 'morgan_bits') else None,
            morgan_radius=self.config.feature.morgan_radius if hasattr(self.config.feature, 'morgan_radius') else None,
            descriptor_count=getattr(self.config.feature, 'descriptor_count', 85)
        )
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†å­æ•°æ®ï¼ˆæœ‰SMILESåˆ—ï¼‰
        has_smiles = any(col in self.data.columns for col in self.config.data.smiles_columns)
        
        if has_smiles and self.config.feature.feature_type in ['morgan', 'descriptors', 'combined']:
            # åˆ†å­ç‰¹å¾æå–ï¼Œä¼˜å…ˆä½¿ç”¨æ–‡ä»¶çº§ç¼“å­˜å¹¶æŒ‰å­é›†ç´¢å¼•åˆ‡ç‰‡
            features = None
            try:
                file_cache = FileFeatureCache(cache_dir='file_feature_cache')
                X_full = file_cache.load_features(
                    file_path=str(self.config.data.data_path),
                    feature_type=self.config.feature.feature_type,
                    morgan_bits=getattr(self.config.feature, 'morgan_bits', 1024),
                    morgan_radius=getattr(self.config.feature, 'morgan_radius', 2),
                    smiles_columns=self.config.data.smiles_columns,
                    combination_method=getattr(self.config.feature, 'combination_method', 'mean'),
                    descriptor_count=getattr(self.config.feature, 'descriptor_count', 85)
                )
                if X_full is not None:
                    # ä½¿ç”¨åŸå§‹ç´¢å¼•é€‰æ‹©å½“å‰ç›®æ ‡çš„æ•°æ®å­é›†
                    subset_index = self.data.index.to_numpy()
                    features = X_full[subset_index]
                    print("   âœ… è®­ç»ƒç‰¹å¾ä½¿ç”¨æ–‡ä»¶çº§ç¼“å­˜ (å·²åˆ‡ç‰‡è‡³å½“å‰å­é›†)")
            except Exception:
                features = None

            if features is None:
                # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œå°è¯•ä¸€æ¬¡æ€§ä¸ºæ•´ä¸ªè®­ç»ƒæ–‡ä»¶è®¡ç®—å¹¶å†™å…¥ç¼“å­˜
                try:
                    raw_df = getattr(self, 'raw_data', None)
                    if raw_df is None:
                        raw_df = pd.read_csv(self.config.data.data_path)
                        self.raw_data = raw_df

                    print("   â³ æœªå‘½ä¸­æ–‡ä»¶çº§ç¼“å­˜ï¼Œæ­£åœ¨ä¸ºæ•´ä¸ªè®­ç»ƒæ–‡ä»¶æå–ä¸€æ¬¡ç‰¹å¾...")
                    feats_full = []
                    for _, row in tqdm(raw_df.iterrows(), total=len(raw_df), desc="æå–åˆ†å­ç‰¹å¾(å…¨æ–‡ä»¶)"):
                        smiles_list = []
                        for col in self.config.data.smiles_columns:
                            if col in row and pd.notna(row[col]):
                                smiles_list.append(row[col])
                            else:
                                smiles_list.append(None)
                        f = feature_extractor.extract_combination(
                            smiles_list,
                            feature_type=self.config.feature.feature_type,
                            combination_method=self.config.feature.combination_method
                        )
                        feats_full.append(f)
                    X_full = np.array(feats_full)

                    # å†™å…¥ç¼“å­˜ä¾›åç»­ç›®æ ‡/é˜¶æ®µå¤ç”¨
                    try:
                        file_cache.save_features(
                            features=X_full,
                            file_path=str(self.config.data.data_path),
                            feature_type=self.config.feature.feature_type,
                            morgan_bits=getattr(self.config.feature, 'morgan_bits', 1024),
                            morgan_radius=getattr(self.config.feature, 'morgan_radius', 2),
                            smiles_columns=self.config.data.smiles_columns,
                            combination_method=getattr(self.config.feature, 'combination_method', 'mean'),
                            descriptor_count=getattr(self.config.feature, 'descriptor_count', 85),
                            row_count=len(raw_df),
                            failed_indices=[]
                        )
                        print("   ğŸ’¾ å·²ç¼“å­˜è®­ç»ƒç‰¹å¾(å…¨æ–‡ä»¶)")
                    except Exception:
                        pass

                    # åˆ‡ç‰‡åˆ°å½“å‰å­é›†
                    subset_index = self.data.index.to_numpy()
                    features = X_full[subset_index]
                except Exception:
                    # å›é€€åˆ°åŸé€è¡Œæå–é€»è¾‘
                    features = []
                    for _, row in tqdm(self.data.iterrows(), total=len(self.data), desc="æå–åˆ†å­ç‰¹å¾"):
                        smiles_list = []
                        for col in self.config.data.smiles_columns:
                            if col in row and pd.notna(row[col]):
                                smiles_list.append(row[col])
                            else:
                                smiles_list.append(None)
                        feat = feature_extractor.extract_combination(
                            smiles_list,
                            feature_type=self.config.feature.feature_type,
                            combination_method=self.config.feature.combination_method
                        )
                        features.append(feat)
                    features = np.array(features)
        else:
            # è¡¨æ ¼æ•°æ®ç‰¹å¾æå–ï¼ˆæ–°åŠŸèƒ½ï¼‰
            print("   æ£€æµ‹åˆ°è¡¨æ ¼æ•°æ®ï¼Œä½¿ç”¨é€šç”¨ç‰¹å¾æå–...")
            
            # è·å–ç›®æ ‡åˆ—ä»¥æ’é™¤
            target_cols = self.config.data.target_columns if hasattr(self.config.data, 'target_columns') else []
            
            # ä½¿ç”¨æ–°çš„DataFrameæå–æ–¹æ³•
            if hasattr(feature_extractor, 'extract_from_dataframe'):
                features = feature_extractor.extract_from_dataframe(
                    self.data,
                    smiles_columns=self.config.data.smiles_columns if has_smiles else None,
                    target_columns=target_cols,
                    feature_type='tabular' if not has_smiles else 'auto'
                )
            else:
                # åå¤‡æ–¹æ¡ˆï¼šä½¿ç”¨æ‰€æœ‰éç›®æ ‡åˆ—ä½œä¸ºç‰¹å¾
                feature_cols = [col for col in self.data.columns if col not in target_cols]
                features = self.data[feature_cols].values
        
        print(f"   ç‰¹å¾ç»´åº¦: {features.shape}")
        
        # å¤„ç†NaNå’ŒInf
        n_nan = np.isnan(features).sum()
        n_inf = np.isinf(features).sum()
        if n_nan > 0 or n_inf > 0:
            print(f"   âš ï¸ å‘ç° {n_nan} ä¸ªNaNå€¼, {n_inf} ä¸ªInfå€¼ï¼Œæ­£åœ¨å¤„ç†...")
        features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
        
        self.features = features
        return features
    
    def prepare_target(self, target_col: str) -> np.ndarray:
        """å‡†å¤‡ç›®æ ‡å˜é‡"""
        if self.data is None:
            self.load_data()
        
        y = self.data[target_col].values
        
        # å•ä½è½¬æ¢
        if target_col == 'PLQY' and y.max() > 1.5:
            print(f"   è½¬æ¢PLQY: ç™¾åˆ†æ¯” â†’ å°æ•°")
            y = y / 100
        
        return y
    
    def train_single_target(self, target_col: str) -> Dict:
        """
        è®­ç»ƒå•ä¸ªç›®æ ‡
        
        Args:
            target_col: ç›®æ ‡åˆ—å
        
        Returns:
            è®­ç»ƒç»“æœ
        """
        print(f"\n{'='*60}")
        print(f"è®­ç»ƒç›®æ ‡: {target_col}")
        print(f"{'='*60}")
        
        # ä¸ºè¯¥ç›®æ ‡ç‹¬ç«‹åŠ è½½å’Œè¿‡æ»¤æ•°æ®
        self.load_data(target_col=target_col)
        
        # æå–ç‰¹å¾ï¼ˆæ¯ä¸ªç›®æ ‡ç‹¬ç«‹æå–ï¼‰
        self.features = None  # é‡ç½®ç‰¹å¾
        self.extract_features()
        
        X = self.features
        y = self.prepare_target(target_col)
        
        print(f"   æ ·æœ¬æ•°: {len(X)}")
        print(f"   ç‰¹å¾æ•°: {X.shape[1]}")
        print(f"   ç›®æ ‡èŒƒå›´: [{y.min():.2f}, {y.max():.2f}]")
        
        # ä¼˜åŒ–åŠŸèƒ½å·²è¢«ç§»é™¤ï¼Œç›´æ¥ä½¿ç”¨é»˜è®¤å‚æ•°è®­ç»ƒ
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = ModelFactory.create_trainer(
            self.config.model.model_type,
            self.config.model.hyperparameters,
            self.config.training.n_folds
        )
        
        # åˆå§‹åŒ–è®°å½•å™¨
        if self.config.logging.auto_save:
            if self.logger is None:
                self.logger = TrainingLogger(
                    project_name=self.config.logging.project_name,
                    base_dir=self.config.logging.base_dir,
                    auto_save=self.config.logging.auto_save,
                    save_plots=self.config.logging.save_plots
                )
            
            # å¼€å§‹å®éªŒ
            experiment_id = self.logger.start_experiment(
                model_type=self.config.model.model_type,
                target=target_col,
                feature_type=self.config.feature.feature_type,
                hyperparameters=self.config.model.hyperparameters,
                n_folds=self.config.training.n_folds,
                n_samples=len(X),
                n_features=X.shape[1],
                config=self.config.to_dict()
            )
        
        # æ‰§è¡Œäº¤å‰éªŒè¯
        kf = KFold(
            n_splits=self.config.training.n_folds,
            shuffle=True,
            random_state=self.config.data.random_seed
        )
        
        all_predictions = np.zeros_like(y)
        fold_models = []
        fold_metrics = []
        
        # åˆå§‹åŒ–ç‰¹å¾é‡è¦æ€§è®°å½•å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        feature_importance_recorder = None
        if self.config.training.save_feature_importance:
            from utils import FeatureImportanceRecorder
            feature_importance_recorder = FeatureImportanceRecorder(
                save_dir=Path(self.config.logging.base_dir) / self.config.logging.project_name,
                model_name=self.config.model.model_type,
                target=target_col
            )
        
        for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X), 1):
            # è®°å½•æŠ˜å¼€å§‹
            if self.logger:
                self.logger.log_fold_start(fold_idx, train_idx.tolist(), val_idx.tolist())
            
            # åˆ†å‰²æ•°æ®
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            
            # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
            from models import BaseModel
            model = BaseModel(self.config.model.model_type, self.config.model.hyperparameters)
            early_rounds = self.config.training.early_stopping_rounds if self.config.training.early_stopping else None
            
            # è®°å½•æ¯æŠ˜çš„è®­ç»ƒæ—¶é—´
            with self.timing.measure(f'fold_{fold_idx}_training', {'fold': fold_idx, 'samples': len(train_idx)}):
                model.fit(
                    X_train, y_train,
                    eval_set=[(X_val, y_val)],
                    verbose=False,
                    early_stopping_rounds=early_rounds
                )
            
            # é¢„æµ‹
            with self.timing.measure(f'fold_{fold_idx}_prediction', {'fold': fold_idx, 'samples': len(val_idx)}):
                y_train_pred = model.predict(X_train)
                y_val_pred = model.predict(X_val)
                all_predictions[val_idx] = y_val_pred
            
            # è¯„ä¼°
            train_metrics = evaluate_model(y_train, y_train_pred)
            val_metrics = evaluate_model(y_val, y_val_pred)
            
            fold_metrics.append(val_metrics)
            fold_models.append(model)
            
            # æå–å¹¶è®°å½•ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœå¯ç”¨ä¸”æ¨¡å‹æ”¯æŒï¼‰
            if feature_importance_recorder:
                try:
                    # å°è¯•ä»æ¨¡å‹ä¸­æå–ç‰¹å¾é‡è¦æ€§
                    importance = FeatureImportanceRecorder.extract_importance_from_model(model.model)
                    if importance is not None:
                        # ç”Ÿæˆç‰¹å¾åç§°ï¼ˆå¦‚æœéœ€è¦ï¼‰
                        feature_names = [f"feature_{i}" for i in range(X.shape[1])]
                        feature_importance_recorder.add_fold_importance(
                            fold_idx, importance, feature_names
                        )
                except Exception as e:
                    if self.config.training.verbose > 1:
                        print(f"    âš ï¸ æ— æ³•æå–ç‰¹å¾é‡è¦æ€§: {e}")
            
            # è®°å½•æŠ˜ç»“æŸ
            if self.logger:
                self.logger.log_fold_end(
                    y_train=y_train,
                    y_train_pred=y_train_pred,
                    y_val=y_val,
                    y_val_pred=y_val_pred,
                    metrics={**val_metrics, 'train_rmse': train_metrics['rmse'], 'train_r2': train_metrics['r2']}
                )
            
            # æ˜¾ç¤ºè¿›åº¦
            if self.config.training.verbose > 0:
                print(f"\n  æŠ˜ {fold_idx}/{self.config.training.n_folds}:")
                print(f"    è®­ç»ƒ - RMSE: {train_metrics['rmse']:.4f}, RÂ²: {train_metrics['r2']:.4f}")
                print(f"    éªŒè¯ - RMSE: {val_metrics['rmse']:.4f}, RÂ²: {val_metrics['r2']:.4f}")
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        final_metrics = evaluate_model(y, all_predictions)
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = {}
        for metric in self.config.training.metrics:
            values = [fold[metric] for fold in fold_metrics if metric in fold]
            if values:
                avg_metrics[f"{metric}_mean"] = np.mean(values)
                avg_metrics[f"{metric}_std"] = np.std(values)
        
        print(f"\nğŸ“Š äº¤å‰éªŒè¯ç»“æœ:")
        for metric in self.config.training.metrics:
            if f"{metric}_mean" in avg_metrics:
                print(f"   {metric.upper()}: {avg_metrics[f'{metric}_mean']:.4f} Â± {avg_metrics[f'{metric}_std']:.4f}")
        
        # ä¿å­˜ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if feature_importance_recorder:
            try:
                feature_importance_recorder.save_importance()
            except Exception as e:
                if self.config.training.verbose > 0:
                    print(f"   âš ï¸ ä¿å­˜ç‰¹å¾é‡è¦æ€§å¤±è´¥: {e}")
        
        # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        final_model = None
        if self.config.training.save_final_model:
            print(f"\nğŸ¯ è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆå…¨éƒ¨æ•°æ®ï¼‰...")
            final_model = BaseModel(self.config.model.model_type, self.config.model.hyperparameters)
            with self.timing.measure('final_model_training'):
                final_model.fit(X, y, verbose=False)
            
            # ä¿å­˜æ¨¡å‹
            model_dir = Path(self.config.logging.base_dir) / self.config.logging.project_name / "models"
            model_dir.mkdir(parents=True, exist_ok=True)
            
            model_filename = generate_model_filename(
                self.config.model.model_type,
                target_col,
                "_final"
            )
            model_path = model_dir / model_filename
            final_model.save(model_path)
            print(f"   ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
            
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.config.training.save_feature_importance:
                try:
                    from utils import FeatureImportanceRecorder
                    importance = FeatureImportanceRecorder.extract_importance_from_model(final_model.model)
                    if importance is not None:
                        # åˆ›å»ºä¸€ä¸ªæ–°çš„è®°å½•å™¨ç”¨äºæœ€ç»ˆæ¨¡å‹
                        final_importance_recorder = FeatureImportanceRecorder(
                            save_dir=Path(self.config.logging.base_dir) / self.config.logging.project_name,
                            model_name=f"{self.config.model.model_type}_final",
                            target=target_col
                        )
                        feature_names = [f"feature_{i}" for i in range(X.shape[1])]
                        final_importance_recorder.add_fold_importance(0, importance, feature_names)
                        final_importance_recorder.save_importance()
                except Exception as e:
                    if self.config.training.verbose > 1:
                        print(f"   âš ï¸ ä¿å­˜æœ€ç»ˆæ¨¡å‹ç‰¹å¾é‡è¦æ€§å¤±è´¥: {e}")

        # è‹¥æä¾›æµ‹è¯•é›†ï¼Œè¿›è¡Œæµ‹è¯•è¯„ä¼°ï¼ˆä»…ä½¿ç”¨å®Œæ•´æ•°æ®è®­ç»ƒçš„æœ€ç»ˆæ¨¡å‹ï¼‰
        test_evaluation = None
        test_predictions = None
        if getattr(self.config.data, 'test_data_path', None):
            try:
                test_path = Path(self.config.data.test_data_path)
                print(f"\n" + "="*50)
                print(f"ğŸ§ª æµ‹è¯•é›†è¯„ä¼° (Test Evaluation)")
                print("="*50)
                print(f"æ–‡ä»¶: {test_path.name}")
                if test_path.exists():
                    print(f"çŠ¶æ€: âœ… æ–‡ä»¶å­˜åœ¨")
                    print(f"è·¯å¾„: {test_path.resolve()}")
                    with self.timing.measure('data_load_test'):
                        df_test = pd.read_csv(test_path)
                    # å‡†å¤‡æµ‹è¯•ç‰¹å¾ï¼šä¸è®­ç»ƒç›¸åŒçš„æµç¨‹
                    feature_extractor = FeatureExtractor(
                        use_cache=self.config.feature.use_cache,
                        cache_dir=self.config.feature.cache_dir,
                        morgan_bits=self.config.feature.morgan_bits if hasattr(self.config.feature, 'morgan_bits') else None,
                        morgan_radius=self.config.feature.morgan_radius if hasattr(self.config.feature, 'morgan_radius') else None,
                        descriptor_count=getattr(self.config.feature, 'descriptor_count', 85)
                    )
                    has_smiles = any(col in df_test.columns for col in self.config.data.smiles_columns)
                    if has_smiles and self.config.feature.feature_type in ['morgan', 'descriptors', 'combined']:
                        # ä¼˜å…ˆå°è¯•æ–‡ä»¶çº§ç¼“å­˜
                        X_test = None
                        try:
                            file_cache = FileFeatureCache(cache_dir='file_feature_cache')
                            X_test = file_cache.load_features(
                                file_path=str(test_path),
                                feature_type=self.config.feature.feature_type,
                                morgan_bits=getattr(self.config.feature, 'morgan_bits', 1024),
                                morgan_radius=getattr(self.config.feature, 'morgan_radius', 2),
                                smiles_columns=self.config.data.smiles_columns,
                                combination_method=getattr(self.config.feature, 'combination_method', 'mean'),
                                descriptor_count=getattr(self.config.feature, 'descriptor_count', 85)
                            )
                            if X_test is not None:
                                print("\nâœ… ä»æ–‡ä»¶çº§ç¼“å­˜åŠ è½½æµ‹è¯•ç‰¹å¾ï¼Œè·³è¿‡æå–")
                                print(f"   å½¢çŠ¶: {X_test.shape}")
                                print("   å¼€å§‹é€‰æ‹©æ¨ç†æ¨¡å‹ä¸é¢„æµ‹")
                        except Exception as _e:
                            # ç¼“å­˜å¤±è´¥æ—¶é™é»˜å›é€€åˆ°æ­£å¸¸æå–
                            X_test = None

                        if X_test is None:
                            feats = []
                            for _, row in tqdm(df_test.iterrows(), total=len(df_test), desc="æå–åˆ†å­ç‰¹å¾(æµ‹è¯•)"):
                                smiles_list = []
                                for col in self.config.data.smiles_columns:
                                    if col in row and pd.notna(row[col]):
                                        smiles_list.append(row[col])
                                    else:
                                        smiles_list.append(None)
                                with self.timing.measure('feature_extract_test_single'):
                                    f = feature_extractor.extract_combination(
                                        smiles_list,
                                        feature_type=self.config.feature.feature_type,
                                        combination_method=self.config.feature.combination_method
                                    )
                                feats.append(f)
                            X_test = np.array(feats)

                            # å†™å…¥æ–‡ä»¶çº§ç¼“å­˜ï¼Œä¾›å…¶å®ƒç›®æ ‡å¤ç”¨
                            try:
                                file_cache.save_features(
                                    features=X_test,
                                    file_path=str(test_path),
                                    feature_type=self.config.feature.feature_type,
                                    morgan_bits=getattr(self.config.feature, 'morgan_bits', 1024),
                                    morgan_radius=getattr(self.config.feature, 'morgan_radius', 2),
                                    smiles_columns=self.config.data.smiles_columns,
                                    combination_method=getattr(self.config.feature, 'combination_method', 'mean'),
                                    descriptor_count=getattr(self.config.feature, 'descriptor_count', 85),
                                    row_count=len(df_test),
                                    failed_indices=[]
                                )
                                print("ğŸ’¾ å·²ç¼“å­˜æµ‹è¯•ç‰¹å¾ï¼Œåç»­ç›®æ ‡å°†å¤ç”¨")
                            except Exception:
                                pass
                    else:
                        target_cols = self.config.data.target_columns if hasattr(self.config.data, 'target_columns') else []
                        with self.timing.measure('feature_extract_test_tabular'):
                            X_test = feature_extractor.extract_from_dataframe(
                                df_test,
                                smiles_columns=self.config.data.smiles_columns if has_smiles else None,
                                target_columns=target_cols,
                                feature_type='tabular' if not has_smiles else 'auto'
                            )
                    X_test = np.nan_to_num(X_test, nan=0.0, posinf=0.0, neginf=0.0)

                    # é€‰æ‹©ç”¨äºé¢„æµ‹çš„æ¨¡å‹ï¼ˆå¦‚æœä¿å­˜äº†æœ€ç»ˆæ¨¡å‹åˆ™ç”¨æœ€ç»ˆæ¨¡å‹ï¼Œå¦åˆ™ç”¨ç®€å•å¹³å‡æŠ˜æ¨¡å‹ï¼‰
                    model_for_inference = final_model
                    if model_for_inference is None and len(fold_models) > 0:
                        print(f"   ä½¿ç”¨æŠ˜æ¨¡å‹é›†æˆé¢„æµ‹, æ•°é‡: {len(fold_models)}")
                        with self.timing.measure('test_predict_oof_ensemble'):
                            preds_list = []
                            for j, m in enumerate(fold_models, 1):
                                print(f"   æŠ˜ {j}/{len(fold_models)} é¢„æµ‹å¼€å§‹")
                                p = m.predict(X_test)
                                preds_list.append(p)
                                print(f"   æŠ˜ {j} é¢„æµ‹å®Œæˆ")
                        test_predictions = np.mean(np.vstack(preds_list), axis=0)
                        print(f"   é›†æˆé¢„æµ‹å®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {np.array(test_predictions).shape}")
                    else:
                        print("   ä½¿ç”¨æœ€ç»ˆæ¨¡å‹è¿›è¡Œé¢„æµ‹")
                        with self.timing.measure('test_predict_final_model'):
                            test_predictions = model_for_inference.predict(X_test)
                        print(f"   æœ€ç»ˆæ¨¡å‹é¢„æµ‹å®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {np.array(test_predictions).shape}")

                    # è‹¥æµ‹è¯•é›†ä¸­åŒ…å«å½“å‰ç›®æ ‡åˆ—ï¼Œè®¡ç®—æŒ‡æ ‡
                    if target_col in df_test.columns:
                        y_test = df_test[target_col].values
                        if target_col == 'PLQY' and y_test.max() > 1.5:
                            y_test = y_test / 100
                        test_evaluation = evaluate_model(y_test, test_predictions)
                        
                        # è¯¦ç»†çš„æµ‹è¯•ç»“æœè¾“å‡º
                        print(f"\nğŸ“Š æµ‹è¯•ç»“æœ ({target_col}):")
                        print(f"   æ ·æœ¬æ•°: {len(y_test)}")
                        print(f"   â”œâ”€ RMSE: {test_evaluation['rmse']:.4f}")
                        print(f"   â”œâ”€ MAE:  {test_evaluation['mae']:.4f}")
                        print(f"   â”œâ”€ RÂ²:   {test_evaluation['r2']:.4f}")
                        print(f"   â””â”€ MAPE: {test_evaluation.get('mape', 0):.2f}%")

                    # ä¿å­˜æµ‹è¯•é¢„æµ‹
                    if self.logger:
                        exp_dir = Path(self.config.logging.base_dir) / self.config.logging.project_name
                        exports_dir = exp_dir / 'exports'
                        exports_dir.mkdir(parents=True, exist_ok=True)
                        out_csv = exports_dir / f"test_predictions_{self.config.model.model_type}_{target_col}.csv"
                        df_out = df_test.copy()
                        df_out['prediction'] = test_predictions
                        df_out.to_csv(out_csv, index=False)
                        
                        # ä¿å­˜æµ‹è¯•æŒ‡æ ‡ï¼ˆè‹¥æœ‰ï¼‰
                        if test_evaluation is not None:
                            out_json = exports_dir / f"test_metrics_{self.config.model.model_type}_{target_col}.json"
                            import json as _json
                            with open(out_json, 'w') as f:
                                _json.dump(test_evaluation, f, indent=2)
                        
                        # è¾“å‡ºä¿å­˜ä¿¡æ¯
                        print(f"\nğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜:")
                        print(f"   é¢„æµ‹æ–‡ä»¶: {out_csv.name}")
                        if test_evaluation is not None:
                            print(f"   æŒ‡æ ‡æ–‡ä»¶: {out_json.name}")
                        print(f"   ä¿å­˜ç›®å½•: {exports_dir}")
                        print("="*50)
                else:
                    print(f"   âš ï¸ æµ‹è¯•é›†è·¯å¾„ä¸å­˜åœ¨: {test_path}")
                    print(f"      å½“å‰å·¥ä½œç›®å½•: {Path.cwd()}")
                    # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
                    alternative_paths = [
                        Path(test_path.name),  # å½“å‰ç›®å½•
                        Path("../data") / test_path.name,  # ../dataç›®å½•
                        Path("data") / test_path.name,  # dataç›®å½•
                    ]
                    for alt_path in alternative_paths:
                        if alt_path.exists():
                            print(f"      ğŸ’¡ æ–‡ä»¶å¯èƒ½åœ¨: {alt_path}")
            except Exception as e:
                print(f"   âš ï¸ æµ‹è¯•é›†è¯„ä¼°å¤±è´¥: {e}")
                import traceback
                if self.config.training.verbose > 1:
                    traceback.print_exc()
        
        # ç»“æŸå®éªŒ
        if self.logger:
            self.logger.end_experiment(final_metrics)
            try:
                timing_summary = self.timing.get_summary()
                for k, v in timing_summary.get('records', {}).items():
                    self.logger.add_timing(k, v.get('duration', 0))
            except Exception:
                pass
        
        # æ‰“å°å’Œä¿å­˜æ—¶é—´ç»Ÿè®¡
        if self.config.training.verbose > 0:
            print("\n" + "="*50)
            print("â±ï¸ æ—¶é—´ç»Ÿè®¡")
            print("="*50)
            self.timing.print_summary()
        
        # ä¿å­˜æ—¶é—´æŠ¥å‘Š
        if self.logger:
            try:
                exp_dir = Path(self.config.logging.base_dir) / self.config.logging.project_name
                timing_dir = exp_dir / 'timing'
                timing_dir.mkdir(parents=True, exist_ok=True)
                
                # ä¿å­˜JSONæ ¼å¼
                self.timing.save_report(
                    timing_dir / f"timing_{self.config.model.model_type}_{target_col}.json",
                    format='json'
                )
                
                # ä¿å­˜æ–‡æœ¬æ ¼å¼
                self.timing.save_report(
                    timing_dir / f"timing_{self.config.model.model_type}_{target_col}.txt",
                    format='txt'
                )
                
                if self.config.training.verbose > 0:
                    print(f"\nğŸ’¾ æ—¶é—´æŠ¥å‘Šå·²ä¿å­˜åˆ°: {timing_dir}")
            except Exception as e:
                if self.config.training.verbose > 1:
                    print(f"âš ï¸ ä¿å­˜æ—¶é—´æŠ¥å‘Šå¤±è´¥: {e}")
            
            # å¯¼å‡ºè®ºæ–‡æ•°æ®
            if self.config.logging.export_for_paper:
                self.logger.export_for_paper(experiment_id)
        
        return {
            'target': target_col,
            'final_metrics': final_metrics,
            'avg_metrics': avg_metrics,
            'fold_metrics': fold_metrics,
            'predictions': all_predictions,
            'true_values': y,
            'test_metrics': test_evaluation,
            'test_predictions_saved': self.config.data.test_data_path is not None
        }
    
    def train_all_targets(self) -> Dict:
        """è®­ç»ƒæ‰€æœ‰ç›®æ ‡"""
        results = {}
        
        for target in self.available_targets:
            try:
                print(f"\nè®­ç»ƒç›®æ ‡: {target}")
                result = self.train_single_target(target)
                results[target] = result
            except Exception as e:
                print(f"è®­ç»ƒ {target} å¤±è´¥: {e}")
                results[target] = {'error': str(e)}
        
        return results
    
    def run(self, targets: Optional[List[str]] = None) -> Dict:
        """
        è¿è¡Œè®­ç»ƒç®¡é“
        
        Args:
            targets: è¦è®­ç»ƒçš„ç›®æ ‡åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºè®­ç»ƒæ‰€æœ‰
        
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ: {self.config.name}")
        
        # åˆå§‹åŠ è½½æ•°æ®ä»¥æ£€æŸ¥å¯ç”¨ç›®æ ‡
        self.load_data()
        
        # ç¡®å®šç›®æ ‡
        if targets:
            targets_to_train = [t for t in targets if t in self.available_targets]
        else:
            targets_to_train = self.available_targets
        
        if not targets_to_train:
            raise ValueError("æ²¡æœ‰å¯è®­ç»ƒçš„ç›®æ ‡")
        
        print(f"\nå°†è®­ç»ƒ {len(targets_to_train)} ä¸ªç›®æ ‡: {targets_to_train}")
        
        # è®­ç»ƒæ‰€æœ‰ç›®æ ‡
        results = {}
        for target in targets_to_train:
            try:
                print(f"\nè®­ç»ƒç›®æ ‡: {target}")
                result = self.train_single_target(target)
                results[target] = result
            except Exception as e:
                print(f"è®­ç»ƒ {target} å¤±è´¥: {e}")
                results[target] = {'error': str(e)}
        
        # æ‰“å°æ±‡æ€»
        self.print_summary(results)
        
        return results
    
    def print_summary(self, results: Dict):
        """æ‰“å°è®­ç»ƒæ±‡æ€»"""
        print("\n" + "="*60)
        print("è®­ç»ƒæ±‡æ€»")
        print("="*60)
        
        for target, result in results.items():
            if 'error' in result:
                print(f"\nâŒ {target}: å¤±è´¥ - {result['error']}")
            else:
                print(f"\nâœ… {target}:")
                if 'final_metrics' in result:
                    metrics = result['final_metrics']
                    print(f"   RMSE: {metrics.get('rmse', 'N/A'):.4f}" if isinstance(metrics.get('rmse'), (int, float)) else f"   RMSE: N/A")
                    print(f"   MAE:  {metrics.get('mae', 'N/A'):.4f}" if isinstance(metrics.get('mae'), (int, float)) else f"   MAE: N/A")
                    print(f"   RÂ²:   {metrics.get('r2', 'N/A'):.4f}" if isinstance(metrics.get('r2'), (int, float)) else f"   RÂ²: N/A")
                if 'avg_metrics' in result:
                    avg = result['avg_metrics']
                    print(f"   CVå¹³å‡: RMSE={avg.get('rmse', 'N/A'):.4f}" if isinstance(avg.get('rmse'), (int, float)) else f"   CVå¹³å‡: N/A")


# ========================================
#           æ‰¹é‡è®­ç»ƒç®¡é“
# ========================================

class BatchTrainingPipeline:
    """æ‰¹é‡è®­ç»ƒç®¡é“"""
    
    def __init__(self, batch_config: BatchExperimentConfig):
        """
        åˆå§‹åŒ–æ‰¹é‡è®­ç»ƒç®¡é“
        
        Args:
            batch_config: æ‰¹é‡å®éªŒé…ç½®
        """
        self.batch_config = batch_config
        self.results = {}
    
    def run(self) -> Dict:
        """è¿è¡Œæ‰¹é‡è®­ç»ƒ"""
        configs = self.batch_config.generate_configs()
        
        print(f"\nğŸš€ æ‰¹é‡è®­ç»ƒ: {len(configs)} ä¸ªå®éªŒ")
        print("="*60)
        
        for i, config in enumerate(configs, 1):
            print(f"\n[{i}/{len(configs)}] å®éªŒ: {config.name}")
            
            try:
                pipeline = TrainingPipeline(config)
                result = pipeline.run()
                self.results[config.name] = {
                    'config': config,
                    'results': result,
                    'status': 'success'
                }
            except Exception as e:
                print(f"âŒ å®éªŒå¤±è´¥: {e}")
                self.results[config.name] = {
                    'config': config,
                    'error': str(e),
                    'status': 'failed'
                }
        
        # æ±‡æ€»ç»“æœ
        self.print_summary()
        
        return self.results
    
    def print_summary(self):
        """æ‰“å°æ‰¹é‡è®­ç»ƒæ±‡æ€»"""
        print("\n" + "="*60)
        print("æ‰¹é‡è®­ç»ƒæ±‡æ€»")
        print("="*60)
        
        success_count = sum(1 for r in self.results.values() if r['status'] == 'success')
        print(f"\næˆåŠŸ: {success_count}/{len(self.results)}")
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_models = {}
        for name, result in self.results.items():
            if result['status'] == 'success':
                for target, target_result in result['results'].items():
                    if 'final_metrics' in target_result:
                        key = f"{target}_rmse"
                        rmse = target_result['final_metrics']['rmse']
                        if key not in best_models or rmse < best_models[key]['value']:
                            best_models[key] = {
                                'experiment': name,
                                'value': rmse
                            }
        
        if best_models:
            print("\nğŸ† æœ€ä½³æ¨¡å‹:")
            for key, info in best_models.items():
                target = key.replace('_rmse', '')
                print(f"   {target}: {info['experiment']} (RMSE: {info['value']:.4f})")


# ========================================
#           å‘½ä»¤è¡Œæ¥å£
# ========================================

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='åŸºäºé…ç½®çš„æœºå™¨å­¦ä¹ è®­ç»ƒç®¡é“',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # é…ç½®ç›¸å…³å‚æ•°
    parser.add_argument('config', nargs='?', help='é…ç½®æ–‡ä»¶è·¯å¾„æˆ–æ¨¡æ¿åç§°')
    parser.add_argument('--template', '-t', help='ä½¿ç”¨é¢„å®šä¹‰æ¨¡æ¿')
    parser.add_argument('--list-templates', action='store_true', help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡æ¿')
    parser.add_argument('--wizard', action='store_true', help='ä½¿ç”¨é…ç½®å‘å¯¼')
    parser.add_argument('--save-config', help='ä¿å­˜é…ç½®åˆ°æ–‡ä»¶')
    
    # è®­ç»ƒç›¸å…³å‚æ•°
    parser.add_argument('--target', help='æŒ‡å®šè®­ç»ƒç›®æ ‡ï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--dry-run', action='store_true', help='åªéªŒè¯é…ç½®ï¼Œä¸æ‰§è¡Œè®­ç»ƒ')
    parser.add_argument('--test-data', dest='test_data', help='å¯é€‰ï¼šæŒ‡å®šæµ‹è¯•é›†CSVè·¯å¾„ï¼Œç”¨äºå®Œæ•´è®­ç»ƒåè¯„ä¼°')
    
    # è¦†ç›–é…ç½®å‚æ•°
    parser.add_argument('--model', help='æ¨¡å‹ç±»å‹')
    parser.add_argument('--feature', help='ç‰¹å¾ç±»å‹')
    parser.add_argument('--folds', type=int, help='äº¤å‰éªŒè¯æŠ˜æ•°')
    parser.add_argument('--project', help='é¡¹ç›®åç§°')
    
    args = parser.parse_args()
    
    # é…ç½®ç®¡ç†å™¨
    manager = ConfigManager()
    
    # åˆ—å‡ºæ¨¡æ¿
    if args.list_templates:
        print("\nå¯ç”¨æ¨¡æ¿:")
        for template in manager.list_templates():
            desc = manager.templates[template].description
            print(f"  - {template}: {desc}")
        return
    
    # é…ç½®å‘å¯¼
    if args.wizard:
        config = manager.create_from_wizard()
    
    # åŠ è½½é…ç½®
    elif args.config:
        # å°è¯•ä½œä¸ºæ¨¡æ¿
        if args.config in manager.list_templates():
            config = manager.get_template(args.config)
        # ä½œä¸ºæ–‡ä»¶è·¯å¾„
        else:
            config = load_config(args.config)
    
    # ä½¿ç”¨æ¨¡æ¿
    elif args.template:
        config = manager.get_template(args.template)
    
    # é»˜è®¤é…ç½®
    else:
        print("ä½¿ç”¨é»˜è®¤é…ç½® (xgboost_quick)")
        config = manager.get_template('xgboost_quick')
    
    # è¦†ç›–é…ç½®
    if args.model:
        config.model.model_type = args.model
    if args.feature:
        config.feature.feature_type = args.feature
    if args.folds:
        config.training.n_folds = args.folds
    if args.project:
        config.logging.project_name = args.project
    # æµ‹è¯•é›†å‚æ•°
    if args.test_data:
        config.data.test_data_path = args.test_data
    
    # ä¿å­˜é…ç½®
    if args.save_config:
        path = manager.save_config(config, args.save_config, 'yaml')
        print(f"é…ç½®å·²ä¿å­˜: {path}")
    
    # éªŒè¯é…ç½®
    if not ConfigValidator.validate_all(config):
        print("é…ç½®éªŒè¯å¤±è´¥")
        return
    
    # å¹²è¿è¡Œ
    if args.dry_run:
        print("\né…ç½®ä¿¡æ¯:")
        print(config.to_yaml())
        print("\nâœ… é…ç½®éªŒè¯é€šè¿‡ï¼ˆå¹²è¿è¡Œæ¨¡å¼ï¼‰")
        return
    
    # è¿è¡Œè®­ç»ƒ
    try:
        pipeline = TrainingPipeline(config)
        
        # ç¡®å®šç›®æ ‡
        targets = None
        if args.target:
            targets = [t.strip() for t in args.target.split(',')]
        
        # æ‰§è¡Œè®­ç»ƒ
        results = pipeline.run(targets)
        
        print("\nâœ¨ è®­ç»ƒå®Œæˆ!")
        
        # ä¿å­˜æœ€ç»ˆé…ç½®
        if config.logging.auto_save:
            final_config_path = (
                Path(config.logging.base_dir) / 
                config.logging.project_name / 
                "experiment_config.yaml"
            )
            config.to_yaml(str(final_config_path))
            print(f"å®éªŒé…ç½®å·²ä¿å­˜: {final_config_path}")
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
