#!/usr/bin/env python3
"""
é«˜æ•ˆç”Ÿæˆè™šæ‹Ÿæ•°æ®åº“ - ä½¿ç”¨å‘é‡åŒ–æ“ä½œ
"""

import os
import sys
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
import argparse
import joblib
from itertools import product
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.feature_extractor import FeatureExtractor

def load_and_deduplicate_ligands(data_file):
    """åŠ è½½æ•°æ®å¹¶å»é‡é…ä½“"""
    print("åŠ è½½å¹¶å»é‡é…ä½“...")
    df = pd.read_csv(data_file)
    
    # åˆå¹¶L1å’ŒL2ï¼Œå»é‡
    l1_series = df['L1'].dropna()
    l2_series = df['L2'].dropna()
    
    # ä½¿ç”¨setå¿«é€Ÿå»é‡
    l12_set = set(l1_series) | set(l2_series)
    l12_unique = sorted(list(l12_set))  # æ’åºä»¥ä¿è¯ç»“æœä¸€è‡´æ€§
    
    # L3å»é‡
    l3_unique = sorted(df['L3'].dropna().unique().tolist())
    
    print(f"  L1åŸå§‹: {len(df['L1'].dropna())} â†’ å”¯ä¸€: {len(df['L1'].dropna().unique())}")
    print(f"  L2åŸå§‹: {len(df['L2'].dropna())} â†’ å”¯ä¸€: {len(df['L2'].dropna().unique())}")
    print(f"  L1âˆªL2å»é‡å: {len(l12_unique)} ä¸ªé…ä½“")
    print(f"  L3å”¯ä¸€: {len(l3_unique)} ä¸ªé…ä½“")
    print(f"  ç†è®ºç»„åˆæ•°: {len(l12_unique) * len(l3_unique):,}")
    
    return l12_unique, l3_unique, df

def generate_combinations_vectorized(l12_unique, l3_unique):
    """ä½¿ç”¨å‘é‡åŒ–æ“ä½œå¿«é€Ÿç”Ÿæˆç»„åˆ"""
    print("\nç”Ÿæˆç»„åˆï¼ˆå‘é‡åŒ–ï¼‰...")
    
    # ä½¿ç”¨NumPyçš„meshgridç”Ÿæˆæ‰€æœ‰ç»„åˆ
    # è¿™æ¯”itertools.productæ›´å¿«
    n_l12 = len(l12_unique)
    n_l3 = len(l3_unique)
    
    # åˆ›å»ºç´¢å¼•ç½‘æ ¼
    l12_idx, l3_idx = np.meshgrid(range(n_l12), range(n_l3), indexing='ij')
    
    # å±•å¹³ç´¢å¼•
    l12_idx_flat = l12_idx.flatten()
    l3_idx_flat = l3_idx.flatten()
    
    # ä½¿ç”¨ç´¢å¼•è·å–å®é™…çš„SMILESå­—ç¬¦ä¸²
    l12_array = np.array(l12_unique)
    l3_array = np.array(l3_unique)
    
    # å‘é‡åŒ–è·å–ç»„åˆ
    l1_values = l12_array[l12_idx_flat]
    l2_values = l1_values.copy()  # L1=L2
    l3_values = l3_array[l3_idx_flat]
    
    # åˆ›å»ºDataFrameï¼ˆä¸€æ¬¡æ€§åˆ›å»ºï¼Œé¿å…é€è¡Œæ·»åŠ ï¼‰
    assembled_df = pd.DataFrame({
        'L1': l1_values,
        'L2': l2_values,
        'L3': l3_values
    })
    
    print(f"  ç”Ÿæˆç»„åˆæ•°: {len(assembled_df):,}")
    
    return assembled_df

def batch_extract_features(df, feature_type='combined', batch_size=1000, combination_method='mean', descriptor_count=85):
    """æ‰¹é‡æå–ç‰¹å¾ï¼Œæé«˜æ•ˆç‡"""
    print("\næ‰¹é‡æå–åˆ†å­ç‰¹å¾...")
    
    # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
    extractor = FeatureExtractor(
        feature_type=feature_type,
        morgan_radius=2,
        morgan_bits=1024,
        use_cache=True,
        descriptor_count=descriptor_count
    )
    
    n_samples = len(df)
    n_batches = (n_samples + batch_size - 1) // batch_size
    
    features_list = []
    valid_indices = []
    
    for batch_idx in range(n_batches):
        start_idx = batch_idx * batch_size
        end_idx = min((batch_idx + 1) * batch_size, n_samples)
        batch_df = df.iloc[start_idx:end_idx]
        
        batch_features = []
        batch_valid_idx = []
        
        for idx, row in batch_df.iterrows():
            try:
                smiles_list = [row['L1'], row['L2'], row['L3']]
                # ä½¿ç”¨extract_combinationæ–¹æ³•åˆå¹¶å¤šä¸ªé…ä½“çš„ç‰¹å¾
                features = extractor.extract_combination(
                    smiles_list,
                    feature_type=feature_type,
                    combination_method=combination_method
                )
                if features is not None:
                    batch_features.append(features)
                    batch_valid_idx.append(idx)
            except Exception:
                continue
        
        if batch_features:
            features_list.extend(batch_features)
            valid_indices.extend(batch_valid_idx)
        
        # è¿›åº¦æ˜¾ç¤º
        if (batch_idx + 1) % 10 == 0 or batch_idx == n_batches - 1:
            print(f"  å¤„ç†è¿›åº¦: {end_idx}/{n_samples} ({100*end_idx/n_samples:.1f}%)")
    
    if features_list:
        X = np.vstack(features_list)
        df_valid = df.iloc[valid_indices].reset_index(drop=True)
        print(f"âœ… æˆåŠŸæå–ç‰¹å¾: {len(X):,} ä¸ªç»„åˆ")
        return X, df_valid
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸæå–ä»»ä½•ç‰¹å¾")
        return None, None

def batch_predict(X, model, batch_size=10000):
    """æ‰¹é‡é¢„æµ‹ï¼Œé¿å…å†…å­˜æº¢å‡º"""
    n_samples = len(X)
    n_batches = (n_samples + batch_size - 1) // batch_size
    
    predictions = []
    
    for batch_idx in range(n_batches):
        start_idx = batch_idx * batch_size
        end_idx = min((batch_idx + 1) * batch_size, n_samples)
        
        batch_X = X[start_idx:end_idx]
        batch_pred = model.predict(batch_X)
        predictions.append(batch_pred)
    
    return np.concatenate(predictions)

def load_trained_models(project_dir, model_name='xgboost'):
    """ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ¨¡å‹"""
    print("\nåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")
    
    project_path = Path(project_dir)
    model_dir = project_path / model_name / 'models'
    
    if not model_dir.exists():
        print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
        return {}
    
    models = {}
    targets = {
        'wavelength': ['wavelength', 'Max_wavelength'],
        'PLQY': ['PLQY', 'plqy'],
        'tau': ['tau', 'lifetime']
    }
    
    for target_key, patterns in targets.items():
        for pattern in patterns:
            model_files = list(model_dir.glob(f"*{pattern}*.joblib"))
            if model_files:
                model_file = sorted(model_files)[-1]
                print(f"  âœ… åŠ è½½ {target_key} æ¨¡å‹: {model_file.name}")
                models[target_key] = joblib.load(model_file)
                break
    
    return models

def predict_all_properties(X, df_valid, models):
    """ä½¿ç”¨æ‰€æœ‰æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
    print("\né¢„æµ‹åˆ†å­æ€§è´¨...")
    
    predictions = {}
    target_mapping = {
        'wavelength': 'Predicted_Max_wavelength(nm)',
        'PLQY': 'Predicted_PLQY',
        'tau': 'Predicted_tau(s*10^-6)'
    }
    
    for target_key, col_name in target_mapping.items():
        if target_key in models:
            print(f"  é¢„æµ‹ {target_key}...")
            pred = batch_predict(X, models[target_key])
            predictions[col_name] = pred
            
            # ç»Ÿè®¡
            print(f"    èŒƒå›´: [{pred.min():.3f}, {pred.max():.3f}]")
            print(f"    å‡å€¼: {pred.mean():.3f} Â± {pred.std():.3f}")
    
    # ä¸€æ¬¡æ€§æ·»åŠ æ‰€æœ‰é¢„æµ‹åˆ—
    for col_name, pred_values in predictions.items():
        df_valid[col_name] = pred_values
    
    return df_valid

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='é«˜æ•ˆç”Ÿæˆè™šæ‹Ÿæ•°æ®åº“')
    
    parser.add_argument('--data', '-d', default='../data/Database_normalized.csv',
                       help='åŸå§‹æ•°æ®æ–‡ä»¶')
    parser.add_argument('--project', '-p', default='paper_table',
                       help='è®­ç»ƒé¡¹ç›®ç›®å½•')
    parser.add_argument('--model', '-m', default='xgboost',
                       help='ä½¿ç”¨çš„æ¨¡å‹')
    parser.add_argument('--output', '-o', default='ir_assemble.csv',
                       help='è¾“å‡ºæ–‡ä»¶å')
    parser.add_argument('--feature-type', default='combined',
                       choices=['morgan', 'descriptors', 'combined'],
                       help='ç‰¹å¾ç±»å‹')
    parser.add_argument('--batch-size', type=int, default=1000,
                       help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--combination-method', default='mean',
                       choices=['mean', 'sum', 'concat'],
                       help='å¤šä¸ªé…ä½“ç‰¹å¾çš„åˆå¹¶æ–¹å¼')
    parser.add_argument('--descriptor-count', type=int, default=85,
                       help='åˆ†å­æè¿°ç¬¦æ•°é‡')
    
    args = parser.parse_args()
    
    start_time = datetime.now()
    
    print("=" * 60)
    print("é«˜æ•ˆç”Ÿæˆè™šæ‹Ÿæ•°æ®åº“")
    print("=" * 60)
    print(f"å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"åŸå§‹æ•°æ®: {args.data}")
    print(f"é¡¹ç›®ç›®å½•: {args.project}")
    print(f"ä½¿ç”¨æ¨¡å‹: {args.model}")
    
    # 1. åŠ è½½å¹¶å»é‡é…ä½“
    l12_unique, l3_unique, original_df = load_and_deduplicate_ligands(args.data)
    
    # 2. ç”Ÿæˆç»„åˆï¼ˆå‘é‡åŒ–ï¼‰
    assembled_df = generate_combinations_vectorized(l12_unique, l3_unique)
    
    # ä¿å­˜ç»„åˆæ–‡ä»¶
    combo_file = args.output.replace('.csv', '_combinations.csv')
    assembled_df.to_csv(combo_file, index=False)
    print(f"\nâœ… ç»„åˆæ–‡ä»¶å·²ä¿å­˜: {combo_file}")
    
    # 3. æ‰¹é‡æå–ç‰¹å¾
    X, df_valid = batch_extract_features(
        assembled_df,
        feature_type=args.feature_type,
        batch_size=args.batch_size,
        combination_method=args.combination_method,
        descriptor_count=args.descriptor_count
    )
    
    if X is None:
        print("âŒ ç‰¹å¾æå–å¤±è´¥")
        return
    
    # 4. åŠ è½½æ‰€æœ‰æ¨¡å‹
    models = load_trained_models(args.project, args.model)
    
    if not models:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹")
        return
    
    # 5. æ‰¹é‡é¢„æµ‹
    df_predicted = predict_all_properties(X, df_valid, models)
    
    # 6. ä¿å­˜ç»“æœ
    print("\nä¿å­˜è™šæ‹Ÿæ•°æ®åº“...")
    df_predicted.to_csv(args.output, index=False)
    print(f"âœ… è™šæ‹Ÿæ•°æ®åº“å·²ä¿å­˜: {args.output}")
    
    # 7. åˆ†æç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š è™šæ‹Ÿæ•°æ®åº“åˆ†æ")
    print("-" * 40)
    print(f"æ€»ç»„åˆæ•°: {len(df_predicted):,}")
    
    # æ‰¾å‡ºæœ€ä¼˜ç»„åˆ
    if 'Predicted_PLQY' in df_predicted.columns:
        # Top 10 PLQY
        top_plqy = df_predicted.nlargest(10, 'Predicted_PLQY')
        print(f"\nğŸ† Top 10 PLQYç»„åˆ:")
        for idx, row in top_plqy.iterrows():
            print(f"  #{idx+1}: PLQY={row['Predicted_PLQY']:.3f}")
            if idx == 0:  # æ˜¾ç¤ºæœ€ä½³ç»„åˆçš„è¯¦ç»†ä¿¡æ¯
                print(f"       L1=L2: {row['L1'][:40]}...")
                print(f"       L3: {row['L3'][:40]}...")
                if 'Predicted_Max_wavelength(nm)' in df_predicted.columns:
                    print(f"       Î»={row['Predicted_Max_wavelength(nm)']:.1f}nm")
    
    # ä¿å­˜Topå€™é€‰
    if 'Predicted_PLQY' in df_predicted.columns:
        top_file = args.output.replace('.csv', '_top1000.csv')
        top_candidates = df_predicted.nlargest(1000, 'Predicted_PLQY')
        top_candidates.to_csv(top_file, index=False)
        print(f"\nâœ… Top 1000å€™é€‰å·²ä¿å­˜: {top_file}")
    
    # æ—¶é—´ç»Ÿè®¡
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    print(f"\næ€»ç”¨æ—¶: {duration:.1f} ç§’")
    
    print("\n" + "=" * 60)
    print("âœ… è™šæ‹Ÿæ•°æ®åº“ç”Ÿæˆå®Œæˆï¼")
    print("=" * 60)

if __name__ == "__main__":
    main()
