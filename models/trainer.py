#!/usr/bin/env python3
"""
XGBoostè®­ç»ƒå™¨æ¨¡å—
æ”¯æŒæ•°æ®åŠ è½½ã€æ¨¡å‹è®­ç»ƒã€10æŠ˜äº¤å‰éªŒè¯
"""

# ========================================
#           å…¨å±€é…ç½®å‚æ•°
# ========================================

# æ•°æ®é…ç½®
DEFAULT_DATA_PATH = "data/Database_normalized.csv"  # é»˜è®¤æ•°æ®è·¯å¾„
SMILES_COLUMNS = ['L1', 'L2', 'L3']       # SMILESåˆ—å
TARGET_COLUMNS = ['Max_wavelength(nm)', 'PLQY', 'tau(s*10^-6)']  # ç›®æ ‡åˆ—å

# ç‰¹å¾é…ç½®
FEATURE_TYPE = 'combined'  # ç‰¹å¾ç±»å‹: 'morgan', 'descriptors', 'combined'
USE_CACHE = True           # æ˜¯å¦ä½¿ç”¨ç‰¹å¾ç¼“å­˜

# è®­ç»ƒé…ç½®
N_FOLDS = 10              # äº¤å‰éªŒè¯æŠ˜æ•°
RANDOM_STATE = 42         # éšæœºç§å­
TEST_SIZE = 0.2           # æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆä»…ç”¨äºtrain_test_splitæ¨¡å¼ï¼‰

# XGBoosté»˜è®¤å‚æ•°
XGB_PARAMS = {
    'objective': 'reg:squarederror',
    'eval_metric': 'rmse',
    'max_depth': 6,
    'learning_rate': 0.1,
    'n_estimators': 100,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'random_state': RANDOM_STATE,
    'n_jobs': -1,
    'verbosity': 1
}

# è¾“å‡ºé…ç½®
SAVE_MODELS = True        # æ˜¯å¦ä¿å­˜æ¨¡å‹
MODEL_DIR = "models"      # æ¨¡å‹ä¿å­˜ç›®å½•
RESULTS_DIR = "results"   # ç»“æœä¿å­˜ç›®å½•
SAVE_PREDICTIONS = True   # æ˜¯å¦ä¿å­˜é¢„æµ‹ç»“æœ

# ========================================
#           å¯¼å…¥ä¾èµ–åº“
# ========================================

import os
import sys
import argparse
import warnings
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import json
from typing import Dict, List, Tuple, Optional, Union
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# å¯¼å…¥ç‰¹å¾æå–æ¨¡å—
from core.feature_extractor import FeatureExtractor, MORGAN_BITS, DESCRIPTOR_NAMES

# å¯¼å…¥æ¨¡å‹æ¨¡å—
from models.base import XGBoostTrainer, ModelFactory, generate_model_filename, evaluate_model

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# ========================================
#           æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
# ========================================

class DataLoader:
    """æ•°æ®åŠ è½½å™¨ç±»"""
    
    def __init__(self, data_path: str = None, feature_type: str = None, use_cache: bool = None):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            feature_type: ç‰¹å¾ç±»å‹
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        """
        self.data_path = data_path or DEFAULT_DATA_PATH
        self.feature_type = feature_type or FEATURE_TYPE
        self.use_cache = use_cache if use_cache is not None else USE_CACHE
        
        # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
        self.feature_extractor = FeatureExtractor(use_cache=self.use_cache)
        
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆå§‹åŒ–")
        print(f"   æ•°æ®è·¯å¾„: {self.data_path}")
        print(f"   ç‰¹å¾ç±»å‹: {self.feature_type}")
        print(f"   ä½¿ç”¨ç¼“å­˜: {self.use_cache}")
    
    def load_data(self) -> pd.DataFrame:
        """åŠ è½½CSVæ•°æ®"""
        if not os.path.exists(self.data_path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.data_path}")
        
        df = pd.read_csv(self.data_path)
        print(f"\nğŸ“Š åŠ è½½æ•°æ®: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
        
        # æ£€æŸ¥å¿…éœ€çš„åˆ—
        missing_cols = []
        for col in SMILES_COLUMNS:
            if col not in df.columns:
                missing_cols.append(col)
        
        if missing_cols:
            raise ValueError(f"ç¼ºå°‘SMILESåˆ—: {missing_cols}")
        
        # æ£€æŸ¥ç›®æ ‡åˆ—
        available_targets = [col for col in TARGET_COLUMNS if col in df.columns]
        if not available_targets:
            raise ValueError(f"æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç›®æ ‡åˆ—: {TARGET_COLUMNS}")
        
        print(f"   SMILESåˆ—: {SMILES_COLUMNS}")
        print(f"   ç›®æ ‡åˆ—: {available_targets}")
        
        return df
    
    def extract_features(self, df: pd.DataFrame, show_progress: bool = True) -> np.ndarray:
        """
        æå–ç‰¹å¾
        
        Args:
            df: æ•°æ®æ¡†
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        
        Returns:
            ç‰¹å¾çŸ©é˜µ
        """
        print(f"\nğŸ”§ æå–{self.feature_type}ç‰¹å¾...")
        
        features = []
        iterator = tqdm(df.iterrows(), total=len(df), desc="æå–ç‰¹å¾") if show_progress else df.iterrows()
        
        for idx, row in iterator:
            # è·å–ä¸‰ä¸ªSMILES
            smiles_list = [row[col] if pd.notna(row[col]) else None for col in SMILES_COLUMNS]
            
            # æå–ç»„åˆç‰¹å¾
            feat = self.feature_extractor.extract_combination(
                smiles_list, 
                feature_type=self.feature_type,
                combination_method='mean'
            )
            
            features.append(feat)
        
        features = np.array(features)
        print(f"   ç‰¹å¾ç»´åº¦: {features.shape}")
        
        # å¤„ç†NaNå’ŒInf
        n_nan = np.isnan(features).sum()
        n_inf = np.isinf(features).sum()
        
        if n_nan > 0 or n_inf > 0:
            print(f"   âš ï¸ å‘ç° {n_nan} ä¸ªNaNå€¼, {n_inf} ä¸ªInfå€¼")
            features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
        
        return features
    
    def prepare_data(self, target_col: str) -> Tuple[np.ndarray, np.ndarray, pd.Index]:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®
        
        Args:
            target_col: ç›®æ ‡åˆ—å
        
        Returns:
            (ç‰¹å¾çŸ©é˜µ, ç›®æ ‡å€¼, æœ‰æ•ˆç´¢å¼•)
        """
        # åŠ è½½æ•°æ®
        df = self.load_data()
        
        # ç­›é€‰æœ‰æ•ˆæ•°æ®
        valid_mask = df[target_col].notna()
        for col in SMILES_COLUMNS:
            valid_mask &= df[col].notna()
        
        df_valid = df[valid_mask].copy()
        print(f"\nğŸ“Š ç›®æ ‡å˜é‡: {target_col}")
        print(f"   æœ‰æ•ˆæ ·æœ¬: {len(df_valid)}/{len(df)}")
        
        if len(df_valid) == 0:
            raise ValueError(f"æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
        
        # æå–ç‰¹å¾
        X = self.extract_features(df_valid)
        
        # è·å–ç›®æ ‡å€¼
        y = df_valid[target_col].values
        
        # å•ä½è½¬æ¢ï¼ˆå¦‚æœPLQYæ˜¯ç™¾åˆ†æ¯”ï¼‰
        if target_col == 'PLQY' and y.max() > 1.5:
            print("   è½¬æ¢PLQY: ç™¾åˆ†æ¯” â†’ å°æ•°")
            y = y / 100
        
        return X, y, df_valid.index

# ========================================
#           ç»“æœä¿å­˜è¾…åŠ©å‡½æ•°
# ========================================

def save_training_results(results: Dict, target_col: str, model_type: str, n_folds: int):
    """
    ä¿å­˜è®­ç»ƒç»“æœ
    
    Args:
        results: ç»“æœå­—å…¸
        target_col: ç›®æ ‡åˆ—å
        model_type: æ¨¡å‹ç±»å‹
        n_folds: äº¤å‰éªŒè¯æŠ˜æ•°
    """
    if not SAVE_PREDICTIONS:
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(RESULTS_DIR).mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜CVé¢„æµ‹ç»“æœ
    cv_df = pd.DataFrame({
        'true': results['true_values'],
        'predicted': results['predictions'],
        'error': results['true_values'] - results['predictions']
    })
    
    csv_file = Path(RESULTS_DIR) / f"cv_predictions_{model_type}_{target_col}_{timestamp}.csv"
    cv_df.to_csv(csv_file, index=False)
    print(f"   ğŸ’¾ é¢„æµ‹ç»“æœå·²ä¿å­˜: {csv_file}")
    
    # ä¿å­˜è¯„ä¼°æŒ‡æ ‡
    metrics = {
        'target': target_col,
        'model_type': model_type,
        'n_samples': len(results['true_values']),
        'n_folds': n_folds,
        'mean_rmse': float(results['mean_rmse']),
        'std_rmse': float(results['std_rmse']),
        'mean_mae': float(results['mean_mae']),
        'std_mae': float(results['std_mae']),
        'mean_r2': float(results['mean_r2']),
        'std_r2': float(results['std_r2']),
        'mean_mape': float(results['mean_mape']) if not np.isnan(results['mean_mape']) else None,
        'std_mape': float(results['std_mape']) if not np.isnan(results['std_mape']) else None,
        'timestamp': timestamp
    }
    
    json_file = Path(RESULTS_DIR) / f"cv_metrics_{model_type}_{target_col}_{timestamp}.json"
    with open(json_file, 'w') as f:
        json.dump(metrics, f, indent=2)
    print(f"   ğŸ’¾ è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜: {json_file}")

# ========================================
#           ä¸»è®­ç»ƒæµç¨‹
# ========================================

class Trainer:
    """ä¸»è®­ç»ƒå™¨ç±»"""
    
    def __init__(self, data_path: str = None, feature_type: str = None):
        """
        åˆå§‹åŒ–ä¸»è®­ç»ƒå™¨
        
        Args:
            data_path: æ•°æ®è·¯å¾„
            feature_type: ç‰¹å¾ç±»å‹
        """
        self.data_loader = DataLoader(data_path, feature_type)
        self.xgb_trainer = XGBoostTrainer()
        self.results = {}
    
    def train_target(self, target_col: str):
        """
        è®­ç»ƒå•ä¸ªç›®æ ‡
        
        Args:
            target_col: ç›®æ ‡åˆ—å
        """
        print(f"\n{'='*60}")
        print(f"è®­ç»ƒç›®æ ‡: {target_col}")
        print(f"{'='*60}")
        
        try:
            # å‡†å¤‡æ•°æ®
            X, y, idx = self.data_loader.prepare_data(target_col)
            
            # äº¤å‰éªŒè¯
            cv_results = self.xgb_trainer.train_cv(X, y)
            
            # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
            final_model = self.xgb_trainer.train_full(X, y)
            
            # ä¿å­˜æ¨¡å‹å’Œç»“æœ
            self.xgb_trainer.save_model(final_model, target_col, "_final")
            self.xgb_trainer.save_results(cv_results, target_col)
            
            # ä¿å­˜ç»“æœ
            self.results[target_col] = {
                'cv_results': cv_results,
                'final_model': final_model,
                'n_samples': len(y)
            }
            
            print(f"\nâœ… {target_col} è®­ç»ƒå®Œæˆ!")
            
        except Exception as e:
            print(f"\nâŒ {target_col} è®­ç»ƒå¤±è´¥: {e}")
            self.results[target_col] = {'error': str(e)}
    
    def train_all(self):
        """è®­ç»ƒæ‰€æœ‰ç›®æ ‡"""
        # åŠ è½½æ•°æ®ä»¥ç¡®å®šå¯ç”¨çš„ç›®æ ‡
        df = self.data_loader.load_data()
        available_targets = [col for col in TARGET_COLUMNS if col in df.columns]
        
        print(f"\nğŸ¯ å°†è®­ç»ƒ {len(available_targets)} ä¸ªç›®æ ‡: {available_targets}")
        
        for target in available_targets:
            self.train_target(target)
        
        # æ±‡æ€»ç»“æœ
        self.print_summary()
    
    def print_summary(self):
        """æ‰“å°è®­ç»ƒæ±‡æ€»"""
        print(f"\n{'='*60}")
        print("è®­ç»ƒæ±‡æ€»")
        print(f"{'='*60}")
        
        for target, result in self.results.items():
            if 'error' in result:
                print(f"\nâŒ {target}: å¤±è´¥ - {result['error']}")
            else:
                cv_res = result['cv_results']
                print(f"\nâœ… {target}:")
                print(f"   æ ·æœ¬æ•°: {result['n_samples']}")
                print(f"   RMSE: {cv_res['mean_rmse']:.4f} Â± {cv_res['std_rmse']:.4f}")
                print(f"   RÂ²:   {cv_res['mean_r2']:.4f} Â± {cv_res['std_r2']:.4f}")

# ========================================
#           å‘½ä»¤è¡Œæ¥å£
# ========================================

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='XGBoostè®­ç»ƒå™¨ - æ”¯æŒ10æŠ˜äº¤å‰éªŒè¯',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument('--data', '-d', type=str, default=DEFAULT_DATA_PATH,
                       help=f'æ•°æ®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: {DEFAULT_DATA_PATH})')
    parser.add_argument('--target', '-t', type=str,
                       help='æŒ‡å®šç›®æ ‡åˆ—ï¼ˆä¸æŒ‡å®šåˆ™è®­ç»ƒæ‰€æœ‰ï¼‰')
    parser.add_argument('--feature', '-f', type=str, default=FEATURE_TYPE,
                       choices=['morgan', 'descriptors', 'combined'],
                       help=f'ç‰¹å¾ç±»å‹ (é»˜è®¤: {FEATURE_TYPE})')
    parser.add_argument('--folds', '-k', type=int,
                       help=f'äº¤å‰éªŒè¯æŠ˜æ•° (é»˜è®¤: {N_FOLDS})')
    parser.add_argument('--no-cache', action='store_true',
                       help='ä¸ä½¿ç”¨ç‰¹å¾ç¼“å­˜')
    parser.add_argument('--no-save', action='store_true',
                       help='ä¸ä¿å­˜æ¨¡å‹å’Œç»“æœ')
    
    args = parser.parse_args()
    
    # è®¾ç½®å‚æ•°
    use_cache = not args.no_cache
    save_models = not args.no_save
    save_predictions = not args.no_save
    n_folds = args.folds if args.folds else N_FOLDS
    
    print("="*60)
    print("XGBoost åˆ†å­æ€§è´¨é¢„æµ‹è®­ç»ƒå™¨")
    print("="*60)
    print(f"\né…ç½®:")
    print(f"  æ•°æ®æ–‡ä»¶: {args.data}")
    print(f"  ç‰¹å¾ç±»å‹: {args.feature}")
    print(f"  äº¤å‰éªŒè¯: {n_folds}æŠ˜")
    print(f"  ä½¿ç”¨ç¼“å­˜: {use_cache}")
    print(f"  ä¿å­˜ç»“æœ: {save_models}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä¼ å…¥use_cacheå‚æ•°ï¼‰
    data_loader = DataLoader(args.data, args.feature, use_cache)
    
    # åˆ›å»ºXGBoostè®­ç»ƒå™¨ï¼ˆä¼ å…¥n_foldså‚æ•°ï¼‰
    xgb_trainer = XGBoostTrainer(n_folds=n_folds)
    
    # æ ¹æ®saveå‚æ•°è®¾ç½®
    if not save_models:
        xgb_trainer.save_model = lambda *args, **kwargs: None
        xgb_trainer.save_results = lambda *args, **kwargs: None
    
    # åˆ›å»ºä¸»è®­ç»ƒå™¨
    trainer = Trainer(args.data, args.feature)
    trainer.data_loader = data_loader
    trainer.xgb_trainer = xgb_trainer
    
    # è®­ç»ƒ
    if args.target:
        trainer.train_target(args.target)
    else:
        trainer.train_all()
    
    print("\nâœ¨ è®­ç»ƒå®Œæˆ!")

if __name__ == "__main__":
    main()