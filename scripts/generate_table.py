#!/usr/bin/env python3
"""
ç»Ÿä¸€è¡¨æ ¼ç”Ÿæˆè„šæœ¬ - ä»è®­ç»ƒç»“æœç”Ÿæˆæ€§èƒ½å¯¹æ¯”è¡¨æ ¼
"""

import os
import sys
import json
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
import argparse

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# æ¨¡å‹åç§°æ˜ å°„ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
MODEL_DISPLAY_NAMES = {
    'adaboost': 'AdaBoost',
    'ada_boost': 'AdaBoost',
    'catboost': 'CatBoost',
    'decision_tree': 'Decision Tree',
    'elastic_net': 'Elastic Net',
    'extra_trees': 'Extra Trees',
    'gradient_boosting': 'Gradient Boosting',
    'knn': 'K-Nearest Neighbors',
    'lasso': 'Lasso',
    'lightgbm': 'LightGBM',
    'random_forest': 'Random Forest',
    'ridge': 'Ridge',
    'xgboost': 'XGBoost',
    'svr': 'Support Vector Machine'
}

def load_model_results(project_path, model_name):
    """åŠ è½½å•ä¸ªæ¨¡å‹çš„ç»“æœ"""
    results = {}
    
    # æŸ¥æ‰¾æ¨¡å‹ç›®å½•
    model_dirs = [
        project_path / model_name,
        project_path / f"{model_name}_standard",
        project_path / model_name.replace('_', '')
    ]
    
    model_dir = None
    for dir_path in model_dirs:
        if dir_path.exists():
            model_dir = dir_path
            break
    
    if not model_dir:
        print(f"  âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹ç›®å½•: {model_name}")
        return results
    
    # é¦–å…ˆå°è¯•ä»exportsç›®å½•è¯»å–summary.jsonæ–‡ä»¶
    exports_dir = model_dir / 'exports'
    if exports_dir.exists():
        summary_files = list(exports_dir.glob("*_summary.json"))
        for summary_file in summary_files:
            try:
                with open(summary_file, 'r') as f:
                    data = json.load(f)
                
                target = data.get('target', '')
                
                # è·å–æŒ‡æ ‡
                if 'mean_r2' in data:
                    results[target] = {
                        'r2_mean': data['mean_r2'],
                        'r2_std': data.get('std_r2', 0),
                        'rmse_mean': data.get('mean_rmse', 0),
                        'rmse_std': data.get('std_rmse', 0),
                        'mae_mean': data.get('mean_mae', 0),
                        'mae_std': data.get('std_mae', 0),
                        'n_folds': data.get('n_folds', 10)
                    }
            except Exception as e:
                print(f"  âš ï¸ è¯»å–summaryæ–‡ä»¶å¤±è´¥ {summary_file}: {e}")
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°summaryæ–‡ä»¶ï¼Œå°è¯•æ—§æ ¼å¼
    if not results:
        # éå†ç›®æ ‡ç›®å½•
        for target_dir in model_dir.iterdir():
            if not target_dir.is_dir():
                continue
            
            # æŸ¥æ‰¾ç»“æœæ–‡ä»¶
            result_files = list(target_dir.glob("**/experiment_results.json"))
            if not result_files:
                continue
            
            # è¯»å–ç»“æœ
            try:
                with open(result_files[0], 'r') as f:
                    data = json.load(f)
                
                target = data.get('target_column', target_dir.name)
                cv_results = data.get('cv_results', {})
                
                # æå–æŒ‡æ ‡
                r2_scores = []
                rmse_scores = []
                mae_scores = []
                
                for fold_result in cv_results.values():
                    if isinstance(fold_result, dict):
                        if 'r2' in fold_result:
                            r2_scores.append(fold_result['r2'])
                        if 'rmse' in fold_result:
                            rmse_scores.append(fold_result['rmse'])
                        if 'mae' in fold_result:
                            mae_scores.append(fold_result['mae'])
                
                if r2_scores:
                    results[target] = {
                        'r2_mean': np.mean(r2_scores),
                        'r2_std': np.std(r2_scores),
                        'rmse_mean': np.mean(rmse_scores),
                        'rmse_std': np.std(rmse_scores),
                        'mae_mean': np.mean(mae_scores),
                        'mae_std': np.std(mae_scores),
                        'n_folds': len(r2_scores)
                    }
                    
            except Exception as e:
                print(f"  âš ï¸ è¯»å–ç»“æœå¤±è´¥ {result_files[0]}: {e}")
    
    return results

def generate_performance_table(project_name, output_format='all', target_names=None):
    """ç”Ÿæˆæ€§èƒ½å¯¹æ¯”è¡¨æ ¼"""
    
    project_path = Path(project_name)
    if not project_path.exists():
        print(f"âŒ é¡¹ç›®ç›®å½•ä¸å­˜åœ¨: {project_name}")
        return None
    
    print("=" * 60)
    print(f"ç”Ÿæˆæ€§èƒ½è¡¨æ ¼")
    print(f"é¡¹ç›®: {project_name}")
    print("=" * 60)
    
    # é»˜è®¤ç›®æ ‡åç§°æ˜ å°„
    if not target_names:
        target_names = {
            'Max_wavelength(nm)': 'Î»em (nm)',
            'Max_wavelengthnm': 'Î»em (nm)',
            'PLQY': 'PLQY',
            'tau(s*10^-6)': 'Ï„ (Î¼s)',
            'tausx10^-6': 'Ï„ (Î¼s)'
        }
    
    # æ”¶é›†æ‰€æœ‰ç»“æœ
    all_results = []
    
    # éå†æ¯ä¸ªæ¨¡å‹
    print("\næ”¶é›†æ¨¡å‹ç»“æœ...")
    for model_dir in project_path.iterdir():
        if not model_dir.is_dir() or model_dir.name.startswith('.'):
            continue
        
        # è·³è¿‡éæ¨¡å‹ç›®å½•
        if model_dir.name in ['tables', 'logs', 'last', 'checkpoints', 'tmp']:
            continue
        
        model_name = model_dir.name.replace('_standard', '')
        print(f"  å¤„ç†: {model_name}")
        
        results = load_model_results(project_path, model_dir.name)
        
        for target, metrics in results.items():
            # ç¡®å®šæ˜¾ç¤ºçš„ç›®æ ‡åç§°
            display_target = target
            for key, value in target_names.items():
                if key in target or key.replace('_', '').replace('(', '').replace(')', '') in target.replace('_', '').replace('(', '').replace(')', ''):
                    display_target = value
                    break
            
            # è·å–æ¨¡å‹æ˜¾ç¤ºåç§°
            display_model = MODEL_DISPLAY_NAMES.get(model_name, model_name)
            
            # æ·»åŠ ç»“æœ
            all_results.append({
                'Target': display_target,
                'Model': display_model,
                'RÂ²': f"{metrics['r2_mean']:.4f} Â± {metrics['r2_std']:.4f}",
                'RMSE': f"{metrics['rmse_mean']:.2f} Â± {metrics['rmse_std']:.2f}",
                'MAE': f"{metrics['mae_mean']:.2f} Â± {metrics['mae_std']:.2f}",
                'Folds': metrics['n_folds']
            })
    
    if not all_results:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç»“æœ")
        return None
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(all_results)
    
    # æŒ‰ç›®æ ‡å’Œæ¨¡å‹æ’åº
    df = df.sort_values(['Target', 'Model'])
    
    # è¾“å‡ºç›®å½•
    output_dir = project_path / 'tables'
    output_dir.mkdir(exist_ok=True)
    
    # ç”Ÿæˆä¸åŒæ ¼å¼çš„è¡¨æ ¼
    outputs = []
    
    if output_format in ['all', 'console']:
        print("\n" + "=" * 80)
        print("ğŸ“Š æ€§èƒ½å¯¹æ¯”è¡¨æ ¼")
        print("=" * 80)
        
        for target in df['Target'].unique():
            target_df = df[df['Target'] == target]
            print(f"\n{target}:")
            print("-" * 60)
            print(target_df[['Model', 'RÂ²', 'RMSE', 'MAE']].to_string(index=False))
            
            # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
            best_r2 = -1
            best_model = None
            for _, row in target_df.iterrows():
                r2_value = float(row['RÂ²'].split(' Â± ')[0])
                if r2_value > best_r2:
                    best_r2 = r2_value
                    best_model = row['Model']
            
            if best_model:
                print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model} (RÂ² = {best_r2:.4f})")
    
    if output_format in ['all', 'csv']:
        csv_file = output_dir / 'performance_table.csv'
        df.to_csv(csv_file, index=False)
        outputs.append(f"CSV: {csv_file}")
        print(f"\nâœ… å·²ä¿å­˜CSVè¡¨æ ¼: {csv_file}")
    
    if output_format in ['all', 'excel']:
        try:
            excel_file = output_dir / 'performance_table.xlsx'
            with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
                # æ€»è¡¨
                df.to_excel(writer, sheet_name='All Results', index=False)
                
                # æ¯ä¸ªç›®æ ‡å•ç‹¬ä¸€ä¸ªsheet
                for target in df['Target'].unique():
                    target_df = df[df['Target'] == target]
                    sheet_name = target.replace('/', '_').replace('(', '').replace(')', '')[:31]
                    target_df.to_excel(writer, sheet_name=sheet_name, index=False)
            
            outputs.append(f"Excel: {excel_file}")
            print(f"âœ… å·²ä¿å­˜Excelè¡¨æ ¼: {excel_file}")
        except ImportError:
            print("âš ï¸ éœ€è¦å®‰è£…openpyxl: pip install openpyxl")
    
    if output_format in ['all', 'markdown', 'md']:
        md_file = output_dir / 'performance_table.md'
        with open(md_file, 'w') as f:
            f.write("# Model Performance Comparison\n\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            for target in df['Target'].unique():
                f.write(f"\n## {target}\n\n")
                target_df = df[df['Target'] == target]
                
                f.write("| Model | RÂ² | RMSE | MAE |\n")
                f.write("|-------|-----|------|-----|\n")
                
                for _, row in target_df.iterrows():
                    f.write(f"| {row['Model']} | {row['RÂ²']} | {row['RMSE']} | {row['MAE']} |\n")
                
                # æ‰¾æœ€ä½³æ¨¡å‹
                best_r2 = -1
                best_model = None
                for _, row in target_df.iterrows():
                    r2_value = float(row['RÂ²'].split(' Â± ')[0])
                    if r2_value > best_r2:
                        best_r2 = r2_value
                        best_model = row['Model']
                
                if best_model:
                    f.write(f"\n**Best Model**: {best_model} (RÂ² = {best_r2:.4f})\n")
        
        outputs.append(f"Markdown: {md_file}")
        print(f"âœ… å·²ä¿å­˜Markdownè¡¨æ ¼: {md_file}")
    
    if output_format in ['all', 'latex', 'tex']:
        latex_file = output_dir / 'performance_table.tex'
        with open(latex_file, 'w') as f:
            # LaTeXè¡¨æ ¼å¤´
            f.write("% Performance Comparison Table\n")
            f.write("% Generated: " + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + "\n\n")
            
            for target in df['Target'].unique():
                f.write(f"% {target}\n")
                f.write("\\begin{table}[htbp]\n")
                f.write("\\centering\n")
                f.write(f"\\caption{{Model Performance for {target}}}\n")
                f.write("\\begin{tabular}{lccc}\n")
                f.write("\\hline\n")
                f.write("Model & RÂ² & RMSE & MAE \\\\\n")
                f.write("\\hline\n")
                
                target_df = df[df['Target'] == target]
                for _, row in target_df.iterrows():
                    # è½¬ä¹‰LaTeXç‰¹æ®Šå­—ç¬¦
                    model = row['Model'].replace('_', '\\_')
                    f.write(f"{model} & {row['RÂ²']} & {row['RMSE']} & {row['MAE']} \\\\\n")
                
                f.write("\\hline\n")
                f.write("\\end{tabular}\n")
                f.write("\\end{table}\n\n")
        
        outputs.append(f"LaTeX: {latex_file}")
        print(f"âœ… å·²ä¿å­˜LaTeXè¡¨æ ¼: {latex_file}")
    
    print("\n" + "=" * 60)
    print("âœ… è¡¨æ ¼ç”Ÿæˆå®Œæˆï¼")
    if outputs:
        print("\nè¾“å‡ºæ–‡ä»¶:")
        for output in outputs:
            print(f"  - {output}")
    print("=" * 60)
    
    return df

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç”Ÿæˆæ€§èƒ½å¯¹æ¯”è¡¨æ ¼')
    
    parser.add_argument('project', help='é¡¹ç›®ç›®å½•åç§°')
    parser.add_argument('--format', '-f', default='all',
                       choices=['all', 'console', 'csv', 'excel', 'markdown', 'md', 'latex', 'tex'],
                       help='è¾“å‡ºæ ¼å¼')
    parser.add_argument('--targets', '-t', nargs='+',
                       help='è‡ªå®šä¹‰ç›®æ ‡åç§°æ˜ å°„ (æ ¼å¼: åŸåç§°=æ˜¾ç¤ºåç§°)')
    
    args = parser.parse_args()
    
    # è§£æè‡ªå®šä¹‰ç›®æ ‡åç§°
    target_names = None
    if args.targets:
        target_names = {}
        for mapping in args.targets:
            if '=' in mapping:
                orig, display = mapping.split('=', 1)
                target_names[orig] = display
    
    # ç”Ÿæˆè¡¨æ ¼
    df = generate_performance_table(args.project, args.format, target_names)
    
    if df is None:
        sys.exit(1)

if __name__ == "__main__":
    main()