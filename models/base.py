#!/usr/bin/env python3
"""
æœºå™¨å­¦ä¹ æ¨¡å‹æ¨¡å—
åŒ…å«å„ç§MLæ¨¡å‹çš„å®ç°å’Œè®­ç»ƒé€»è¾‘
"""

import numpy as np
import inspect
from typing import Dict, List, Tuple, Optional, Union
import joblib
from pathlib import Path
from datetime import datetime

# æœºå™¨å­¦ä¹ ç›¸å…³
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neural_network import MLPRegressor
import lightgbm as lgb
import catboost as cb
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler


# ========================================
#           æ¨¡å‹é»˜è®¤å‚æ•°é…ç½®
# ========================================

MODEL_PARAMS = {
    'xgboost': {
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse',
        'max_depth': 6,
        'learning_rate': 0.1,
        'n_estimators': 100,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'random_state': 42,
        'n_jobs': -1,
        'verbosity': 1
    },
    'lightgbm': {
        'objective': 'regression',
        'metric': 'rmse',
        'num_leaves': 31,
        'learning_rate': 0.1,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'n_estimators': 100,
        'random_state': 42,
        'n_jobs': -1,
        'verbosity': -1
    },
    'catboost': {
        'loss_function': 'RMSE',
        'iterations': 100,
        'learning_rate': 0.1,
        'depth': 6,
        'random_state': 42,
        'verbose': False
    },
    'random_forest': {
        'n_estimators': 100,
        'max_depth': None,
        'min_samples_split': 2,
        'min_samples_leaf': 1,
        'random_state': 42,
        'n_jobs': -1
    },
    'gradient_boosting': {
        'n_estimators': 100,
        'learning_rate': 0.1,
        'max_depth': 3,
        'random_state': 42
    },
    'adaboost': {
        'n_estimators': 300,
        'learning_rate': 0.3,
        'loss': 'square',
        'random_state': 42
    },
    'extra_trees': {
        'n_estimators': 100,
        'max_depth': None,
        'min_samples_split': 2,
        'min_samples_leaf': 1,
        'random_state': 42,
        'n_jobs': -1
    },
    'svr': {
        'kernel': 'rbf',
        'C': 100.0,
        'epsilon': 0.01,
        'gamma': 'scale',
        'cache_size': 1000,
        'max_iter': 5000
    },
    'knn': {
        'n_neighbors': 15,
        'weights': 'distance',
        'algorithm': 'ball_tree',
        'leaf_size': 20,
        'p': 2,
        'metric': 'minkowski',
        'n_jobs': -1
    },
    'decision_tree': {
        'max_depth': 15,
        'min_samples_split': 10,
        'min_samples_leaf': 5,
        'max_features': 'sqrt',
        'random_state': 42
    },
    'ridge': {
        'alpha': 1.0,
        'random_state': 42
    },
    'lasso': {
        'alpha': 0.01,
        'max_iter': 5000,
        'tol': 0.0001,
        'selection': 'random',
        'random_state': 42
    },
    'elastic_net': {
        'alpha': 0.01,
        'l1_ratio': 0.3,
        'max_iter': 5000,
        'tol': 0.0001,
        'selection': 'random',
        'random_state': 42
    },
    'mlp': {
        'hidden_layer_sizes': (256, 128),  # ä¼˜åŒ–åçš„ç½‘ç»œç»“æ„ï¼Œå¹³è¡¡æ€§èƒ½å’Œè¿‡æ‹Ÿåˆé£é™©
        'activation': 'relu',
        'solver': 'adam',
        'alpha': 0.001,  # é™ä½æ­£åˆ™åŒ–å¼ºåº¦ï¼Œå…è®¸æ›´å¥½æ‹Ÿåˆ
        'batch_size': 128,  # å›ºå®šæ‰¹æ¬¡å¤§å°ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§
        'learning_rate': 'adaptive',  # è‡ªé€‚åº”å­¦ä¹ ç‡ç­–ç•¥
        'learning_rate_init': 0.0005,  # é€‚ä¸­çš„åˆå§‹å­¦ä¹ ç‡
        'max_iter': 2000,  # å¢åŠ æœ€å¤§è¿­ä»£æ¬¡æ•°
        'random_state': 42,
        'early_stopping': True,
        'validation_fraction': 0.2,  # å¢åŠ éªŒè¯é›†æ¯”ä¾‹
        'n_iter_no_change': 50,  # å¢åŠ æ—©åœè€å¿ƒå€¼
        'tol': 0.0001  # é™ä½æ”¶æ•›å®¹å¿åº¦ï¼Œæå‡ç²¾åº¦
    }
}


# ========================================
#           åŸºç¡€æ¨¡å‹ç±»
# ========================================

class BaseModel:
    """åŸºç¡€æ¨¡å‹ç±»"""
    
    def __init__(self, model_type: str, params: Dict = None):
        """
        åˆå§‹åŒ–æ¨¡å‹
        
        Args:
            model_type: æ¨¡å‹ç±»å‹
            params: æ¨¡å‹å‚æ•°
        """
        self.model_type = model_type
        # è·å–é»˜è®¤å‚æ•°
        default_params = MODEL_PARAMS.get(model_type, {}).copy()
        
        # å¦‚æœæä¾›äº†paramsï¼Œåªä½¿ç”¨å¯¹è¯¥æ¨¡å‹æœ‰æ•ˆçš„å‚æ•°
        if params:
            # è¿‡æ»¤å‡ºåªå¯¹è¯¥æ¨¡å‹æœ‰æ•ˆçš„å‚æ•°
            valid_params = {}
            for key, value in params.items():
                # åªæ·»åŠ åœ¨é»˜è®¤å‚æ•°ä¸­å­˜åœ¨çš„é”®
                if key in default_params:
                    valid_params[key] = value
            # æ›´æ–°é»˜è®¤å‚æ•°
            default_params.update(valid_params)
        
        self.params = default_params
        self.model = None
        self.is_trained = False
        self.scaler = None
        # SVRã€KNNå’ŒMLPéœ€è¦æ•°æ®æ ‡å‡†åŒ–
        self.needs_scaling = model_type in ['svr', 'knn', 'mlp']
        # MLPè¿˜éœ€è¦å¯¹ç›®æ ‡å€¼æ ‡å‡†åŒ–
        self.needs_target_scaling = model_type in ['mlp']
        self.target_scaler = None
        
    def create_model(self):
        """åˆ›å»ºæ¨¡å‹å®ä¾‹"""
        if self.model_type == 'xgboost':
            self.model = xgb.XGBRegressor(**self.params)
        elif self.model_type == 'lightgbm':
            self.model = lgb.LGBMRegressor(**self.params)
        elif self.model_type == 'catboost':
            self.model = cb.CatBoostRegressor(**self.params)
        elif self.model_type == 'random_forest':
            self.model = RandomForestRegressor(**self.params)
        elif self.model_type == 'gradient_boosting':
            self.model = GradientBoostingRegressor(**self.params)
        elif self.model_type == 'adaboost':
            self.model = AdaBoostRegressor(**self.params)
        elif self.model_type == 'extra_trees':
            self.model = ExtraTreesRegressor(**self.params)
        elif self.model_type == 'svr':
            self.model = SVR(**self.params)
        elif self.model_type == 'knn':
            self.model = KNeighborsRegressor(**self.params)
        elif self.model_type == 'decision_tree':
            self.model = DecisionTreeRegressor(**self.params)
        elif self.model_type == 'ridge':
            self.model = Ridge(**self.params)
        elif self.model_type == 'lasso':
            self.model = Lasso(**self.params)
        elif self.model_type == 'elastic_net':
            self.model = ElasticNet(**self.params)
        elif self.model_type == 'mlp':
            self.model = MLPRegressor(**self.params)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
        
        return self.model
    
    def fit(self, X, y, **kwargs):
        """è®­ç»ƒæ¨¡å‹"""
        if self.model is None:
            self.create_model()

        # å¯¹SVRã€KNNå’ŒMLPè¿›è¡Œæ•°æ®æ ‡å‡†åŒ–
        if self.needs_scaling:
            self.scaler = StandardScaler()
            X = self.scaler.fit_transform(X)

        # MLPè¿˜éœ€è¦å¯¹ç›®æ ‡å€¼è¿›è¡Œæ ‡å‡†åŒ–
        if self.needs_target_scaling:
            self.target_scaler = StandardScaler()
            y = self.target_scaler.fit_transform(y.reshape(-1, 1)).ravel()
        
        # ç‰¹æ®Šå¤„ç†æŸäº›æ¨¡å‹çš„è®­ç»ƒå‚æ•°
        if self.model_type == 'xgboost' and 'eval_set' in kwargs:
            fit_fn = getattr(self.model, 'fit')
            sig = inspect.signature(fit_fn)
            fit_kwargs = {
                'eval_set': kwargs['eval_set'],
                'verbose': kwargs.get('verbose', False)
            }
            es_rounds = kwargs.get('early_stopping_rounds', None)
            # Prefer callbacks if supported
            if 'callbacks' in sig.parameters and es_rounds:
                try:
                    import xgboost as xgb
                    fit_kwargs['callbacks'] = [xgb.callback.EarlyStopping(rounds=es_rounds, save_best=True)]
                except Exception:
                    pass
            # Fallback to early_stopping_rounds if supported
            if 'early_stopping_rounds' in sig.parameters and es_rounds and 'callbacks' not in fit_kwargs:
                fit_kwargs['early_stopping_rounds'] = es_rounds
            # Call fit with supported args only
            self.model.fit(X, y, **fit_kwargs)
        elif self.model_type == 'lightgbm' and 'eval_set' in kwargs:
            fit_fn = getattr(self.model, 'fit')
            sig = inspect.signature(fit_fn)
            fit_kwargs = {
                'eval_set': kwargs['eval_set']
            }
            # Handle verbosity
            if 'verbose' in sig.parameters:
                fit_kwargs['verbose'] = kwargs.get('verbose', False)
            # Early stopping preference: callbacks -> param
            es_rounds = kwargs.get('early_stopping_rounds', None)
            if es_rounds:
                if 'callbacks' in sig.parameters:
                    cb = []
                    try:
                        cb.append(lgb.early_stopping(es_rounds, verbose=False))
                        if not kwargs.get('verbose', False):
                            cb.append(lgb.log_evaluation(0))
                    except Exception:
                        pass
                    if cb:
                        fit_kwargs['callbacks'] = cb
                if 'early_stopping_rounds' in sig.parameters and 'callbacks' not in fit_kwargs:
                    fit_kwargs['early_stopping_rounds'] = es_rounds
            self.model.fit(X, y, **fit_kwargs)
        elif self.model_type == 'catboost':
            self.model.fit(X, y, verbose=kwargs.get('verbose', False))
        else:
            self.model.fit(X, y)
        
        self.is_trained = True
        return self.model
    
    def predict(self, X):
        """é¢„æµ‹"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")

        # å¦‚æœè®­ç»ƒæ—¶ä½¿ç”¨äº†æ ‡å‡†åŒ–ï¼Œé¢„æµ‹æ—¶ä¹Ÿè¦æ ‡å‡†åŒ–
        if self.needs_scaling and self.scaler is not None:
            X = self.scaler.transform(X)

        predictions = self.model.predict(X)

        # å¦‚æœå¯¹ç›®æ ‡å€¼è¿›è¡Œäº†æ ‡å‡†åŒ–ï¼Œéœ€è¦åæ ‡å‡†åŒ–
        if self.needs_target_scaling and self.target_scaler is not None:
            predictions = self.target_scaler.inverse_transform(predictions.reshape(-1, 1)).ravel()

        return predictions
    
    def save(self, filepath: Union[str, Path]):
        """ä¿å­˜æ¨¡å‹"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")
        
        # å¦‚æœæœ‰scaleræˆ–target_scalerï¼Œä¸€èµ·ä¿å­˜
        if self.scaler is not None or self.target_scaler is not None:
            save_dict = {
                'model': self.model,
                'scaler': self.scaler,
                'target_scaler': self.target_scaler,
                'model_type': self.model_type
            }
            joblib.dump(save_dict, filepath)
        else:
            joblib.dump(self.model, filepath)
    
    def load(self, filepath: Union[str, Path]):
        """åŠ è½½æ¨¡å‹"""
        loaded = joblib.load(filepath)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«scaler
        if isinstance(loaded, dict) and 'model' in loaded:
            self.model = loaded['model']
            self.scaler = loaded.get('scaler', None)
        else:
            self.model = loaded
            self.scaler = None
        
        self.is_trained = True
        return self.model


# ========================================
#           XGBoostä¸“ç”¨è®­ç»ƒå™¨
# ========================================

class XGBoostTrainer:
    """XGBoostè®­ç»ƒå™¨ç±»"""
    
    def __init__(self, params: Dict = None, n_folds: int = 10):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            params: XGBoostå‚æ•°
            n_folds: äº¤å‰éªŒè¯æŠ˜æ•°
        """
        self.params = params or MODEL_PARAMS['xgboost'].copy()
        self.n_folds = n_folds
        self.models = []
        self.cv_results = []
        self.best_model = None
        
        print(f"\nâœ… XGBoostè®­ç»ƒå™¨åˆå§‹åŒ–")
        print(f"   äº¤å‰éªŒè¯: {self.n_folds}æŠ˜")
        print(f"   XGBoostå‚æ•°:")
        for key, value in self.params.items():
            print(f"     {key}: {value}")
    
    def train_cv(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """
        æ‰§è¡ŒKæŠ˜äº¤å‰éªŒè¯è®­ç»ƒ
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            y: ç›®æ ‡å€¼
        
        Returns:
            äº¤å‰éªŒè¯ç»“æœ
        """
        print(f"\nğŸš€ å¼€å§‹{self.n_folds}æŠ˜äº¤å‰éªŒè¯è®­ç»ƒ...")
        
        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.params.get('random_state', 42))
        
        cv_scores = {
            'rmse': [],
            'mae': [],
            'r2': [],
            'mape': []
        }
        
        all_predictions = np.zeros_like(y)
        fold_models = []
        
        for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):
            print(f"\n  æŠ˜ {fold}/{self.n_folds}:")
            
            # åˆ†å‰²æ•°æ®
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            
            # è®­ç»ƒæ¨¡å‹
            model = xgb.XGBRegressor(**self.params)
            model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                verbose=False
            )
            
            # é¢„æµ‹
            y_pred = model.predict(X_val)
            all_predictions[val_idx] = y_pred
            
            # è®¡ç®—æŒ‡æ ‡
            rmse = np.sqrt(mean_squared_error(y_val, y_pred))
            mae = mean_absolute_error(y_val, y_pred)
            r2 = r2_score(y_val, y_pred)
            
            # MAPE (é¿å…é™¤é›¶)
            mask = y_val != 0
            if mask.sum() > 0:
                mape = np.mean(np.abs((y_val[mask] - y_pred[mask]) / y_val[mask])) * 100
            else:
                mape = np.nan
            
            cv_scores['rmse'].append(rmse)
            cv_scores['mae'].append(mae)
            cv_scores['r2'].append(r2)
            cv_scores['mape'].append(mape)
            
            fold_models.append(model)
            
            print(f"    RMSE: {rmse:.4f}")
            print(f"    MAE:  {mae:.4f}")
            print(f"    RÂ²:   {r2:.4f}")
            if not np.isnan(mape):
                print(f"    MAPE: {mape:.2f}%")
        
        # ä¿å­˜æ¨¡å‹
        self.models = fold_models
        
        # è®¡ç®—å¹³å‡å¾—åˆ†
        results = {
            'cv_scores': cv_scores,
            'mean_rmse': np.mean(cv_scores['rmse']),
            'std_rmse': np.std(cv_scores['rmse']),
            'mean_mae': np.mean(cv_scores['mae']),
            'std_mae': np.std(cv_scores['mae']),
            'mean_r2': np.mean(cv_scores['r2']),
            'std_r2': np.std(cv_scores['r2']),
            'mean_mape': np.nanmean(cv_scores['mape']),
            'std_mape': np.nanstd(cv_scores['mape']),
            'predictions': all_predictions,
            'true_values': y
        }
        
        self.cv_results = results
        
        print(f"\nğŸ“Š äº¤å‰éªŒè¯ç»“æœæ±‡æ€»:")
        print(f"   RMSE: {results['mean_rmse']:.4f} Â± {results['std_rmse']:.4f}")
        print(f"   MAE:  {results['mean_mae']:.4f} Â± {results['std_mae']:.4f}")
        print(f"   RÂ²:   {results['mean_r2']:.4f} Â± {results['std_r2']:.4f}")
        if not np.isnan(results['mean_mape']):
            print(f"   MAPE: {results['mean_mape']:.2f}% Â± {results['std_mape']:.2f}%")
        
        return results
    
    def train_full(self, X: np.ndarray, y: np.ndarray) -> xgb.XGBRegressor:
        """
        åœ¨å…¨éƒ¨æ•°æ®ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡å‹
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            y: ç›®æ ‡å€¼
        
        Returns:
            è®­ç»ƒå¥½çš„æ¨¡å‹
        """
        print(f"\nğŸ¯ è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆå…¨éƒ¨æ•°æ®ï¼‰...")
        
        model = xgb.XGBRegressor(**self.params)
        model.fit(X, y, verbose=False)
        
        self.best_model = model
        
        # è®¡ç®—è®­ç»ƒé›†æŒ‡æ ‡
        y_pred = model.predict(X)
        train_rmse = np.sqrt(mean_squared_error(y, y_pred))
        train_r2 = r2_score(y, y_pred)
        
        print(f"   è®­ç»ƒRMSE: {train_rmse:.4f}")
        print(f"   è®­ç»ƒRÂ²:   {train_r2:.4f}")
        
        return model
    
    def save_model(self, model: xgb.XGBRegressor, filepath: Union[str, Path]):
        """
        ä¿å­˜æ¨¡å‹
        
        Args:
            model: æ¨¡å‹å¯¹è±¡
            filepath: ä¿å­˜è·¯å¾„
        """
        joblib.dump(model, filepath)
        print(f"   ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {filepath}")
        
        return filepath


# ========================================
#           é€šç”¨æ¨¡å‹è®­ç»ƒå™¨
# ========================================

class ModelTrainer:
    """é€šç”¨æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model_type: str, params: Dict = None, n_folds: int = 10):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            model_type: æ¨¡å‹ç±»å‹
            params: æ¨¡å‹å‚æ•°
            n_folds: äº¤å‰éªŒè¯æŠ˜æ•°
        """
        self.model_type = model_type
        self.params = params or MODEL_PARAMS.get(model_type, {}).copy()
        self.n_folds = n_folds
        self.models = []
        self.cv_results = []
        self.best_model = None
        
        print(f"\nâœ… {model_type.upper()}è®­ç»ƒå™¨åˆå§‹åŒ–")
        print(f"   äº¤å‰éªŒè¯: {self.n_folds}æŠ˜")
    
    def train_cv(self, X: np.ndarray, y: np.ndarray, verbose: bool = True) -> Dict:
        """
        æ‰§è¡ŒKæŠ˜äº¤å‰éªŒè¯è®­ç»ƒ
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            y: ç›®æ ‡å€¼
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        
        Returns:
            äº¤å‰éªŒè¯ç»“æœ
        """
        if verbose:
            print(f"\nğŸš€ å¼€å§‹{self.n_folds}æŠ˜äº¤å‰éªŒè¯è®­ç»ƒ...")
        
        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)
        
        cv_scores = {
            'rmse': [],
            'mae': [],
            'r2': [],
            'mape': []
        }
        
        all_predictions = np.zeros_like(y)
        fold_models = []
        
        for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):
            if verbose:
                print(f"\n  æŠ˜ {fold}/{self.n_folds}:")
            
            # åˆ†å‰²æ•°æ®
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            
            # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
            model = BaseModel(self.model_type, self.params)
            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
            
            # é¢„æµ‹
            y_pred = model.predict(X_val)
            all_predictions[val_idx] = y_pred
            
            # è®¡ç®—æŒ‡æ ‡
            rmse = np.sqrt(mean_squared_error(y_val, y_pred))
            mae = mean_absolute_error(y_val, y_pred)
            r2 = r2_score(y_val, y_pred)
            
            # MAPE (é¿å…é™¤é›¶)
            mask = y_val != 0
            if mask.sum() > 0:
                mape = np.mean(np.abs((y_val[mask] - y_pred[mask]) / y_val[mask])) * 100
            else:
                mape = np.nan
            
            cv_scores['rmse'].append(rmse)
            cv_scores['mae'].append(mae)
            cv_scores['r2'].append(r2)
            cv_scores['mape'].append(mape)
            
            fold_models.append(model)
            
            if verbose:
                print(f"    RMSE: {rmse:.4f}")
                print(f"    MAE:  {mae:.4f}")
                print(f"    RÂ²:   {r2:.4f}")
                if not np.isnan(mape):
                    print(f"    MAPE: {mape:.2f}%")
        
        # ä¿å­˜æ¨¡å‹
        self.models = fold_models
        
        # è®¡ç®—å¹³å‡å¾—åˆ†
        results = {
            'model_type': self.model_type,
            'cv_scores': cv_scores,
            'mean_rmse': np.mean(cv_scores['rmse']),
            'std_rmse': np.std(cv_scores['rmse']),
            'mean_mae': np.mean(cv_scores['mae']),
            'std_mae': np.std(cv_scores['mae']),
            'mean_r2': np.mean(cv_scores['r2']),
            'std_r2': np.std(cv_scores['r2']),
            'mean_mape': np.nanmean(cv_scores['mape']),
            'std_mape': np.nanstd(cv_scores['mape']),
            'predictions': all_predictions,
            'true_values': y
        }
        
        self.cv_results = results
        
        if verbose:
            print(f"\nğŸ“Š äº¤å‰éªŒè¯ç»“æœæ±‡æ€»:")
            print(f"   RMSE: {results['mean_rmse']:.4f} Â± {results['std_rmse']:.4f}")
            print(f"   MAE:  {results['mean_mae']:.4f} Â± {results['std_mae']:.4f}")
            print(f"   RÂ²:   {results['mean_r2']:.4f} Â± {results['std_r2']:.4f}")
            if not np.isnan(results['mean_mape']):
                print(f"   MAPE: {results['mean_mape']:.2f}% Â± {results['std_mape']:.2f}%")
        
        return results
    
    def train_full(self, X: np.ndarray, y: np.ndarray, verbose: bool = True):
        """
        åœ¨å…¨éƒ¨æ•°æ®ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡å‹
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            y: ç›®æ ‡å€¼
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        
        Returns:
            è®­ç»ƒå¥½çš„æ¨¡å‹
        """
        if verbose:
            print(f"\nğŸ¯ è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆå…¨éƒ¨æ•°æ®ï¼‰...")
        
        model = BaseModel(self.model_type, self.params)
        model.fit(X, y, verbose=False)
        
        self.best_model = model
        
        # è®¡ç®—è®­ç»ƒé›†æŒ‡æ ‡
        y_pred = model.predict(X)
        train_rmse = np.sqrt(mean_squared_error(y, y_pred))
        train_r2 = r2_score(y, y_pred)
        
        if verbose:
            print(f"   è®­ç»ƒRMSE: {train_rmse:.4f}")
            print(f"   è®­ç»ƒRÂ²:   {train_r2:.4f}")
        
        return model
    
    def save_model(self, model, filepath: Union[str, Path]):
        """
        ä¿å­˜æ¨¡å‹
        
        Args:
            model: æ¨¡å‹å¯¹è±¡
            filepath: ä¿å­˜è·¯å¾„
        """
        if isinstance(model, BaseModel):
            model.save(filepath)
        else:
            joblib.dump(model, filepath)
        print(f"   ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {filepath}")
        
        return filepath


# ========================================
#           æ¨¡å‹å·¥å‚
# ========================================

class ModelFactory:
    """æ¨¡å‹å·¥å‚ç±»ï¼Œç”¨äºåˆ›å»ºå„ç§æ¨¡å‹è®­ç»ƒå™¨"""
    
    SUPPORTED_MODELS = [
        'xgboost', 'lightgbm', 'catboost',
        'random_forest', 'gradient_boosting', 'adaboost', 'extra_trees',
        'svr', 'knn', 'decision_tree',
        'ridge', 'lasso', 'elastic_net', 'mlp'
    ]
    
    @classmethod
    def create_trainer(cls, model_type: str, params: Dict = None, n_folds: int = 10):
        """
        åˆ›å»ºæ¨¡å‹è®­ç»ƒå™¨
        
        Args:
            model_type: æ¨¡å‹ç±»å‹
            params: æ¨¡å‹å‚æ•°
            n_folds: äº¤å‰éªŒè¯æŠ˜æ•°
        
        Returns:
            è®­ç»ƒå™¨å®ä¾‹
        """
        if model_type not in cls.SUPPORTED_MODELS:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}. æ”¯æŒçš„æ¨¡å‹: {cls.SUPPORTED_MODELS}")
        
        if model_type == 'xgboost':
            return XGBoostTrainer(params, n_folds)
        else:
            return ModelTrainer(model_type, params, n_folds)
    
    @classmethod
    def get_supported_models(cls) -> List[str]:
        """è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨"""
        return cls.SUPPORTED_MODELS.copy()
    
    @classmethod
    def get_model_params(cls, model_type: str) -> Dict:
        """è·å–æ¨¡å‹é»˜è®¤å‚æ•°"""
        if model_type not in cls.SUPPORTED_MODELS:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
        return MODEL_PARAMS.get(model_type, {}).copy()


# ========================================
#           è¾…åŠ©å‡½æ•°
# ========================================

def generate_model_filename(model_type: str, target_col: str, suffix: str = "") -> str:
    """
    ç”Ÿæˆæ¨¡å‹æ–‡ä»¶å
    
    Args:
        model_type: æ¨¡å‹ç±»å‹
        target_col: ç›®æ ‡åˆ—å
        suffix: æ–‡ä»¶ååç¼€
    
    Returns:
        æ–‡ä»¶å
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # æ›´å…¨é¢çš„ç‰¹æ®Šå­—ç¬¦æ›¿æ¢ï¼Œç”Ÿæˆshellå‹å¥½çš„æ–‡ä»¶å
    clean_target = (target_col
                   .replace('(', '_')
                   .replace(')', '')
                   .replace('/', '_')
                   .replace('*', 'x')
                   .replace('^', '')
                   .replace(' ', '_'))
    
    # ç§»é™¤å¯èƒ½çš„é‡å¤ä¸‹åˆ’çº¿
    while '__' in clean_target:
        clean_target = clean_target.replace('__', '_')
    clean_target = clean_target.strip('_')
    
    filename = f"{model_type}_{clean_target}{suffix}_{timestamp}.joblib"
    return filename


def load_model(filepath: Union[str, Path]):
    """
    åŠ è½½æ¨¡å‹
    
    Args:
        filepath: æ¨¡å‹æ–‡ä»¶è·¯å¾„
    
    Returns:
        åŠ è½½çš„æ¨¡å‹
    """
    return joblib.load(filepath)


def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray) -> Dict:
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    
    Args:
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼
    
    Returns:
        è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    
    # MAPE (é¿å…é™¤é›¶)
    mask = y_true != 0
    if mask.sum() > 0:
        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
    else:
        mape = np.nan
    
    return {
        'rmse': rmse,
        'mae': mae,
        'r2': r2,
        'mape': mape
    }