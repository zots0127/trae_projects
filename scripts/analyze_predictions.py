#!/usr/bin/env python3
"""
é¢„æµ‹ç»“æœåˆ†æè„šæœ¬ - ç”ŸæˆPLQYèŒƒå›´å‡†ç¡®ç‡çƒ­å›¾å’Œå…¶ä»–åˆ†æå›¾è¡¨
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime
import argparse
import json
from sklearn.metrics import confusion_matrix, r2_score, mean_absolute_error
import glob

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# è®¾ç½®ç»˜å›¾é£æ ¼
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")

def load_predictions(project_dir, model_name=None):
    """
    åŠ è½½é¢„æµ‹ç»“æœ
    
    Args:
        project_dir: é¡¹ç›®ç›®å½•
        model_name: æŒ‡å®šæ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æœ€ä½³æ¨¡å‹
    
    Returns:
        DataFrameåŒ…å«å®é™…å€¼å’Œé¢„æµ‹å€¼
    """
    project_path = Path(project_dir)
    
    if not project_path.exists():
        print(f"âŒ é¡¹ç›®ç›®å½•ä¸å­˜åœ¨: {project_dir}")
        return None
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œæ‰¾æœ€ä½³æ¨¡å‹ï¼ˆè¿™é‡Œé»˜è®¤ç”¨xgboostï¼‰
    if model_name is None:
        model_name = 'xgboost'
    
    model_dir = project_path / model_name
    if not model_dir.exists():
        print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
        # å°è¯•æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„ç›®å½•
        for possible_name in ['xgboost', 'lightgbm', 'catboost', 'gradient_boosting']:
            model_dir = project_path / possible_name
            if model_dir.exists():
                print(f"âœ… ä½¿ç”¨æ¨¡å‹: {possible_name}")
                break
        else:
            print("âŒ æ‰¾ä¸åˆ°ä»»ä½•æ¨¡å‹ç›®å½•")
            return None
    
    # æŸ¥æ‰¾é¢„æµ‹æ–‡ä»¶
    predictions_dir = model_dir / 'predictions'
    if not predictions_dir.exists():
        print(f"âŒ é¢„æµ‹ç›®å½•ä¸å­˜åœ¨: {predictions_dir}")
        return None
    
    # æ”¶é›†æ‰€æœ‰ç›®æ ‡çš„é¢„æµ‹ç»“æœ
    all_predictions = {}
    
    # æŒ‰ç›®æ ‡ç±»å‹æ”¶é›†æ‰€æœ‰foldçš„æ•°æ®
    target_types = {'wavelength': [], 'PLQY': [], 'tau': []}
    
    # æŸ¥æ‰¾CSVæ–‡ä»¶
    csv_files = list(predictions_dir.glob("*.csv"))
    
    for csv_file in csv_files:
        # ä»æ–‡ä»¶åæå–ç›®æ ‡ç±»å‹
        filename = csv_file.stem
        
        # åˆ¤æ–­ç›®æ ‡ç±»å‹
        target_type = None
        if 'wavelength' in filename.lower():
            target_type = 'wavelength'
        elif 'plqy' in filename.lower():
            target_type = 'PLQY'
        elif 'tau' in filename.lower():
            target_type = 'tau'
        else:
            continue
        
        # è¯»å–é¢„æµ‹æ•°æ®
        try:
            df = pd.read_csv(csv_file)
            
            # æŸ¥æ‰¾å®é™…å€¼å’Œé¢„æµ‹å€¼åˆ—
            actual_col = None
            pred_col = None
            
            for col in df.columns:
                if 'actual' in col.lower() or 'true' in col.lower() or 'experimental' in col.lower():
                    actual_col = col
                elif 'predict' in col.lower() or 'pred' in col.lower():
                    pred_col = col
            
            if actual_col and pred_col:
                # ä½¿ç”¨éªŒè¯é›†æ•°æ®ï¼ˆå¦‚æœæœ‰splitåˆ—ï¼‰
                if 'split' in df.columns:
                    # ä¼˜å…ˆä½¿ç”¨testï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨val
                    if 'test' in df['split'].values:
                        test_df = df[df['split'] == 'test']
                    elif 'val' in df['split'].values:
                        test_df = df[df['split'] == 'val']
                    else:
                        # å¦‚æœéƒ½æ²¡æœ‰ï¼Œä½¿ç”¨æ‰€æœ‰æ•°æ®
                        test_df = df
                else:
                    test_df = df
                
                if len(test_df) > 0:
                    target_types[target_type].append({
                        'actual': test_df[actual_col].values,
                        'predicted': test_df[pred_col].values
                    })
        except Exception as e:
            print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥ {csv_file}: {e}")
    
    # åˆå¹¶æ‰€æœ‰foldçš„æ•°æ®
    for target_type in ['wavelength', 'PLQY', 'tau']:
        if target_types[target_type]:
            actual_all = np.concatenate([d['actual'] for d in target_types[target_type]])
            predicted_all = np.concatenate([d['predicted'] for d in target_types[target_type]])
            
            all_predictions[target_type] = {
                'actual': actual_all,
                'predicted': predicted_all
            }
            print(f"âœ… åŠ è½½ {target_type} é¢„æµ‹æ•°æ®: {len(actual_all)} ä¸ªæ ·æœ¬")
    
    return all_predictions

def plot_plqy_range_accuracy(predictions, output_dir):
    """
    ç»˜åˆ¶PLQYèŒƒå›´é¢„æµ‹å‡†ç¡®ç‡çƒ­å›¾ï¼ˆç±»ä¼¼å›¾gï¼‰
    
    Args:
        predictions: åŒ…å«actualå’Œpredictedçš„å­—å…¸
        output_dir: è¾“å‡ºç›®å½•
    """
    if 'PLQY' not in predictions:
        print("âš ï¸ æ²¡æœ‰PLQYé¢„æµ‹æ•°æ®")
        return
    
    actual = predictions['PLQY']['actual']
    predicted = predictions['PLQY']['predicted']
    
    # ç§»é™¤NaNå€¼
    mask = ~(np.isnan(actual) | np.isnan(predicted))
    actual = actual[mask]
    predicted = predicted[mask]
    
    # å®šä¹‰PLQYèŒƒå›´
    bins = [0, 0.1, 0.5, 1.0]
    labels = ['0-0.1', '0.1-0.5', '0.5-1.0']
    
    # å°†å®é™…å€¼å’Œé¢„æµ‹å€¼åˆ†ç»„
    actual_binned = pd.cut(actual, bins=bins, labels=labels, include_lowest=True)
    predicted_binned = pd.cut(predicted, bins=bins, labels=labels, include_lowest=True)
    
    # ç§»é™¤åˆ†ç»„åçš„NaNå€¼ï¼ˆå¯èƒ½å› ä¸ºè¶…å‡ºèŒƒå›´ï¼‰
    mask2 = ~(actual_binned.isna() | predicted_binned.isna())
    actual_binned = actual_binned[mask2]
    predicted_binned = predicted_binned[mask2]
    
    # åˆ›å»ºæ··æ·†çŸ©é˜µ
    cm = confusion_matrix(actual_binned, predicted_binned, labels=labels)
    
    # å½’ä¸€åŒ–ä¸ºç™¾åˆ†æ¯”ï¼ˆæŒ‰è¡Œå½’ä¸€åŒ–ï¼Œå³æ¯ä¸ªå®é™…èŒƒå›´å†…çš„é¢„æµ‹åˆ†å¸ƒï¼‰
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    # åˆ›å»ºå›¾å½¢
    fig, ax = plt.subplots(1, 1, figsize=(8, 6))
    
    # ä½¿ç”¨è“è‰²è°ƒè‰²æ¿
    cmap = sns.color_palette("Blues", as_cmap=True)
    
    # ç»˜åˆ¶çƒ­å›¾
    sns.heatmap(cm_normalized, 
                annot=True, 
                fmt='.2f',
                cmap=cmap,
                vmin=0, 
                vmax=1,
                xticklabels=labels,
                yticklabels=labels,
                cbar_kws={'label': 'Accuracy'},
                ax=ax)
    
    ax.set_xlabel('Predicted PLQY Range', fontsize=12)
    ax.set_ylabel('Actual PLQY Range', fontsize=12)
    ax.set_title('PLQY Prediction Accuracy by Range', fontsize=14)
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜å›¾å½¢
    save_path = output_dir / 'plqy_range_accuracy.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… ä¿å­˜PLQYèŒƒå›´å‡†ç¡®ç‡å›¾: {save_path}")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š PLQYèŒƒå›´é¢„æµ‹ç»Ÿè®¡:")
    print("-" * 40)
    for i, actual_label in enumerate(labels):
        total = cm[i].sum()
        if total > 0:
            accuracy = cm[i, i] / total
            print(f"{actual_label}: {accuracy:.2%} å‡†ç¡®ç‡ ({cm[i, i]}/{total} æ ·æœ¬)")
    
    # è®¡ç®—æ•´ä½“å‡†ç¡®ç‡
    overall_accuracy = np.trace(cm) / cm.sum()
    print(f"\næ•´ä½“å‡†ç¡®ç‡: {overall_accuracy:.2%}")

def plot_prediction_scatter_all(predictions, output_dir):
    """
    ç»˜åˆ¶æ‰€æœ‰ç›®æ ‡çš„é¢„æµ‹æ•£ç‚¹å›¾
    """
    n_targets = len(predictions)
    if n_targets == 0:
        print("âš ï¸ æ²¡æœ‰é¢„æµ‹æ•°æ®")
        return
    
    # åˆ›å»ºå­å›¾
    fig, axes = plt.subplots(1, n_targets, figsize=(6*n_targets, 5))
    
    if n_targets == 1:
        axes = [axes]
    
    target_names = {
        'wavelength': 'Î»em (nm)',
        'PLQY': 'PLQY', 
        'tau': 'Ï„ (Î¼s)'
    }
    
    for idx, (target, data) in enumerate(predictions.items()):
        ax = axes[idx]
        
        actual = data['actual']
        predicted = data['predicted']
        
        # ç§»é™¤NaNå€¼
        mask = ~(np.isnan(actual) | np.isnan(predicted))
        actual = actual[mask]
        predicted = predicted[mask]
        
        # è®¡ç®—æŒ‡æ ‡
        r2 = r2_score(actual, predicted)
        mae = mean_absolute_error(actual, predicted)
        
        # ç»˜åˆ¶æ•£ç‚¹å›¾
        ax.scatter(actual, predicted, alpha=0.5, s=20, c='#1f77b4')
        
        # æ·»åŠ å¯¹è§’çº¿
        min_val = min(actual.min(), predicted.min())
        max_val = max(actual.max(), predicted.max())
        ax.plot([min_val, max_val], [min_val, max_val], 
               'r--', lw=1, alpha=0.7, label='Perfect prediction')
        
        # è®¾ç½®æ ‡ç­¾
        display_name = target_names.get(target, target)
        ax.set_xlabel(f'Actual {display_name}', fontsize=11)
        ax.set_ylabel(f'Predicted {display_name}', fontsize=11)
        ax.set_title(f'{display_name} Prediction', fontsize=12)
        
        # æ·»åŠ æŒ‡æ ‡æ–‡æœ¬
        ax.text(0.05, 0.95, f'RÂ² = {r2:.3f}\nMAE = {mae:.2f}',
               transform=ax.transAxes, fontsize=10,
               verticalalignment='top',
               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        ax.grid(True, alpha=0.3)
        ax.legend(loc='lower right', fontsize=9)
    
    plt.suptitle('Model Prediction Performance', fontsize=14, y=1.02)
    plt.tight_layout()
    
    # ä¿å­˜å›¾å½¢
    save_path = output_dir / 'prediction_scatter_all.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… ä¿å­˜é¢„æµ‹æ•£ç‚¹å›¾: {save_path}")

def plot_residual_analysis(predictions, output_dir):
    """
    ç»˜åˆ¶æ®‹å·®åˆ†æå›¾
    """
    for target, data in predictions.items():
        actual = data['actual']
        predicted = data['predicted']
        
        # ç§»é™¤NaNå€¼
        mask = ~(np.isnan(actual) | np.isnan(predicted))
        actual = actual[mask]
        predicted = predicted[mask]
        
        # è®¡ç®—æ®‹å·®
        residuals = predicted - actual
        
        # åˆ›å»ºå›¾å½¢
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # 1. æ®‹å·®vsé¢„æµ‹å€¼
        ax = axes[0, 0]
        ax.scatter(predicted, residuals, alpha=0.5, s=20)
        ax.axhline(y=0, color='r', linestyle='--', alpha=0.7)
        ax.set_xlabel('Predicted Value')
        ax.set_ylabel('Residual')
        ax.set_title('Residuals vs Predicted')
        ax.grid(True, alpha=0.3)
        
        # 2. æ®‹å·®ç›´æ–¹å›¾
        ax = axes[0, 1]
        ax.hist(residuals, bins=30, edgecolor='black', alpha=0.7)
        ax.set_xlabel('Residual')
        ax.set_ylabel('Frequency')
        ax.set_title('Residual Distribution')
        ax.grid(True, alpha=0.3)
        
        # 3. Q-Qå›¾
        ax = axes[1, 0]
        from scipy import stats
        stats.probplot(residuals, dist="norm", plot=ax)
        ax.set_title('Q-Q Plot')
        ax.grid(True, alpha=0.3)
        
        # 4. æ®‹å·®vså®é™…å€¼
        ax = axes[1, 1]
        ax.scatter(actual, residuals, alpha=0.5, s=20)
        ax.axhline(y=0, color='r', linestyle='--', alpha=0.7)
        ax.set_xlabel('Actual Value')
        ax.set_ylabel('Residual')
        ax.set_title('Residuals vs Actual')
        ax.grid(True, alpha=0.3)
        
        target_names = {
            'wavelength': 'Wavelength',
            'PLQY': 'PLQY',
            'tau': 'Lifetime'
        }
        display_name = target_names.get(target, target)
        plt.suptitle(f'Residual Analysis - {display_name}', fontsize=14)
        plt.tight_layout()
        
        # ä¿å­˜å›¾å½¢
        save_path = output_dir / f'residual_analysis_{target}.png'
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… ä¿å­˜æ®‹å·®åˆ†æå›¾: {save_path}")

def generate_prediction_report(predictions, output_dir):
    """
    ç”Ÿæˆé¢„æµ‹åˆ†ææŠ¥å‘Š
    """
    report = {}
    
    for target, data in predictions.items():
        actual = data['actual']
        predicted = data['predicted']
        
        # ç§»é™¤NaNå€¼
        mask = ~(np.isnan(actual) | np.isnan(predicted))
        actual = actual[mask]
        predicted = predicted[mask]
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error
        
        metrics = {
            'n_samples': len(actual),
            'r2_score': r2_score(actual, predicted),
            'rmse': np.sqrt(mean_squared_error(actual, predicted)),
            'mae': mean_absolute_error(actual, predicted),
            'mape': mean_absolute_percentage_error(actual, predicted) * 100,
            'residual_mean': np.mean(predicted - actual),
            'residual_std': np.std(predicted - actual),
            'actual_range': [float(actual.min()), float(actual.max())],
            'predicted_range': [float(predicted.min()), float(predicted.max())]
        }
        
        report[target] = metrics
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = output_dir / 'prediction_report.json'
    with open(report_file, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"âœ… ä¿å­˜é¢„æµ‹æŠ¥å‘Š: {report_file}")
    
    # æ‰“å°æŠ¥å‘Šæ‘˜è¦
    print("\n" + "=" * 60)
    print("ğŸ“Š é¢„æµ‹æ€§èƒ½æŠ¥å‘Š")
    print("=" * 60)
    
    target_names = {
        'wavelength': 'Wavelength (nm)',
        'PLQY': 'PLQY',
        'tau': 'Lifetime (Î¼s)'
    }
    
    for target, metrics in report.items():
        display_name = target_names.get(target, target)
        print(f"\n{display_name}:")
        print("-" * 40)
        print(f"  æ ·æœ¬æ•°: {metrics['n_samples']}")
        print(f"  RÂ² Score: {metrics['r2_score']:.4f}")
        print(f"  RMSE: {metrics['rmse']:.4f}")
        print(f"  MAE: {metrics['mae']:.4f}")
        print(f"  MAPE: {metrics['mape']:.2f}%")
        print(f"  å®é™…èŒƒå›´: [{metrics['actual_range'][0]:.2f}, {metrics['actual_range'][1]:.2f}]")
        print(f"  é¢„æµ‹èŒƒå›´: [{metrics['predicted_range'][0]:.2f}, {metrics['predicted_range'][1]:.2f}]")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='é¢„æµ‹ç»“æœåˆ†æ')
    
    parser.add_argument('project', help='é¡¹ç›®ç›®å½•')
    parser.add_argument('--model', '-m', help='æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä½¿ç”¨æœ€ä½³æ¨¡å‹ï¼‰')
    parser.add_argument('--output', '-o', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--plots', nargs='+',
                       choices=['range', 'scatter', 'residual', 'all'],
                       default=['all'],
                       help='è¦ç”Ÿæˆçš„å›¾è¡¨ç±»å‹')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if args.output:
        output_dir = Path(args.output)
    else:
        output_dir = Path(args.project) / 'analysis'
    
    output_dir.mkdir(exist_ok=True, parents=True)
    
    print("=" * 60)
    print("é¢„æµ‹ç»“æœåˆ†æ")
    print("=" * 60)
    print(f"é¡¹ç›®: {args.project}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    # åŠ è½½é¢„æµ‹æ•°æ®
    predictions = load_predictions(args.project, args.model)
    
    if not predictions:
        print("âŒ æ— æ³•åŠ è½½é¢„æµ‹æ•°æ®")
        return
    
    # ç”Ÿæˆå›¾è¡¨
    if 'all' in args.plots or 'range' in args.plots:
        plot_plqy_range_accuracy(predictions, output_dir)
    
    if 'all' in args.plots or 'scatter' in args.plots:
        plot_prediction_scatter_all(predictions, output_dir)
    
    if 'all' in args.plots or 'residual' in args.plots:
        plot_residual_analysis(predictions, output_dir)
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_prediction_report(predictions, output_dir)
    
    print("\n" + "=" * 60)
    print("âœ… åˆ†æå®Œæˆï¼")
    print(f"ç»“æœä¿å­˜åœ¨: {output_dir}")
    print("=" * 60)

if __name__ == "__main__":
    main()