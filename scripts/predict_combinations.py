#!/usr/bin/env python3
"""
ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹ç»„åˆè¿›è¡Œé¢„æµ‹
"""

import pandas as pd
import numpy as np
import joblib
from pathlib import Path
import argparse
from datetime import datetime
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.feature_extractor import FeatureExtractor

def load_models(project_dir, model_name='xgboost'):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆæ”¯æŒAutoMLè·¯å¾„ä¸è‡ªåŠ¨å‘ç°æœ€æ–°Paper_*ç›®å½•ï¼‰"""
    print("åŠ è½½æ¨¡å‹...")

    models = {}

    project_path = Path(project_dir)
    possible_dirs = [
        project_path / 'all_models' / 'automl_train' / model_name / 'models',
        project_path / model_name / 'models',
        project_path / 'models' / model_name,
    ]

    model_dir = None
    for d in possible_dirs:
        if d.exists():
            model_dir = d
            break

    if model_dir is None:
        root = project_path.parent if project_path.name == 'paper_table' else project_path
        candidates = []
        try:
            for d in root.glob('Paper_*'):
                mdir = d / 'all_models' / 'automl_train' / model_name / 'models'
                if mdir.exists():
                    candidates.append(mdir)
            if candidates:
                candidates.sort(key=lambda p: p.stat().st_mtime, reverse=True)
                model_dir = candidates[0]
                print(f"ğŸ” è‡ªåŠ¨åˆ‡æ¢åˆ°æœ€æ–°æ¨¡å‹ç›®å½•: {model_dir}")
        except Exception:
            pass

    if model_dir is None or not model_dir.exists():
        print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {project_path}/{model_name}/models")
        return models

    print(f"ğŸ“ æ¨¡å‹ç›®å½•: {model_dir}")

    for model_file in model_dir.glob("*.joblib"):
        filename = model_file.stem
        if 'wavelength' in filename.lower():
            models['wavelength'] = joblib.load(model_file)
            print(f"  âœ… æ³¢é•¿æ¨¡å‹: {model_file.name}")
        elif 'plqy' in filename.lower():
            models['PLQY'] = joblib.load(model_file)
            print(f"  âœ… PLQYæ¨¡å‹: {model_file.name}")

    print(f"æˆåŠŸåŠ è½½ {len(models)} ä¸ªæ¨¡å‹")
    return models

def extract_features_batch(df, feature_type='combined', batch_size=1000):
    """æ‰¹é‡æå–ç‰¹å¾"""
    print(f"\næå–ç‰¹å¾ (æ‰¹å¤§å°: {batch_size})...")
    
    extractor = FeatureExtractor(
        feature_type=feature_type,
        morgan_radius=2,
        morgan_bits=1024,
        use_cache=True
    )
    
    n_samples = len(df)
    features_list = []
    valid_indices = []
    
    for i in range(0, n_samples, batch_size):
        batch_end = min(i + batch_size, n_samples)
        batch_df = df.iloc[i:batch_end]
        
        for idx, row in batch_df.iterrows():
            try:
                # æå–ç»„åˆç‰¹å¾
                smiles_list = [row['L1'], row['L2'], row['L3']]
                features = extractor.extract_combination(smiles_list)
                
                if features is not None:
                    features_list.append(features)
                    valid_indices.append(idx)
            except:
                continue
        
        # è¿›åº¦æ˜¾ç¤º
        if (i + batch_size) % 10000 == 0 or batch_end == n_samples:
            print(f"  è¿›åº¦: {batch_end:,}/{n_samples:,} ({100*batch_end/n_samples:.1f}%)")
    
    if features_list:
        X = np.vstack(features_list)
        df_valid = df.iloc[valid_indices].reset_index(drop=True)
        print(f"  âœ… æˆåŠŸæå–: {len(X):,} ä¸ªç‰¹å¾")
        return X, df_valid
    else:
        return None, None

def predict_batch(models, X, df_valid, batch_size=10000):
    """æ‰¹é‡é¢„æµ‹"""
    print("\né¢„æµ‹æ€§è´¨...")
    
    predictions = {}
    
    # é¢„æµ‹æ¯ä¸ªç›®æ ‡
    for target, model in models.items():
        print(f"  é¢„æµ‹ {target}...")
        
        n_samples = len(X)
        preds = []
        
        for i in range(0, n_samples, batch_size):
            batch_end = min(i + batch_size, n_samples)
            batch_X = X[i:batch_end]
            batch_pred = model.predict(batch_X)
            preds.append(batch_pred)
        
        predictions[target] = np.concatenate(preds)
        
        # ç»Ÿè®¡
        print(f"    èŒƒå›´: [{predictions[target].min():.3f}, {predictions[target].max():.3f}]")
        print(f"    å‡å€¼: {predictions[target].mean():.3f}")
    
    # æ·»åŠ é¢„æµ‹åˆ°DataFrame
    if 'wavelength' in predictions:
        df_valid['Predicted_wavelength'] = predictions['wavelength']
    if 'PLQY' in predictions:
        df_valid['Predicted_PLQY'] = predictions['PLQY']
    
    return df_valid

def analyze_results(df):
    """åˆ†æé¢„æµ‹ç»“æœ"""
    print("\n" + "=" * 60)
    print("é¢„æµ‹ç»“æœåˆ†æ")
    print("-" * 60)
    
    if 'Predicted_PLQY' in df.columns:
        # Top 10 PLQY
        top10 = df.nlargest(10, 'Predicted_PLQY')
        print("\nğŸ† Top 10 PLQYç»„åˆ:")
        for i, row in enumerate(top10.iterrows(), 1):
            idx, data = row
            print(f"\n  #{i}: PLQY = {data['Predicted_PLQY']:.4f}")
            if i <= 3:  # æ˜¾ç¤ºå‰3ä¸ªçš„è¯¦ç»†ä¿¡æ¯
                print(f"      L1/L2: {data['L1'][:50]}...")
                print(f"      L3: {data['L3'][:50]}...")
                if 'Predicted_wavelength' in df.columns:
                    print(f"      Î» = {data['Predicted_wavelength']:.1f} nm")
    
    if 'Predicted_wavelength' in df.columns:
        # æ³¢é•¿åˆ†å¸ƒ
        print(f"\nğŸ“Š æ³¢é•¿åˆ†å¸ƒ:")
        print(f"  æœ€çŸ­: {df['Predicted_wavelength'].min():.1f} nm")
        print(f"  æœ€é•¿: {df['Predicted_wavelength'].max():.1f} nm")
        print(f"  å¹³å‡: {df['Predicted_wavelength'].mean():.1f} nm")
    
    print("=" * 60)

def main():
    parser = argparse.ArgumentParser(description='é¢„æµ‹ç»„åˆæ€§è´¨')
    parser.add_argument('--input', '-i', 
                       default='ir_assemble.csv',
                       help='ç»„åˆæ–‡ä»¶')
    parser.add_argument('--project', '-p',
                       default='paper_table',
                       help='æ¨¡å‹é¡¹ç›®ç›®å½•')
    parser.add_argument('--output', '-o',
                       default='ir_assemble_predicted.csv',
                       help='è¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--top', '-t', type=int, default=1000,
                       help='ä¿å­˜Top Nä¸ªå€™é€‰')
    parser.add_argument('--batch-size', type=int, default=5000,
                       help='æ‰¹å¤„ç†å¤§å°')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("é¢„æµ‹ç»„åˆæ€§è´¨")
    print("=" * 60)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # 1. åŠ è½½ç»„åˆ
    print(f"\nåŠ è½½ç»„åˆæ–‡ä»¶: {args.input}")
    df = pd.read_csv(args.input)
    print(f"  ç»„åˆæ•°: {len(df):,}")
    
    # 2. åŠ è½½æ¨¡å‹
    models = load_models(args.project)
    if not models:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹")
        return
    
    # 3. æå–ç‰¹å¾
    X, df_valid = extract_features_batch(df, batch_size=args.batch_size)
    if X is None:
        print("âŒ ç‰¹å¾æå–å¤±è´¥")
        return
    
    # 4. é¢„æµ‹
    df_predicted = predict_batch(models, X, df_valid)
    
    # 5. ä¿å­˜ç»“æœ
    print(f"\nä¿å­˜é¢„æµ‹ç»“æœ...")
    df_predicted.to_csv(args.output, index=False)
    print(f"  âœ… å®Œæ•´ç»“æœ: {args.output}")
    
    # ä¿å­˜Topå€™é€‰
    if 'Predicted_PLQY' in df_predicted.columns:
        top_file = args.output.replace('.csv', f'_top{args.top}.csv')
        top_df = df_predicted.nlargest(args.top, 'Predicted_PLQY')
        top_df.to_csv(top_file, index=False)
        print(f"  âœ… Top {args.top}: {top_file}")
    
    # 6. åˆ†æç»“æœ
    analyze_results(df_predicted)
    
    print(f"\nå®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    main()
