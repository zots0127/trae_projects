#!/usr/bin/env python3
"""
ä¿®å¤AutoMLè®­ç»ƒç»“æœçš„æ¨¡å‹å¯¹æ¯”è¡¨æ ¼ç”Ÿæˆ
é€‚é…optuna_resultsç›®å½•ç»“æ„
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path
import sys
from datetime import datetime

def collect_automl_results(output_dir):
    """æ”¶é›†AutoMLè®­ç»ƒçš„æ‰€æœ‰æ¨¡å‹ç»“æœ"""
    output_dir = Path(output_dir)
    results = []
    
    # æŸ¥æ‰¾automl_trainç›®å½•
    automl_dir = output_dir / 'automl_train'
    if not automl_dir.exists():
        print(f"âŒ æœªæ‰¾åˆ°automl_trainç›®å½•: {automl_dir}")
        return pd.DataFrame()
    
    # æ‰«ææ‰€æœ‰æ¨¡å‹çš„optuna_results
    for model_dir in automl_dir.iterdir():
        if not model_dir.is_dir():
            continue
            
        # æŸ¥æ‰¾optuna_resultsç›®å½•
        optuna_dir = model_dir / 'optuna_results'
        if optuna_dir.exists():
            # æŸ¥æ‰¾automl_summaryæ–‡ä»¶
            for summary_file in optuna_dir.glob('automl_summary_*.json'):
                target_name = summary_file.stem.replace('automl_summary_', '')
                
                with open(summary_file, 'r') as f:
                    summary_data = json.load(f)
                
                # éå†æ‰€æœ‰æ¨¡å‹çš„ç»“æœ
                for model_name, model_data in summary_data.get('all_models', {}).items():
                    if 'fold_results' in model_data:
                        fold_results = model_data['fold_results']
                        
                        # æå–å„æŠ˜çš„æŒ‡æ ‡
                        r2_scores = [fold['r2'] for fold in fold_results]
                        rmse_scores = [fold['rmse'] for fold in fold_results]
                        mae_scores = [fold['mae'] for fold in fold_results]
                        
                        results.append({
                            'Model': model_name.upper(),
                            'Target': target_name.replace('_', ' '),
                            'R2_mean': np.mean(r2_scores),
                            'R2_std': np.std(r2_scores),
                            'RMSE_mean': np.mean(rmse_scores),
                            'RMSE_std': np.std(rmse_scores),
                            'MAE_mean': np.mean(mae_scores),
                            'MAE_std': np.std(mae_scores),
                            'N_folds': len(fold_results),
                            'Best_R2': model_data.get('best_r2', np.mean(r2_scores))
                        })
    
    # å»é‡ï¼ˆå¯èƒ½æœ‰å¤šä¸ªç›¸åŒçš„ç»“æœï¼‰
    df = pd.DataFrame(results)
    if not df.empty:
        # æ ¹æ®Modelå’ŒTargetå»é‡ï¼Œä¿ç•™ç¬¬ä¸€ä¸ª
        df = df.drop_duplicates(subset=['Model', 'Target'], keep='first')
    return df

def generate_comparison_tables(df, output_dir):
    """ç”Ÿæˆå¤šç§æ ¼å¼çš„å¯¹æ¯”è¡¨æ ¼"""
    if df.empty:
        print("âŒ æ²¡æœ‰æ•°æ®ç”Ÿæˆè¡¨æ ¼")
        return
    
    output_dir = Path(output_dir)
    tables_dir = output_dir / 'tables'
    tables_dir.mkdir(exist_ok=True)
    
    # 1. ç”Ÿæˆå®Œæ•´CSV
    df.to_csv(tables_dir / 'model_comparison_full.csv', index=False)
    print(f"âœ… å®Œæ•´å¯¹æ¯”è¡¨: {tables_dir}/model_comparison_full.csv")
    
    # 2. ç”ŸæˆMarkdownè¡¨æ ¼
    markdown_content = "# Model Performance Comparison (Cross-Validation)\n\n"
    
    for target in df['Target'].unique():
        markdown_content += f"\n## {target}\n\n"
        target_df = df[df['Target'] == target].copy()
        target_df = target_df.sort_values('R2_mean', ascending=False)
        
        markdown_content += "| Model | RÂ² | RMSE | MAE |\n"
        markdown_content += "|-------|-----|------|-----|\n"
        
        for _, row in target_df.iterrows():
            r2 = f"{row['R2_mean']:.4f} Â± {row['R2_std']:.4f}"
            rmse = f"{row['RMSE_mean']:.2f} Â± {row['RMSE_std']:.2f}"
            mae = f"{row['MAE_mean']:.2f} Â± {row['MAE_std']:.2f}"
            
            # æ ‡è®°æœ€ä½³æ¨¡å‹
            if row['R2_mean'] == target_df['R2_mean'].max():
                model_name = f"**{row['Model']}** ğŸ†"
            else:
                model_name = row['Model']
            
            markdown_content += f"| {model_name} | {r2} | {rmse} | {mae} |\n"
    
    with open(tables_dir / 'model_comparison.md', 'w') as f:
        f.write(markdown_content)
    print(f"âœ… Markdownè¡¨æ ¼: {tables_dir}/model_comparison.md")
    
    # 3. ç”ŸæˆLaTeXè¡¨æ ¼
    latex_content = r"""\documentclass{article}
\usepackage{booktabs}
\usepackage{multirow}
\begin{document}

\begin{table}[htbp]
\centering
\caption{Model Performance Comparison}
\label{tab:model_comparison}
\begin{tabular}{llccc}
\toprule
Target & Model & R$^2$ & RMSE & MAE \\
\midrule
"""
    
    for target in df['Target'].unique():
        target_df = df[df['Target'] == target].copy()
        target_df = target_df.sort_values('R2_mean', ascending=False)
        
        for idx, row in target_df.iterrows():
            if idx == target_df.index[0]:
                target_str = target.replace('_', r'\_')
            else:
                target_str = ""
            
            r2 = f"{row['R2_mean']:.4f} $\\pm$ {row['R2_std']:.4f}"
            rmse = f"{row['RMSE_mean']:.2f} $\\pm$ {row['RMSE_std']:.2f}"
            mae = f"{row['MAE_mean']:.2f} $\\pm$ {row['MAE_std']:.2f}"
            
            latex_content += f"{target_str} & {row['Model']} & {r2} & {rmse} & {mae} \\\\\n"
        
        if target != df['Target'].unique()[-1]:
            latex_content += r"\midrule" + "\n"
    
    latex_content += r"""\bottomrule
\end{tabular}
\end{table}

\end{document}"""
    
    with open(tables_dir / 'model_comparison.tex', 'w') as f:
        f.write(latex_content)
    print(f"âœ… LaTeXè¡¨æ ¼: {tables_dir}/model_comparison.tex")
    
    # 4. ç”Ÿæˆæœ€ä½³æ¨¡å‹æ€»ç»“
    best_models = []
    for target in df['Target'].unique():
        target_df = df[df['Target'] == target]
        best_idx = target_df['R2_mean'].idxmax()
        best = target_df.loc[best_idx]
        
        best_models.append({
            'Target': target,
            'Best Model': best['Model'],
            'RÂ²': f"{best['R2_mean']:.4f} Â± {best['R2_std']:.4f}",
            'RMSE': f"{best['RMSE_mean']:.2f} Â± {best['RMSE_std']:.2f}",
            'MAE': f"{best['MAE_mean']:.2f} Â± {best['MAE_std']:.2f}"
        })
    
    best_df = pd.DataFrame(best_models)
    best_df.to_csv(tables_dir / 'best_models_summary.csv', index=False)
    print(f"âœ… æœ€ä½³æ¨¡å‹æ€»ç»“: {tables_dir}/best_models_summary.csv")
    
    # æ‰“å°æ€»ç»“
    print("\n" + "="*60)
    print("æœ€ä½³æ¨¡å‹æ€»ç»“")
    print("="*60)
    print(best_df.to_string(index=False))

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    import argparse
    parser = argparse.ArgumentParser(description='ç”ŸæˆAutoMLæ¨¡å‹å¯¹æ¯”è¡¨æ ¼')
    parser.add_argument('--project', default='runs/train', help='é¡¹ç›®ç›®å½•')
    args = parser.parse_args()
    
    print("="*60)
    print("ç”ŸæˆAutoMLæ¨¡å‹å¯¹æ¯”è¡¨æ ¼")
    print("="*60)
    
    # æ”¶é›†ç»“æœ
    print("\næ”¶é›†æ¨¡å‹ç»“æœ...")
    df = collect_automl_results(args.project)
    
    if df.empty:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹ç»“æœ")
        return 1
    
    print(f"âœ… æ‰¾åˆ° {len(df)} ä¸ªæ¨¡å‹ç»“æœ")
    print(f"   æ¨¡å‹: {df['Model'].nunique()} ä¸ª")
    print(f"   ç›®æ ‡: {df['Target'].nunique()} ä¸ª")
    
    # ç”Ÿæˆè¡¨æ ¼
    print("\nç”Ÿæˆå¯¹æ¯”è¡¨æ ¼...")
    generate_comparison_tables(df, args.project)
    
    print("\nâœ… å®Œæˆï¼")
    return 0

if __name__ == '__main__':
    sys.exit(main())