#!/usr/bin/env python3
"""
é¡¹ç›®çº§æ‰¹é‡é¢„æµ‹å™¨
ç”¨äºç®¡ç†å’Œæ‰§è¡Œæ•´ä¸ªé¡¹ç›®çš„æ‰¹é‡é¢„æµ‹ä»»åŠ¡
"""

import json
import yaml
import joblib
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
import sys
sys.path.append(str(Path(__file__).parent.parent))
from core.feature_extractor import FeatureExtractor
from utils.batch_predictor_v2 import BatchPredictorV2
from utils.file_feature_cache import FileFeatureCache
from utils.timing import TimingTracker


class ProjectPredictor:
    """é¡¹ç›®çº§æ‰¹é‡é¢„æµ‹å™¨"""
    
    def __init__(self, project_dir: str, verbose: bool = True):
        """
        åˆå§‹åŒ–é¡¹ç›®é¢„æµ‹å™¨
        
        Args:
            project_dir: é¡¹ç›®ç›®å½•è·¯å¾„
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        """
        self.project_dir = Path(project_dir)
        self.verbose = verbose
        
        if not self.project_dir.exists():
            raise ValueError(f"é¡¹ç›®ç›®å½•ä¸å­˜åœ¨: {project_dir}")
        
        # åŠ è½½é¡¹ç›®ä¿¡æ¯
        self.models = {}
        self.configs = {}
        self.metadata = {}
        
        # åˆå§‹åŒ–æ—¶é—´è¿½è¸ªå™¨
        self.timing = TimingTracker(f"project_predictor_{project_dir}")
        
        # æ‰«æå¹¶åŠ è½½æ‰€æœ‰æ¨¡å‹
        with self.timing.measure('load_models'):
            self._load_all_models()
        
        # åˆå§‹åŒ–æ‰¹é‡é¢„æµ‹å™¨
        self.batch_predictor = BatchPredictorV2(
            batch_size=5000,
            show_progress=verbose
        )
        
        if self.verbose:
            print(f"âœ… åŠ è½½é¡¹ç›®: {self.project_dir}")
            print(f"   æ‰¾åˆ° {len(self.models)} ä¸ªæ¨¡å‹")
    
    def _load_all_models(self):
        """åŠ è½½é¡¹ç›®ä¸­çš„æ‰€æœ‰æ¨¡å‹"""
        # æŸ¥æ‰¾æ‰€æœ‰æ¨¡å‹æ–‡ä»¶
        model_files = list(self.project_dir.rglob("*.joblib"))
        
        for model_file in model_files:
            # è§£ææ¨¡å‹ä¿¡æ¯
            model_name = model_file.stem
            parts = model_name.split('_')
            
            if len(parts) >= 3:
                model_type = parts[0]
                # æŸ¥æ‰¾ 'final' åœ¨éƒ¨åˆ†ä¸­çš„ä½ç½®
                if 'final' in parts:
                    final_idx = parts.index('final')
                    target = '_'.join(parts[1:final_idx])
                else:
                    # å‡è®¾æœ€åä¸€éƒ¨åˆ†æ˜¯æ—¶é—´æˆ³
                    target = '_'.join(parts[1:-1])
                
                # æ„å»ºæ¨¡å‹é”®
                key = f"{model_type}_{target}"
                
                # åŠ è½½æ¨¡å‹
                try:
                    model = joblib.load(model_file)
                    
                    # åˆ›å»ºç›®æ ‡åç§°æ˜ å°„
                    target_mappings = {
                        'Max_wavelength_nm': 'Max_wavelength(nm)',
                        'tau_sx10-6': 'tau(s*10^-6)',
                        'PLQY': 'PLQY'
                    }
                    original_target = target_mappings.get(target, target)
                    
                    self.models[key] = {
                        'model': model,
                        'path': str(model_file),
                        'type': model_type,
                        'target': target,
                        'original_target': original_target,
                        'name': model_name
                    }
                    
                    # å°è¯•åŠ è½½å¯¹åº”çš„é…ç½®
                    config_file = model_file.parent.parent / 'config.yaml'
                    if config_file.exists():
                        with open(config_file, 'r') as f:
                            self.configs[key] = yaml.safe_load(f)
                    
                    # å°è¯•åŠ è½½æ€§èƒ½æŒ‡æ ‡
                    # åˆ›å»ºå¯èƒ½çš„ç›®æ ‡åç§°æ˜ å°„
                    target_mappings = {
                        'Max_wavelength_nm': 'Max_wavelength(nm)',
                        'tau_sx10-6': 'tau(s*10^-6)',
                        'PLQY': 'PLQY'
                    }
                    
                    # è·å–åŸå§‹ç›®æ ‡åç§°
                    original_target = target_mappings.get(target, target)
                    
                    exports_dir = model_file.parent.parent / "exports"
                    summary_files = []
                    if exports_dir.exists():
                        # ä½¿ç”¨åŸå§‹ç›®æ ‡åç§°æŸ¥æ‰¾
                        for f in exports_dir.glob(f"{model_type}_*_summary.json"):
                            if original_target in f.name:
                                summary_files.append(f)
                                break
                    if summary_files:
                        with open(summary_files[0], 'r') as f:
                            summary = json.load(f)
                            self.models[key]['performance'] = {
                                'r2': summary.get('mean_r2', 0),
                                'r2_std': summary.get('std_r2', 0),
                                'rmse': summary.get('mean_rmse', 0),
                                'rmse_std': summary.get('std_rmse', 0),
                                'mae': summary.get('mean_mae', 0),
                                'mae_std': summary.get('std_mae', 0)
                            }
                    
                except Exception as e:
                    if self.verbose:
                        print(f"âš ï¸ æ— æ³•åŠ è½½æ¨¡å‹ {model_file}: {e}")
    
    def list_models(self) -> pd.DataFrame:
        """
        åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
        
        Returns:
            åŒ…å«æ¨¡å‹ä¿¡æ¯çš„DataFrame
        """
        if not self.models:
            return pd.DataFrame()
        
        data = []
        for key, info in self.models.items():
            perf = info.get('performance', {})
            
            # æ ¼å¼åŒ–å¸¦æ ‡å‡†å·®çš„å€¼
            def format_metric(mean_key, std_key, decimals=4):
                mean_val = perf.get(mean_key, 'N/A')
                std_val = perf.get(std_key, 0)
                if isinstance(mean_val, (int, float)):
                    if std_val > 0:
                        return f"{mean_val:.{decimals}f}Â±{std_val:.{decimals}f}"
                    else:
                        return f"{mean_val:.{decimals}f}"
                return 'N/A'
            
            row = {
                'Model': info['type'],
                'Target': info.get('original_target', info['target']),
                'RÂ² (meanÂ±std)': format_metric('r2', 'r2_std', 4),
                'RMSE (meanÂ±std)': format_metric('rmse', 'rmse_std', 2),
                'MAE (meanÂ±std)': format_metric('mae', 'mae_std', 2)
            }
            data.append(row)
        
        df = pd.DataFrame(data)
        if self.verbose:
            print("\nğŸ“Š é¡¹ç›®æ¨¡å‹åˆ—è¡¨:")
            print(df.to_string(index=False))
        
        return df
    
    def predict_all_models(self, data_path: str, output_dir: str = None,
                          smiles_columns: List[str] = None) -> Dict[str, pd.DataFrame]:
        """
        ä½¿ç”¨æ‰€æœ‰æ¨¡å‹è¿›è¡Œé¢„æµ‹
        
        Args:
            data_path: è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            smiles_columns: SMILESåˆ—å
        
        Returns:
            åŒ…å«æ‰€æœ‰é¢„æµ‹ç»“æœçš„å­—å…¸
        """
        # è¯»å–æ•°æ®
        with self.timing.measure('data_loading', {'file': data_path}):
            df = pd.read_csv(data_path)
        print(f"\nğŸ“ åŠ è½½æ•°æ®: {data_path}")
        print(f"   æ ·æœ¬æ•°: {len(df)}")
        
        # è®°å½•æ€»ä½“é¢„æµ‹çš„ååé‡
        self.timing.calculate_throughput('data_loading', len(df))
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if output_dir is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_dir = self.project_dir / f"batch_predictions_{timestamp}"
        else:
            output_dir = Path(output_dir)
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # é»˜è®¤SMILESåˆ—
        if smiles_columns is None:
            smiles_columns = ['L1', 'L2', 'L3']
        
        results = {}
        
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡é¢„æµ‹ ({len(self.models)} ä¸ªæ¨¡å‹)...")
        
        for i, (key, model_info) in enumerate(self.models.items(), 1):
            print(f"\n[{i}/{len(self.models)}] é¢„æµ‹: {key}")
            
            try:
                with self.timing.measure(f'predict_{key}', {'model': key, 'samples': len(df)}):
                    # è·å–é…ç½®
                    config = self.configs.get(key, {})
                    feature_config = config.get('feature', {})
                    
                    # åˆ›å»ºç‰¹å¾æå–å™¨
                    feature_extractor = FeatureExtractor(
                        feature_type=feature_config.get('feature_type', 'combined'),
                        morgan_bits=feature_config.get('morgan_bits', 1024),
                        morgan_radius=feature_config.get('morgan_radius', 2),
                        use_cache=True
                    )
                    
                    # è¿›è¡Œé¢„æµ‹
                    pred_values, failed_indices = self.batch_predictor.predict_with_cache(
                        df=df,
                        model=model_info['model'],
                        feature_extractor=feature_extractor,
                        smiles_columns=smiles_columns,
                        feature_type=feature_config.get('feature_type', 'combined'),
                        combination_method=feature_config.get('combination_method', 'mean'),
                        input_file=str(data_path)
                    )
                    
                    # åˆ›å»ºé¢„æµ‹ç»“æœDataFrame
                    predictions = df.copy()
                    pred_col = f"Predicted_{model_info.get('original_target', model_info['target'])}"
                    # ç›´æ¥ä½¿ç”¨é¢„æµ‹å€¼ï¼ˆfailed_indiceså·²æ ‡è®°ä¸ºNaNï¼‰
                    predictions[pred_col] = pred_values
                    
                    # ä¿å­˜ç»“æœ
                    output_file = output_dir / f"{key}_predictions.csv"
                    predictions.to_csv(output_file, index=False)
                    print(f"   âœ… ä¿å­˜åˆ°: {output_file}")
                    
                    results[key] = predictions
                
                # è®¡ç®—ååé‡
                self.timing.calculate_throughput(f'predict_{key}', len(df))
                
            except Exception as e:
                print(f"   âŒ é¢„æµ‹å¤±è´¥: {e}")
                continue
        
        # ç”Ÿæˆæ±‡æ€»æ–‡ä»¶
        self._generate_summary(results, output_dir)
        
        print(f"\nâœ… æ‰¹é‡é¢„æµ‹å®Œæˆ!")
        print(f"   ç»“æœç›®å½•: {output_dir}")
        
        # æ‰“å°æ—¶é—´ç»Ÿè®¡
        if self.verbose:
            print("\n" + "="*50)
            print("â±ï¸ é¢„æµ‹æ—¶é—´ç»Ÿè®¡")
            print("="*50)
            self.timing.print_summary()
            
            # ä¿å­˜æ—¶é—´æŠ¥å‘Š
            try:
                timing_file = output_dir / "timing_report.json"
                self.timing.save_report(timing_file, format='json')
                print(f"\nğŸ’¾ æ—¶é—´æŠ¥å‘Šå·²ä¿å­˜åˆ°: {timing_file}")
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜æ—¶é—´æŠ¥å‘Šå¤±è´¥: {e}")
        
        return results
    
    def predict_best_models(self, data_path: str, output_path: str = None,
                           smiles_columns: List[str] = None) -> pd.DataFrame:
        """
        åªä½¿ç”¨æ¯ä¸ªç›®æ ‡çš„æœ€ä½³æ¨¡å‹è¿›è¡Œé¢„æµ‹
        
        Args:
            data_path: è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            smiles_columns: SMILESåˆ—å
        
        Returns:
            åŒ…å«æœ€ä½³æ¨¡å‹é¢„æµ‹ç»“æœçš„DataFrame
        """
        # æ‰¾å‡ºæ¯ä¸ªç›®æ ‡çš„æœ€ä½³æ¨¡å‹
        best_models = self._find_best_models()
        
        if not best_models:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ€§èƒ½æŒ‡æ ‡ï¼Œæ— æ³•é€‰æ‹©æœ€ä½³æ¨¡å‹")
            return pd.DataFrame()
        
        # è¯»å–æ•°æ®
        df = pd.read_csv(data_path)
        result_df = df.copy()
        
        print(f"\nğŸ“ åŠ è½½æ•°æ®: {data_path}")
        print(f"   æ ·æœ¬æ•°: {len(df)}")
        
        # é»˜è®¤SMILESåˆ—
        if smiles_columns is None:
            smiles_columns = ['L1', 'L2', 'L3']
        
        print(f"\nğŸ† ä½¿ç”¨æœ€ä½³æ¨¡å‹é¢„æµ‹...")
        
        for target, model_key in best_models.items():
            model_info = self.models[model_key]
            print(f"\nç›®æ ‡: {target}")
            print(f"  æœ€ä½³æ¨¡å‹: {model_info['type']} (RÂ²={model_info['performance']['r2']:.4f})")
            
            try:
                # è·å–é…ç½®
                config = self.configs.get(model_key, {})
                feature_config = config.get('feature', {})
                
                # åˆ›å»ºç‰¹å¾æå–å™¨
                feature_extractor = FeatureExtractor(
                    feature_type=feature_config.get('feature_type', 'combined'),
                    morgan_bits=feature_config.get('morgan_bits', 1024),
                    morgan_radius=feature_config.get('morgan_radius', 2),
                    use_cache=True
                )
                
                # è¿›è¡Œé¢„æµ‹
                pred_values, failed_indices = self.batch_predictor.predict_with_cache(
                    df=df,
                    model=model_info['model'],
                    feature_extractor=feature_extractor,
                    smiles_columns=smiles_columns,
                    feature_type=feature_config.get('feature_type', 'combined'),
                    combination_method=feature_config.get('combination_method', 'mean'),
                    input_file=str(data_path)
                )
                
                # æ·»åŠ åˆ°ç»“æœï¼ˆç›´æ¥ä½¿ç”¨é¢„æµ‹å€¼ï¼Œfailed_indiceså·²æ ‡è®°ä¸ºNaNï¼‰
                result_df[f"Best_{target}"] = pred_values
                
            except Exception as e:
                print(f"  âŒ é¢„æµ‹å¤±è´¥: {e}")
                continue
        
        # ä¿å­˜ç»“æœ
        if output_path is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_path = f"best_predictions_{timestamp}.csv"
        
        result_df.to_csv(output_path, index=False)
        print(f"\nâœ… æœ€ä½³æ¨¡å‹é¢„æµ‹å®Œæˆ!")
        print(f"   ä¿å­˜åˆ°: {output_path}")
        
        return result_df
    
    def predict_ensemble(self, data_path: str, output_path: str = None,
                        smiles_columns: List[str] = None,
                        method: str = 'mean') -> pd.DataFrame:
        """
        é›†æˆé¢„æµ‹ï¼ˆå¤šæ¨¡å‹å¹³å‡ï¼‰
        
        Args:
            data_path: è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            smiles_columns: SMILESåˆ—å
            method: é›†æˆæ–¹æ³• ('mean', 'median', 'weighted')
        
        Returns:
            åŒ…å«é›†æˆé¢„æµ‹ç»“æœçš„DataFrame
        """
        # å…ˆè¿›è¡Œæ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
        all_predictions = self.predict_all_models(
            data_path=data_path,
            output_dir=None,
            smiles_columns=smiles_columns
        )
        
        if not all_predictions:
            return pd.DataFrame()
        
        # è¯»å–åŸå§‹æ•°æ®
        df = pd.read_csv(data_path)
        result_df = df.copy()
        
        print(f"\nğŸ”® è¿›è¡Œé›†æˆé¢„æµ‹ (æ–¹æ³•: {method})...")
        
        # æŒ‰ç›®æ ‡åˆ†ç»„
        targets = {}
        for key in all_predictions.keys():
            target = self.models[key]['target']
            if target not in targets:
                targets[target] = []
            targets[target].append(key)
        
        # å¯¹æ¯ä¸ªç›®æ ‡è¿›è¡Œé›†æˆ
        for target, model_keys in targets.items():
            print(f"\nç›®æ ‡: {target}")
            print(f"  å‚ä¸æ¨¡å‹: {len(model_keys)}")
            
            # æ”¶é›†æ‰€æœ‰é¢„æµ‹
            predictions = []
            weights = []
            
            for key in model_keys:
                pred_col = f"Predicted_{target}"
                if pred_col in all_predictions[key].columns:
                    predictions.append(all_predictions[key][pred_col].values)
                    
                    # å¦‚æœä½¿ç”¨åŠ æƒå¹³å‡ï¼Œä½¿ç”¨RÂ²ä½œä¸ºæƒé‡
                    if method == 'weighted' and 'performance' in self.models[key]:
                        r2 = self.models[key]['performance'].get('r2', 0)
                        weights.append(max(r2, 0))  # ç¡®ä¿æƒé‡éè´Ÿ
                    else:
                        weights.append(1.0)
            
            if predictions:
                predictions = np.array(predictions)
                
                if method == 'mean':
                    ensemble_pred = np.mean(predictions, axis=0)
                elif method == 'median':
                    ensemble_pred = np.median(predictions, axis=0)
                elif method == 'weighted':
                    weights = np.array(weights)
                    weights = weights / weights.sum()  # å½’ä¸€åŒ–
                    ensemble_pred = np.average(predictions, axis=0, weights=weights)
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„é›†æˆæ–¹æ³•: {method}")
                
                result_df[f"Ensemble_{target}"] = ensemble_pred
                print(f"  âœ… é›†æˆå®Œæˆ")
        
        # ä¿å­˜ç»“æœ
        if output_path is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_path = f"ensemble_predictions_{timestamp}.csv"
        
        result_df.to_csv(output_path, index=False)
        print(f"\nâœ… é›†æˆé¢„æµ‹å®Œæˆ!")
        print(f"   ä¿å­˜åˆ°: {output_path}")
        
        return result_df
    
    def _find_best_models(self) -> Dict[str, str]:
        """
        æ‰¾å‡ºæ¯ä¸ªç›®æ ‡çš„æœ€ä½³æ¨¡å‹
        
        Returns:
            ç›®æ ‡åˆ°æœ€ä½³æ¨¡å‹é”®çš„æ˜ å°„
        """
        best_models = {}
        
        # æŒ‰ç›®æ ‡åˆ†ç»„ (ä½¿ç”¨åŸå§‹ç›®æ ‡åç§°)
        targets = {}
        for key, info in self.models.items():
            target = info.get('original_target', info['target'])
            if target not in targets:
                targets[target] = []
            if 'performance' in info:
                targets[target].append((key, info['performance'].get('r2', -1)))
        
        # é€‰æ‹©æ¯ä¸ªç›®æ ‡çš„æœ€ä½³æ¨¡å‹
        for target, models in targets.items():
            if models:
                # æŒ‰RÂ²æ’åºï¼Œé€‰æ‹©æœ€é«˜çš„
                models.sort(key=lambda x: x[1], reverse=True)
                best_models[target] = models[0][0]
        
        return best_models
    
    def _generate_summary(self, results: Dict[str, pd.DataFrame], output_dir: Path):
        """ç”Ÿæˆé¢„æµ‹ç»“æœæ±‡æ€»"""
        summary = {
            'project': str(self.project_dir),
            'timestamp': datetime.now().isoformat(),
            'models_used': len(results),
            'predictions': {}
        }
        
        for key, df in results.items():
            # æ‰¾å‡ºé¢„æµ‹åˆ—
            pred_cols = [col for col in df.columns if col.startswith('Predicted_')]
            if pred_cols:
                pred_col = pred_cols[0]
                summary['predictions'][key] = {
                    'file': f"{key}_predictions.csv",
                    'samples': len(df),
                    'mean': float(df[pred_col].mean()),
                    'std': float(df[pred_col].std()),
                    'min': float(df[pred_col].min()),
                    'max': float(df[pred_col].max())
                }
        
        # ä¿å­˜æ±‡æ€»
        summary_file = output_dir / 'prediction_summary.json'
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        print(f"\nğŸ“Š æ±‡æ€»æ–‡ä»¶: {summary_file}")
    
    def get_project_info(self) -> Dict:
        """
        è·å–é¡¹ç›®ä¿¡æ¯
        
        Returns:
            é¡¹ç›®ä¿¡æ¯å­—å…¸
        """
        info = {
            'project_path': str(self.project_dir),
            'models_count': len(self.models),
            'models': {},
            'targets': set(),
            'best_models': {}
        }
        
        # æ”¶é›†æ¨¡å‹ä¿¡æ¯
        for key, model_info in self.models.items():
            model_type = model_info['type']
            target = model_info['target']
            
            info['targets'].add(target)
            
            if model_type not in info['models']:
                info['models'][model_type] = []
            
            info['models'][model_type].append({
                'target': target,
                'performance': model_info.get('performance', {})
            })
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best = self._find_best_models()
        for target, model_key in best.items():
            model_info = self.models[model_key]
            info['best_models'][target] = {
                'model': model_info['type'],
                'r2': model_info.get('performance', {}).get('r2', 'N/A')
            }
        
        info['targets'] = list(info['targets'])
        
        return info


def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•"""
    import argparse
    
    parser = argparse.ArgumentParser(description='é¡¹ç›®çº§æ‰¹é‡é¢„æµ‹')
    parser.add_argument('project', help='é¡¹ç›®ç›®å½•')
    parser.add_argument('--data', required=True, help='é¢„æµ‹æ•°æ®æ–‡ä»¶')
    parser.add_argument('--mode', default='all', 
                       choices=['all', 'best', 'ensemble'],
                       help='é¢„æµ‹æ¨¡å¼')
    parser.add_argument('--output', help='è¾“å‡ºè·¯å¾„')
    parser.add_argument('--smiles-columns', help='SMILESåˆ—åï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--list-models', action='store_true',
                       help='åˆ—å‡ºæ‰€æœ‰æ¨¡å‹')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé¢„æµ‹å™¨
    predictor = ProjectPredictor(args.project)
    
    if args.list_models:
        predictor.list_models()
        return
    
    # è§£æSMILESåˆ—
    smiles_columns = None
    if args.smiles_columns:
        smiles_columns = args.smiles_columns.split(',')
    
    # æ‰§è¡Œé¢„æµ‹
    if args.mode == 'all':
        predictor.predict_all_models(
            data_path=args.data,
            output_dir=args.output,
            smiles_columns=smiles_columns
        )
    elif args.mode == 'best':
        predictor.predict_best_models(
            data_path=args.data,
            output_path=args.output,
            smiles_columns=smiles_columns
        )
    elif args.mode == 'ensemble':
        predictor.predict_ensemble(
            data_path=args.data,
            output_path=args.output,
            smiles_columns=smiles_columns
        )


if __name__ == '__main__':
    main()