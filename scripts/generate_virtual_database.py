#!/usr/bin/env python3
"""
ç”Ÿæˆè™šæ‹Ÿæ•°æ®åº“ - é‡ç»„L1ã€L2ã€L3çš„æ‰€æœ‰ç»„åˆå¹¶é¢„æµ‹
"""

import os
import sys
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
import argparse
import joblib
from itertools import product
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.feature_extractor import FeatureExtractor

def load_original_data(data_file):
    """åŠ è½½åŸå§‹æ•°æ®å¹¶æå–å”¯ä¸€çš„é…ä½“"""
    df = pd.read_csv(data_file)
    
    # æå–æ‰€æœ‰å”¯ä¸€çš„L1, L2, L3
    l1_unique = df['L1'].dropna().unique()
    l2_unique = df['L2'].dropna().unique()
    l3_unique = df['L3'].dropna().unique()
    
    print(f"åŸå§‹æ•°æ®ç»Ÿè®¡:")
    print(f"  L1: {len(l1_unique)} ä¸ªå”¯ä¸€é…ä½“")
    print(f"  L2: {len(l2_unique)} ä¸ªå”¯ä¸€é…ä½“")
    print(f"  L3: {len(l3_unique)} ä¸ªå”¯ä¸€é…ä½“")
    print(f"  åŸå§‹ç»„åˆæ•°: {len(df)}")
    
    return l1_unique, l2_unique, l3_unique, df

def generate_all_combinations(l1_unique, l2_unique, l3_unique, max_combinations=None):
    """ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„L1ã€L2ã€L3ç»„åˆ
    
    æ³¨æ„ï¼šL1å’ŒL2åº”è¯¥æ˜¯ç›¸åŒçš„é…ä½“ï¼ˆç†è®ºä¸Šï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨L1=L2çš„ç»„åˆ
    """
    
    # åˆå¹¶L1å’ŒL2çš„å”¯ä¸€å€¼ï¼ˆå› ä¸ºç†è®ºä¸Šå®ƒä»¬åº”è¯¥æ˜¯ç›¸åŒçš„é…ä½“é›†ï¼‰
    l12_unique = np.unique(np.concatenate([l1_unique, l2_unique]))
    
    print(f"\nç»„åˆç­–ç•¥ï¼š")
    print(f"  L1/L2å…±äº«é…ä½“æ± : {len(l12_unique)} ä¸ªé…ä½“")
    print(f"  L3é…ä½“æ± : {len(l3_unique)} ä¸ªé…ä½“")
    
    # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„ç»„åˆ
    all_combinations = []
    
    # L1=L2çš„æƒ…å†µï¼ˆå¯¹ç§°é…ä½“ï¼‰
    total_possible = len(l12_unique) * len(l3_unique)
    print(f"  ç†è®ºç»„åˆæ•°: {total_possible:,} (L1=L2é…å¯¹)")
    
    if max_combinations and total_possible > max_combinations:
        print(f"âš ï¸ é™åˆ¶ç»„åˆæ•°ä¸º: {max_combinations:,}")
        # éšæœºé‡‡æ ·
        import random
        random.seed(42)
        sampled_indices = random.sample(range(total_possible), min(max_combinations, total_possible))
        sampled_indices.sort()
        
        count = 0
        for idx, (l12, l3) in enumerate(product(l12_unique, l3_unique)):
            if idx in sampled_indices:
                all_combinations.append({
                    'L1': l12,
                    'L2': l12,  # L1å’ŒL2ç›¸åŒ
                    'L3': l3
                })
                count += 1
                if count >= max_combinations:
                    break
    else:
        # ç”Ÿæˆæ‰€æœ‰L1=L2çš„ç»„åˆ
        for l12 in l12_unique:
            for l3 in l3_unique:
                all_combinations.append({
                    'L1': l12,
                    'L2': l12,  # L1å’ŒL2ç›¸åŒ
                    'L3': l3
                })
    
    # åˆ›å»ºDataFrame
    assembled_df = pd.DataFrame(all_combinations)
    print(f"ç”Ÿæˆç»„åˆæ•°: {len(assembled_df):,}")
    
    return assembled_df

def remove_existing_combinations(assembled_df, original_df):
    """ç§»é™¤å·²å­˜åœ¨çš„ç»„åˆï¼Œåªä¿ç•™æ–°ç»„åˆ"""
    
    # åˆ›å»ºç»„åˆé”®
    assembled_df['combo_key'] = assembled_df['L1'] + '|' + assembled_df['L2'] + '|' + assembled_df['L3']
    original_df['combo_key'] = original_df['L1'] + '|' + original_df['L2'] + '|' + original_df['L3']
    
    # æ‰¾å‡ºæ–°ç»„åˆ
    existing_keys = set(original_df['combo_key'].dropna())
    new_df = assembled_df[~assembled_df['combo_key'].isin(existing_keys)].copy()
    
    # åˆ é™¤è¾…åŠ©åˆ—
    new_df = new_df.drop('combo_key', axis=1)
    
    print(f"æ–°ç»„åˆæ•°ï¼ˆæ’é™¤å·²æœ‰ï¼‰: {len(new_df):,}")
    
    return new_df

def extract_features_for_prediction(df, feature_type='combined'):
    """ä¸ºé¢„æµ‹æå–ç‰¹å¾"""
    
    print("\næå–åˆ†å­ç‰¹å¾...")
    
    # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
    extractor = FeatureExtractor(
        feature_type=feature_type,
        morgan_radius=2,
        morgan_bits=1024,
        combination_method='mean',
        use_cache=True
    )
    
    # æå–ç‰¹å¾
    features_list = []
    valid_indices = []
    
    for idx, row in df.iterrows():
        try:
            smiles_list = [row['L1'], row['L2'], row['L3']]
            # è¿‡æ»¤æ‰NaN
            smiles_list = [s for s in smiles_list if pd.notna(s) and s != '']
            
            if smiles_list:
                features = extractor.extract_features(smiles_list)
                if features is not None:
                    features_list.append(features)
                    valid_indices.append(idx)
                    
                    if len(features_list) % 100 == 0:
                        print(f"  å·²å¤„ç†: {len(features_list)} ä¸ªç»„åˆ")
        except Exception as e:
            # è·³è¿‡æœ‰é—®é¢˜çš„SMILES
            continue
    
    if features_list:
        X = np.vstack(features_list)
        df_valid = df.iloc[valid_indices].reset_index(drop=True)
        print(f"âœ… æˆåŠŸæå–ç‰¹å¾: {len(X)} ä¸ªç»„åˆ")
        return X, df_valid
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸæå–ä»»ä½•ç‰¹å¾")
        return None, None

def load_trained_model(project_dir, model_name='xgboost', target='PLQY'):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    
    project_path = Path(project_dir)
    model_dir = project_path / model_name / 'models'
    
    if not model_dir.exists():
        print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
        return None
    
    # æŸ¥æ‰¾å¯¹åº”ç›®æ ‡çš„æ¨¡å‹æ–‡ä»¶
    model_files = list(model_dir.glob(f"*{target}*.joblib"))
    
    if not model_files:
        # å°è¯•å…¶ä»–å¯èƒ½çš„å‘½å
        if target == 'wavelength':
            model_files = list(model_dir.glob("*wavelength*.joblib")) + \
                         list(model_dir.glob("*Max_wavelength*.joblib"))
        elif target == 'PLQY':
            model_files = list(model_dir.glob("*PLQY*.joblib")) + \
                         list(model_dir.glob("*plqy*.joblib"))
        elif target == 'tau':
            model_files = list(model_dir.glob("*tau*.joblib")) + \
                         list(model_dir.glob("*lifetime*.joblib"))
    
    if model_files:
        # ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹
        model_file = sorted(model_files)[-1]
        print(f"âœ… åŠ è½½æ¨¡å‹: {model_file.name}")
        model = joblib.load(model_file)
        return model
    else:
        print(f"âŒ æœªæ‰¾åˆ°{target}çš„æ¨¡å‹æ–‡ä»¶")
        return None

def predict_properties(X, df_valid, project_dir, model_name='xgboost'):
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹åˆ†å­æ€§è´¨"""
    
    print("\nåŠ è½½æ¨¡å‹å¹¶é¢„æµ‹...")
    
    predictions = {}
    
    # é¢„æµ‹ä¸‰ä¸ªç›®æ ‡
    targets = {
        'Max_wavelength(nm)': 'wavelength',
        'PLQY': 'PLQY',
        'tau(s*10^-6)': 'tau'
    }
    
    for target_col, target_key in targets.items():
        model = load_trained_model(project_dir, model_name, target_key)
        
        if model:
            print(f"  é¢„æµ‹ {target_col}...")
            try:
                pred = model.predict(X)
                predictions[target_col] = pred
                
                # ç»Ÿè®¡é¢„æµ‹ç»“æœ
                print(f"    èŒƒå›´: [{pred.min():.3f}, {pred.max():.3f}]")
                print(f"    å‡å€¼: {pred.mean():.3f}")
                print(f"    æ ‡å‡†å·®: {pred.std():.3f}")
            except Exception as e:
                print(f"    âŒ é¢„æµ‹å¤±è´¥: {e}")
                predictions[target_col] = np.zeros(len(X))
        else:
            print(f"  âš ï¸ è·³è¿‡ {target_col} (æ— æ¨¡å‹)")
            predictions[target_col] = np.zeros(len(X))
    
    # å°†é¢„æµ‹ç»“æœæ·»åŠ åˆ°DataFrame
    for col, pred in predictions.items():
        df_valid[f'Predicted_{col}'] = pred
    
    return df_valid

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç”Ÿæˆè™šæ‹Ÿæ•°æ®åº“')
    
    parser.add_argument('--data', '-d', default='data/Database_normalized.csv',
                       help='åŸå§‹æ•°æ®æ–‡ä»¶')
    parser.add_argument('--project', '-p', default='paper_table',
                       help='è®­ç»ƒé¡¹ç›®ç›®å½•')
    parser.add_argument('--model', '-m', default='xgboost',
                       help='ä½¿ç”¨çš„æ¨¡å‹')
    parser.add_argument('--output', '-o', default='data/ir_assemble.csv',
                       help='è¾“å‡ºæ–‡ä»¶å')
    parser.add_argument('--max-combinations', type=int,
                       help='æœ€å¤§ç»„åˆæ•°é™åˆ¶')
    parser.add_argument('--include-existing', action='store_true',
                       help='åŒ…å«å·²å­˜åœ¨çš„ç»„åˆ')
    parser.add_argument('--feature-type', default='combined',
                       choices=['morgan', 'descriptors', 'combined'],
                       help='ç‰¹å¾ç±»å‹')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ç”Ÿæˆè™šæ‹Ÿæ•°æ®åº“")
    print("=" * 60)
    print(f"åŸå§‹æ•°æ®: {args.data}")
    print(f"é¡¹ç›®ç›®å½•: {args.project}")
    print(f"ä½¿ç”¨æ¨¡å‹: {args.model}")
    print(f"ç‰¹å¾ç±»å‹: {args.feature_type}")
    
    # 1. åŠ è½½åŸå§‹æ•°æ®
    print("\n" + "-" * 40)
    print("æ­¥éª¤1: åŠ è½½åŸå§‹æ•°æ®")
    l1_unique, l2_unique, l3_unique, original_df = load_original_data(args.data)
    
    # 2. ç”Ÿæˆæ‰€æœ‰ç»„åˆ
    print("\n" + "-" * 40)
    print("æ­¥éª¤2: ç”Ÿæˆæ‰€æœ‰ç»„åˆ")
    assembled_df = generate_all_combinations(
        l1_unique, l2_unique, l3_unique, 
        max_combinations=args.max_combinations
    )
    
    # 3. å¯é€‰ï¼šç§»é™¤å·²å­˜åœ¨çš„ç»„åˆ
    if not args.include_existing:
        print("\n" + "-" * 40)
        print("æ­¥éª¤3: ç§»é™¤å·²å­˜åœ¨çš„ç»„åˆ")
        assembled_df = remove_existing_combinations(assembled_df, original_df)
    
    # ä¿å­˜ç»„åˆæ–‡ä»¶
    assembled_file = args.output.replace('.csv', '_combinations.csv')
    assembled_df.to_csv(assembled_file, index=False)
    print(f"\nâœ… ä¿å­˜ç»„åˆæ–‡ä»¶: {assembled_file}")
    
    # 4. æå–ç‰¹å¾
    print("\n" + "-" * 40)
    print("æ­¥éª¤4: æå–åˆ†å­ç‰¹å¾")
    X, df_valid = extract_features_for_prediction(assembled_df, args.feature_type)
    
    if X is None:
        print("âŒ ç‰¹å¾æå–å¤±è´¥")
        return
    
    # 5. é¢„æµ‹æ€§è´¨
    print("\n" + "-" * 40)
    print("æ­¥éª¤5: é¢„æµ‹åˆ†å­æ€§è´¨")
    df_predicted = predict_properties(X, df_valid, args.project, args.model)
    
    # 6. ä¿å­˜ç»“æœ
    print("\n" + "-" * 40)
    print("æ­¥éª¤6: ä¿å­˜è™šæ‹Ÿæ•°æ®åº“")
    
    # ä¿å­˜å®Œæ•´çš„è™šæ‹Ÿæ•°æ®åº“
    output_file = args.output
    df_predicted.to_csv(output_file, index=False)
    print(f"âœ… è™šæ‹Ÿæ•°æ®åº“å·²ä¿å­˜: {output_file}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 60)
    print("ğŸ“Š è™šæ‹Ÿæ•°æ®åº“ç»Ÿè®¡:")
    print("-" * 40)
    print(f"æ€»ç»„åˆæ•°: {len(df_predicted):,}")
    
    # æ‰¾å‡ºæœ€ä¼˜ç»„åˆ
    if 'Predicted_PLQY' in df_predicted.columns:
        # PLQYæœ€é«˜çš„ç»„åˆ
        best_plqy_idx = df_predicted['Predicted_PLQY'].idxmax()
        best_plqy = df_predicted.loc[best_plqy_idx]
        print(f"\nğŸ† æœ€é«˜PLQYç»„åˆ:")
        print(f"  L1: {best_plqy['L1'][:30]}...")
        print(f"  L2: {best_plqy['L2'][:30]}...")
        print(f"  L3: {best_plqy['L3'][:30]}...")
        print(f"  é¢„æµ‹PLQY: {best_plqy['Predicted_PLQY']:.3f}")
    
    if 'Predicted_Max_wavelength(nm)' in df_predicted.columns:
        # æ³¢é•¿æœ€é•¿çš„ç»„åˆ
        best_wl_idx = df_predicted['Predicted_Max_wavelength(nm)'].idxmax()
        best_wl = df_predicted.loc[best_wl_idx]
        print(f"\nğŸ† æœ€é•¿æ³¢é•¿ç»„åˆ:")
        print(f"  L1: {best_wl['L1'][:30]}...")
        print(f"  L2: {best_wl['L2'][:30]}...")
        print(f"  L3: {best_wl['L3'][:30]}...")
        print(f"  é¢„æµ‹æ³¢é•¿: {best_wl['Predicted_Max_wavelength(nm)']:.1f} nm")
    
    # ä¿å­˜Topå€™é€‰ç»„åˆ
    top_candidates = df_predicted.nlargest(100, 'Predicted_PLQY')
    top_file = output_file.replace('.csv', '_top100.csv')
    top_candidates.to_csv(top_file, index=False)
    print(f"\nâœ… Top 100å€™é€‰å·²ä¿å­˜: {top_file}")
    
    print("\n" + "=" * 60)
    print("âœ… è™šæ‹Ÿæ•°æ®åº“ç”Ÿæˆå®Œæˆï¼")
    print("=" * 60)

if __name__ == "__main__":
    main()
