#!/usr/bin/env python3
"""
YOLOé£æ ¼çš„é…ç½®ç³»ç»Ÿ
é€šè¿‡é…ç½®æ–‡ä»¶å®šä¹‰å®Œæ•´çš„è®­ç»ƒæµç¨‹
"""

import yaml
import json
import os
from pathlib import Path
from typing import Dict, List, Optional, Union, Any
from dataclasses import dataclass, field, asdict
import copy
from datetime import datetime


# ========================================
#           é…ç½®æ•°æ®ç±»
# ========================================

@dataclass
class DataConfig:
    """æ•°æ®é…ç½®"""
    data_path: str = "data/Database_normalized.csv"
    smiles_columns: List[str] = field(default_factory=lambda: ['L1', 'L2', 'L3'])
    target_columns: List[str] = field(default_factory=lambda: ['Max_wavelength(nm)', 'PLQY', 'tau(s*10^-6)'])
    train_ratio: float = 0.8
    val_ratio: float = 0.2
    test_ratio: float = 0.0
    random_seed: int = 42
    # å¯é€‰ï¼šå¤–éƒ¨æä¾›çš„æµ‹è¯•é›†CSVè·¯å¾„ã€‚è‹¥æä¾›ï¼Œåˆ™åœ¨å®Œæ•´è®­ç»ƒåè¿›è¡Œä¸€æ¬¡æµ‹è¯•è¯„ä¼°
    test_data_path: Optional[str] = None
    
    # ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥
    # å¯é€‰: 'skip' (è·³è¿‡å«NaNçš„è¡Œ), 'mean' (å‡å€¼å¡«å……), 'median' (ä¸­ä½æ•°å¡«å……), 
    #      'zero' (é›¶å€¼å¡«å……), 'forward' (å‰å‘å¡«å……), 'interpolate' (æ’å€¼)
    nan_handling: str = "skip"
    
    # ç¼ºå¤±å€¼å¤„ç†çš„è¯¦ç»†é…ç½®
    nan_threshold: float = 0.5  # å½“æŸè¡Œç¼ºå¤±å€¼æ¯”ä¾‹è¶…è¿‡æ­¤é˜ˆå€¼æ—¶è·³è¿‡
    feature_nan_strategy: str = "zero"  # ç‰¹å¾ç¼ºå¤±å€¼å¤„ç†ï¼ˆå½“nan_handlingä¸æ˜¯skipæ—¶ï¼‰
    target_nan_strategy: str = "skip"   # ç›®æ ‡å€¼ç¼ºå¤±å¤„ç†
    
    # å¤šç›®æ ‡æ•°æ®é€‰æ‹©ç­–ç•¥
    # 'intersection': åªä½¿ç”¨æ‰€æœ‰ç›®æ ‡éƒ½æœ‰å€¼çš„æ•°æ®ï¼ˆæœ€ä¸¥æ ¼ï¼Œæ•°æ®æœ€å°‘ï¼‰
    # 'independent': æ¯ä¸ªç›®æ ‡ç‹¬ç«‹ä½¿ç”¨å…¶æœ‰æ•ˆæ•°æ®ï¼ˆé»˜è®¤ï¼Œæ•°æ®åˆ©ç”¨ç‡é«˜ï¼‰
    # 'union': ä½¿ç”¨æ‰€æœ‰æ•°æ®ï¼Œç¼ºå¤±å€¼å¡«å……ï¼ˆæœ€å®½æ¾ï¼Œéœ€é…åˆnan_handlingï¼‰
    multi_target_strategy: str = "independent"
    
    # æ•°æ®é‡‡æ ·ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    sample_size: Optional[int] = None  # å¦‚æœè®¾ç½®ï¼Œåªä½¿ç”¨å‰Nä¸ªæ ·æœ¬
    
    def validate(self):
        """éªŒè¯é…ç½®"""
        assert self.train_ratio + self.val_ratio + self.test_ratio == 1.0, "æ•°æ®åˆ†å‰²æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ä¸º1"
        assert len(self.smiles_columns) > 0, "è‡³å°‘éœ€è¦ä¸€ä¸ªSMILESåˆ—"
        assert len(self.target_columns) > 0, "è‡³å°‘éœ€è¦ä¸€ä¸ªç›®æ ‡åˆ—"
        assert self.nan_handling in ["skip", "mean", "median", "zero", "forward", "interpolate"], \
            f"ä¸æ”¯æŒçš„ç¼ºå¤±å€¼å¤„ç†æ–¹æ³•: {self.nan_handling}"
        assert self.multi_target_strategy in ["intersection", "independent", "union"], \
            f"ä¸æ”¯æŒçš„å¤šç›®æ ‡ç­–ç•¥: {self.multi_target_strategy}"


@dataclass
class FeatureConfig:
    """ç‰¹å¾é…ç½®"""
    feature_type: str = "combined"  # morgan, descriptors, combined
    morgan_bits: int = 1024
    morgan_radius: int = 2
    combination_method: str = "mean"  # mean, sum, concat
    use_cache: bool = True
    cache_dir: str = "feature_cache"
    descriptor_count: int = 85
    
    def validate(self):
        """éªŒè¯é…ç½®"""
        assert self.feature_type in ["morgan", "descriptors", "combined", "tabular", "auto"], \
            f"ä¸æ”¯æŒçš„ç‰¹å¾ç±»å‹: {self.feature_type}"
        assert self.combination_method in ["mean", "sum", "concat"], \
            f"ä¸æ”¯æŒçš„ç»„åˆæ–¹æ³•: {self.combination_method}"
        assert isinstance(self.descriptor_count, int) and self.descriptor_count > 0, \
            f"descriptor_count å¿…é¡»æ˜¯æ­£æ•´æ•°: {self.descriptor_count}"


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    model_type: str = "xgboost"
    hyperparameters: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†"""
        # è®¾ç½®é»˜è®¤è¶…å‚æ•°
        if not self.hyperparameters:
            self.hyperparameters = self.get_default_params()
    
    def get_default_params(self) -> Dict:
        """è·å–é»˜è®¤å‚æ•°"""
        from models import MODEL_PARAMS
        return MODEL_PARAMS.get(self.model_type, {}).copy()
    
    def validate(self):
        """éªŒè¯é…ç½®"""
        from models import ModelFactory
        assert self.model_type in ModelFactory.get_supported_models(), \
            f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}"


@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    n_folds: int = 10
    metrics: List[str] = field(default_factory=lambda: ["rmse", "mae", "r2", "mape"])
    early_stopping: bool = False
    early_stopping_rounds: int = 10
    verbose: int = 1
    save_fold_models: bool = True
    save_final_model: bool = True
    save_training_curves: bool = True  # ä¿å­˜è®­ç»ƒæ›²çº¿ï¼ˆé»˜è®¤å¼€å¯ï¼‰
    save_feature_importance: bool = True  # ä¿å­˜ç‰¹å¾é‡è¦æ€§ï¼ˆé»˜è®¤å¼€å¯ï¼‰
    model_selection: Optional[str] = None  # æ¨¡å‹é€‰æ‹©ç­–ç•¥ï¼ˆç”¨äºAutoMLï¼‰: best_r2, best_rmseç­‰
    
    def validate(self):
        """éªŒè¯é…ç½®"""
        assert self.n_folds > 1, "äº¤å‰éªŒè¯æŠ˜æ•°å¿…é¡»å¤§äº1"
        valid_metrics = ["rmse", "mae", "r2", "mape", "mse"]
        for metric in self.metrics:
            assert metric in valid_metrics, f"ä¸æ”¯æŒçš„æŒ‡æ ‡: {metric}"


@dataclass
class ComparisonConfig:
    """æ¨¡å‹å¯¹æ¯”é…ç½®"""
    enable: bool = False  # æ˜¯å¦å¯ç”¨å¯¹æ¯”è¡¨ç”Ÿæˆ
    formats: List[str] = field(default_factory=lambda: ["markdown", "html", "latex", "csv"])
    highlight_best: bool = True  # é«˜äº®æœ€ä½³æ¨¡å‹
    include_std: bool = True  # åŒ…å«æ ‡å‡†å·®
    save_to_file: bool = True  # ä¿å­˜åˆ°æ–‡ä»¶
    output_dir: Optional[str] = None  # è¾“å‡ºç›®å½•ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨è®­ç»ƒç›®å½•ï¼‰
    
    # æ•°å€¼ç²¾åº¦é…ç½®
    decimal_places: Dict[str, int] = field(default_factory=lambda: {
        'r2': 4,
        'rmse': 4,
        'mae': 4
    })
    
    def validate(self):
        """éªŒè¯é…ç½®"""
        valid_formats = ["markdown", "html", "latex", "csv", "excel"]
        for fmt in self.formats:
            assert fmt in valid_formats, f"ä¸æ”¯æŒçš„è¡¨æ ¼æ ¼å¼: {fmt}"


@dataclass
class ExportConfig:
    """å¯¼å‡ºé…ç½®"""
    enable: bool = True
    formats: List[str] = field(default_factory=lambda: ["json", "csv"])
    include_predictions: bool = True
    include_feature_importance: bool = True
    include_cv_details: bool = True
    generate_plots: bool = True
    generate_report: bool = True
    stratified_analysis: bool = False  # ç”Ÿæˆåˆ†æ®µæ€§èƒ½åˆ†æå›¾ï¼ˆå¦‚PLQYèŒƒå›´æ··æ·†çŸ©é˜µï¼‰
    
    def validate(self):
        """éªŒè¯é…ç½®"""
        valid_formats = ["json", "csv", "excel", "pickle"]
        for fmt in self.formats:
            assert fmt in valid_formats, f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {fmt}"


@dataclass
class LoggingConfig:
    """æ—¥å¿—é…ç½®"""
    project_name: str = "ml_experiment"
    base_dir: str = "training_logs"
    auto_save: bool = True
    save_plots: bool = True
    generate_report: bool = True
    export_for_paper: bool = False
    log_level: str = "INFO"
    
    def validate(self):
        """éªŒè¯é…ç½®"""
        assert self.log_level in ["DEBUG", "INFO", "WARNING", "ERROR"], \
            f"ä¸æ”¯æŒçš„æ—¥å¿—çº§åˆ«: {self.log_level}"


@dataclass
class ExperimentConfig:
    """å®éªŒé…ç½® - ä¸»é…ç½®ç±»"""
    name: str = "default_experiment"
    description: str = ""
    version: str = "1.0.0"
    author: str = ""
    
    # å­é…ç½®
    data: DataConfig = field(default_factory=DataConfig)
    feature: FeatureConfig = field(default_factory=FeatureConfig)
    model: ModelConfig = field(default_factory=ModelConfig)
    training: TrainingConfig = field(default_factory=TrainingConfig)
    comparison: ComparisonConfig = field(default_factory=ComparisonConfig)
    export: ExportConfig = field(default_factory=ExportConfig)
    logging: LoggingConfig = field(default_factory=LoggingConfig)
    
    # AutoMLç‰¹æ®Šé…ç½®ï¼ˆç”¨äºautomlæ¨¡æ¿ï¼‰
    automl_models: Optional[List[str]] = None  # AutoMLè¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨ï¼ˆå·²åºŸå¼ƒï¼Œä½¿ç”¨models_to_trainï¼‰
    automl_model_configs: Optional[Dict[str, Dict]] = None  # æ¯ä¸ªæ¨¡å‹çš„é…ç½®
    models_to_train: Optional[List[str]] = None  # å¤šæ¨¡å‹è®­ç»ƒæ—¶çš„æ¨¡å‹åˆ—è¡¨
    
    # å…ƒæ•°æ®
    created_at: str = field(default_factory=lambda: datetime.now().isoformat())
    config_path: Optional[str] = None
    
    def validate(self):
        """éªŒè¯æ‰€æœ‰é…ç½®"""
        self.data.validate()
        self.feature.validate()
        self.model.validate()
        self.training.validate()
        
        # å¤„ç†æ·±æ‹·è´åå¯èƒ½å˜æˆdictçš„æƒ…å†µ
        if isinstance(self.comparison, dict):
            self.comparison = ComparisonConfig(**self.comparison)
        self.comparison.validate()
        
        if isinstance(self.export, dict):
            self.export = ExportConfig(**self.export)
        self.export.validate()
        
        self.logging.validate()
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)
    
    def to_yaml(self, path: Optional[str] = None) -> str:
        """è½¬æ¢ä¸ºYAMLæ ¼å¼"""
        yaml_str = yaml.dump(self.to_dict(), default_flow_style=False, sort_keys=False)
        if path:
            with open(path, 'w') as f:
                f.write(yaml_str)
        return yaml_str
    
    def to_json(self, path: Optional[str] = None) -> str:
        """è½¬æ¢ä¸ºJSONæ ¼å¼"""
        json_str = json.dumps(self.to_dict(), indent=2)
        if path:
            with open(path, 'w') as f:
                f.write(json_str)
        return json_str
    
    @classmethod
    def from_yaml(cls, path: str) -> 'ExperimentConfig':
        """ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®"""
        with open(path, 'r') as f:
            data = yaml.safe_load(f)
        return cls.from_dict(data, config_path=path)
    
    @classmethod
    def from_json(cls, path: str) -> 'ExperimentConfig':
        """ä»JSONæ–‡ä»¶åŠ è½½é…ç½®"""
        with open(path, 'r') as f:
            data = json.load(f)
        return cls.from_dict(data, config_path=path)
    
    @classmethod
    def from_dict(cls, data: Dict, config_path: Optional[str] = None) -> 'ExperimentConfig':
        """ä»å­—å…¸åˆ›å»ºé…ç½®"""
        # åˆ›å»ºå­é…ç½®
        if 'data' in data and isinstance(data['data'], dict):
            data['data'] = DataConfig(**data['data'])
        if 'feature' in data and isinstance(data['feature'], dict):
            data['feature'] = FeatureConfig(**data['feature'])
        if 'model' in data and isinstance(data['model'], dict):
            data['model'] = ModelConfig(**data['model'])
        if 'training' in data and isinstance(data['training'], dict):
            data['training'] = TrainingConfig(**data['training'])
        if 'comparison' in data and isinstance(data['comparison'], dict):
            data['comparison'] = ComparisonConfig(**data['comparison'])
        if 'export' in data and isinstance(data['export'], dict):
            data['export'] = ExportConfig(**data['export'])
        if 'logging' in data and isinstance(data['logging'], dict):
            data['logging'] = LoggingConfig(**data['logging'])
        
        config = cls(**data)
        config.config_path = config_path
        return config
    
    def copy(self) -> 'ExperimentConfig':
        """æ·±æ‹·è´é…ç½®"""
        return copy.deepcopy(self)
    
    def update(self, updates: Dict) -> 'ExperimentConfig':
        """æ›´æ–°é…ç½®"""
        new_config = self.copy()
        
        for key, value in updates.items():
            if '.' in key:  # æ”¯æŒåµŒå¥—æ›´æ–°ï¼Œå¦‚ "model.hyperparameters.n_estimators"
                parts = key.split('.')
                obj = new_config
                for part in parts[:-1]:
                    obj = getattr(obj, part)
                setattr(obj, parts[-1], value)
            else:
                setattr(new_config, key, value)
        
        return new_config


# ========================================
#           é…ç½®ç®¡ç†å™¨
# ========================================

class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_dir: str = "config"):
        """
        åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        
        Args:
            config_dir: é…ç½®æ–‡ä»¶ç›®å½•
        """
        self.config_dir = Path(config_dir)
        self.config_dir.mkdir(exist_ok=True)
        
        # é¢„å®šä¹‰é…ç½®æ¨¡æ¿
        self.templates = {}
        self.load_templates()
    
    def load_templates(self):
        """åŠ è½½é¢„å®šä¹‰æ¨¡æ¿"""
        # XGBoostå¿«é€Ÿæ¨¡æ¿
        self.templates['xgboost_quick'] = ExperimentConfig(
            name="xgboost_quick",
            description="XGBoostå¿«é€Ÿè®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="xgboost",
                hyperparameters={
                    'n_estimators': 100,
                    'max_depth': 6,
                    'learning_rate': 0.1
                }
            ),
            training=TrainingConfig(n_folds=5)
        )
        
        # XGBoostæ ‡å‡†æ¨¡æ¿
        self.templates['xgboost_standard'] = ExperimentConfig(
            name="xgboost_standard",
            description="XGBoostæ ‡å‡†è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="xgboost",
                hyperparameters={
                    'n_estimators': 300,
                    'max_depth': 7,
                    'learning_rate': 0.07,
                    'subsample': 0.8,
                    'colsample_bytree': 0.8
                }
            ),
            training=TrainingConfig(n_folds=10)
        )
        
        # XGBoostå®Œæ•´æ¨¡æ¿
        self.templates['xgboost_full'] = ExperimentConfig(
            name="xgboost_full",
            description="XGBoostå®Œæ•´è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="xgboost",
                hyperparameters={
                    'n_estimators': 500,
                    'max_depth': 8,
                    'learning_rate': 0.05,
                    'subsample': 0.8,
                    'colsample_bytree': 0.8
                }
            ),
            training=TrainingConfig(
                n_folds=10,
                early_stopping=True,
                early_stopping_rounds=50
            ),
            logging=LoggingConfig(
                save_plots=True,
                generate_report=True,
                export_for_paper=True
            )
        )
        
        # LightGBMæ¨¡æ¿
        self.templates['lightgbm'] = ExperimentConfig(
            name="lightgbm",
            description="LightGBMè®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="lightgbm",
                hyperparameters={
                    'n_estimators': 200,
                    'num_leaves': 31,
                    'learning_rate': 0.1
                }
            )
        )
        
        # é›†æˆå­¦ä¹ æ¨¡æ¿
        self.templates['ensemble'] = ExperimentConfig(
            name="ensemble",
            description="é›†æˆå­¦ä¹ æ¨¡æ¿ï¼ˆå¤šæ¨¡å‹ï¼‰",
            model=ModelConfig(model_type="random_forest"),
            training=TrainingConfig(n_folds=10)
        )
        
        # è°ƒè¯•æ¨¡æ¿
        self.templates['debug'] = ExperimentConfig(
            name="debug",
            description="è°ƒè¯•æ¨¡æ¿ï¼ˆå°æ•°æ®é›†ï¼Œå¿«é€Ÿè®­ç»ƒï¼‰",
            model=ModelConfig(
                model_type="xgboost",
                hyperparameters={'n_estimators': 10, 'max_depth': 3}
            ),
            training=TrainingConfig(n_folds=2),
            logging=LoggingConfig(save_plots=False, generate_report=False)
        )
        

        # LightGBM - å¿«é€Ÿä¸å®Œæ•´æ¨¡æ¿
        self.templates['lightgbm_quick'] = ExperimentConfig(
            name="lightgbm_quick",
            description="LightGBMå¿«é€Ÿè®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="lightgbm",
                hyperparameters={
                    'n_estimators': 100,
                    'num_leaves': 31,
                    'learning_rate': 0.1,
                    'feature_fraction': 0.8,
                    'bagging_fraction': 0.8,
                    'bagging_freq': 5
                }
            ),
            training=TrainingConfig(n_folds=5)
        )

        self.templates['lightgbm_full'] = ExperimentConfig(
            name="lightgbm_full",
            description="LightGBMå®Œæ•´è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="lightgbm",
                hyperparameters={
                    'n_estimators': 300,
                    'num_leaves': 63,
                    'learning_rate': 0.05,
                    'feature_fraction': 0.8,
                    'bagging_fraction': 0.8,
                    'bagging_freq': 5
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # LightGBM - æ ‡å‡†æ¨¡æ¿
        self.templates['lightgbm_standard'] = ExperimentConfig(
            name="lightgbm_standard",
            description="LightGBMæ ‡å‡†è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="lightgbm",
                hyperparameters={
                    'n_estimators': 200,
                    'num_leaves': 47,
                    'learning_rate': 0.07,
                    'feature_fraction': 0.8,
                    'bagging_fraction': 0.8,
                    'bagging_freq': 5
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # LightGBM - å¤§å‹æ¨¡æ¿
        self.templates['lightgbm_large'] = ExperimentConfig(
            name="lightgbm_large",
            description="LightGBMå¤§å‹è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="lightgbm",
                hyperparameters={
                    'n_estimators': 500,
                    'num_leaves': 95,
                    'learning_rate': 0.03,
                    'feature_fraction': 0.8,
                    'bagging_fraction': 0.8,
                    'bagging_freq': 5
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # CatBoost - å¿«é€Ÿæ¨¡æ¿
        self.templates['catboost_quick'] = ExperimentConfig(
            name="catboost_quick",
            description="CatBoostå¿«é€Ÿè®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="catboost",
                hyperparameters={
                    'iterations': 200,
                    'depth': 6,
                    'learning_rate': 0.1,
                    'verbose': False
                }
            ),
            training=TrainingConfig(n_folds=5)
        )

        # CatBoost - æ ‡å‡†æ¨¡æ¿
        self.templates['catboost_standard'] = ExperimentConfig(
            name="catboost_standard",
            description="CatBoostæ ‡å‡†è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="catboost",
                hyperparameters={
                    'iterations': 500,
                    'depth': 8,
                    'learning_rate': 0.07,
                    'verbose': False
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # CatBoost - å¤§å‹æ¨¡æ¿
        self.templates['catboost_large'] = ExperimentConfig(
            name="catboost_large",
            description="CatBoostå¤§å‹è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="catboost",
                hyperparameters={
                    'iterations': 1000,
                    'depth': 10,
                    'learning_rate': 0.03,
                    'verbose': False
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # éšæœºæ£®æ— - å¿«é€Ÿæ¨¡æ¿
        self.templates['random_forest_fast'] = ExperimentConfig(
            name="random_forest_fast",
            description="éšæœºæ£®æ—å¿«é€Ÿè®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="random_forest",
                hyperparameters={
                    'n_estimators': 100,
                    'max_depth': 10,
                    'min_samples_split': 5,
                    'min_samples_leaf': 2
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # éšæœºæ£®æ— - æ ‡å‡†æ¨¡æ¿
        self.templates['random_forest_standard'] = ExperimentConfig(
            name="random_forest_standard",
            description="éšæœºæ£®æ—æ ‡å‡†è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="random_forest",
                hyperparameters={
                    'n_estimators': 300,
                    'max_depth': 20,
                    'min_samples_split': 2,
                    'min_samples_leaf': 1
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # éšæœºæ£®æ— - å¤§å‹æ¨¡æ¿
        self.templates['random_forest_large'] = ExperimentConfig(
            name="random_forest_large",
            description="éšæœºæ£®æ—å¤§å‹è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="random_forest",
                hyperparameters={
                    'n_estimators': 500,
                    'max_depth': None,
                    'min_samples_split': 2,
                    'min_samples_leaf': 1
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # ä¿ç•™åŸå§‹æ¨¡æ¿ä»¥ä¿æŒå…¼å®¹æ€§
        self.templates['random_forest'] = self.templates['random_forest_standard']

        # æ¢¯åº¦æå‡æ ‘ - å¿«é€Ÿæ¨¡æ¿
        self.templates['gradient_boosting_fast'] = ExperimentConfig(
            name="gradient_boosting_fast",
            description="Gradient Boostingå¿«é€Ÿè®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="gradient_boosting",
                hyperparameters={
                    'n_estimators': 100,
                    'learning_rate': 0.1,
                    'max_depth': 3
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # æ¢¯åº¦æå‡æ ‘ - æ ‡å‡†æ¨¡æ¿
        self.templates['gradient_boosting_standard'] = ExperimentConfig(
            name="gradient_boosting_standard",
            description="Gradient Boostingæ ‡å‡†è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="gradient_boosting",
                hyperparameters={
                    'n_estimators': 300,
                    'learning_rate': 0.07,
                    'max_depth': 5
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # æ¢¯åº¦æå‡æ ‘ - å¤§å‹æ¨¡æ¿
        self.templates['gradient_boosting_large'] = ExperimentConfig(
            name="gradient_boosting_large",
            description="Gradient Boostingå¤§å‹è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="gradient_boosting",
                hyperparameters={
                    'n_estimators': 500,
                    'learning_rate': 0.03,
                    'max_depth': 7
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # ä¿ç•™åŸå§‹æ¨¡æ¿ä»¥ä¿æŒå…¼å®¹æ€§
        self.templates['gradient_boosting'] = self.templates['gradient_boosting_standard']

        # AdaBoost - å¿«é€Ÿæ¨¡æ¿
        self.templates['adaboost_fast'] = ExperimentConfig(
            name="adaboost_fast",
            description="AdaBoostå¿«é€Ÿè®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="adaboost",
                hyperparameters={
                    'n_estimators': 50,
                    'learning_rate': 1.0
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # AdaBoost - æ ‡å‡†æ¨¡æ¿
        self.templates['adaboost_standard'] = ExperimentConfig(
            name="adaboost_standard",
            description="AdaBoostæ ‡å‡†è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="adaboost",
                hyperparameters={
                    'n_estimators': 200,
                    'learning_rate': 0.5
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # AdaBoost - å¤§å‹æ¨¡æ¿
        self.templates['adaboost_large'] = ExperimentConfig(
            name="adaboost_large",
            description="AdaBoostå¤§å‹è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="adaboost",
                hyperparameters={
                    'n_estimators': 500,
                    'learning_rate': 0.1
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # ä¿ç•™åŸå§‹æ¨¡æ¿ä»¥ä¿æŒå…¼å®¹æ€§
        self.templates['adaboost'] = self.templates['adaboost_standard']

        # Extra Trees - å¿«é€Ÿæ¨¡æ¿
        self.templates['extra_trees_fast'] = ExperimentConfig(
            name="extra_trees_fast",
            description="Extra Treeså¿«é€Ÿè®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="extra_trees",
                hyperparameters={
                    'n_estimators': 100,
                    'max_depth': 10,
                    'min_samples_split': 5,
                    'min_samples_leaf': 2
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # Extra Trees - æ ‡å‡†æ¨¡æ¿
        self.templates['extra_trees_standard'] = ExperimentConfig(
            name="extra_trees_standard",
            description="Extra Treesæ ‡å‡†è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="extra_trees",
                hyperparameters={
                    'n_estimators': 300,
                    'max_depth': 20,
                    'min_samples_split': 2,
                    'min_samples_leaf': 1
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # Extra Trees - å¤§å‹æ¨¡æ¿
        self.templates['extra_trees_large'] = ExperimentConfig(
            name="extra_trees_large",
            description="Extra Treeså¤§å‹è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="extra_trees",
                hyperparameters={
                    'n_estimators': 500,
                    'max_depth': None,
                    'min_samples_split': 2,
                    'min_samples_leaf': 1
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # ä¿ç•™åŸå§‹æ¨¡æ¿ä»¥ä¿æŒå…¼å®¹æ€§
        self.templates['extra_trees'] = self.templates['extra_trees_standard']

        # SVR - å¿«é€Ÿæ¨¡æ¿
        self.templates['svr_fast'] = ExperimentConfig(
            name="svr_fast",
            description="SVRå¿«é€Ÿè®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="svr",
                hyperparameters={
                    'kernel': 'rbf',
                    'C': 1.0,
                    'epsilon': 0.1
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # SVR - æ ‡å‡†æ¨¡æ¿
        self.templates['svr_standard'] = ExperimentConfig(
            name="svr_standard",
            description="SVRæ ‡å‡†è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="svr",
                hyperparameters={
                    'kernel': 'rbf',
                    'C': 10.0,
                    'epsilon': 0.1
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # SVR - å¤§å‹æ¨¡æ¿
        self.templates['svr_large'] = ExperimentConfig(
            name="svr_large",
            description="SVRå¤§å‹è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="svr",
                hyperparameters={
                    'kernel': 'rbf',
                    'C': 100.0,
                    'epsilon': 0.01
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # ä¿ç•™åŸå§‹æ¨¡æ¿ä»¥ä¿æŒå…¼å®¹æ€§ (é‡å‘½åä¸ºsvr)
        self.templates['svr'] = self.templates['svr_standard']
        self.templates['svr_rbf'] = self.templates['svr_standard']

        # KNN - å¿«é€Ÿæ¨¡æ¿
        self.templates['knn_fast'] = ExperimentConfig(
            name="knn_fast",
            description="KNNå¿«é€Ÿè®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="knn",
                hyperparameters={
                    'n_neighbors': 5,
                    'weights': 'uniform'
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # KNN - æ ‡å‡†æ¨¡æ¿
        self.templates['knn_standard'] = ExperimentConfig(
            name="knn_standard",
            description="KNNæ ‡å‡†è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="knn",
                hyperparameters={
                    'n_neighbors': 7,
                    'weights': 'distance'
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # KNN - å¤§å‹æ¨¡æ¿
        self.templates['knn_large'] = ExperimentConfig(
            name="knn_large",
            description="KNNå¤§å‹è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="knn",
                hyperparameters={
                    'n_neighbors': 10,
                    'weights': 'distance'
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # ä¿ç•™åŸå§‹æ¨¡æ¿ä»¥ä¿æŒå…¼å®¹æ€§
        self.templates['knn'] = self.templates['knn_standard']

        # å†³ç­–æ ‘ - å¿«é€Ÿæ¨¡æ¿
        self.templates['decision_tree_fast'] = ExperimentConfig(
            name="decision_tree_fast",
            description="å†³ç­–æ ‘å¿«é€Ÿè®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="decision_tree",
                hyperparameters={
                    'max_depth': 5,
                    'min_samples_split': 10,
                    'min_samples_leaf': 5
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # å†³ç­–æ ‘ - æ ‡å‡†æ¨¡æ¿
        self.templates['decision_tree_standard'] = ExperimentConfig(
            name="decision_tree_standard",
            description="å†³ç­–æ ‘æ ‡å‡†è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="decision_tree",
                hyperparameters={
                    'max_depth': 10,
                    'min_samples_split': 5,
                    'min_samples_leaf': 2
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # å†³ç­–æ ‘ - å¤§å‹æ¨¡æ¿
        self.templates['decision_tree_large'] = ExperimentConfig(
            name="decision_tree_large",
            description="å†³ç­–æ ‘å¤§å‹è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="decision_tree",
                hyperparameters={
                    'max_depth': None,
                    'min_samples_split': 2,
                    'min_samples_leaf': 1
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # ä¿ç•™åŸå§‹æ¨¡æ¿ä»¥ä¿æŒå…¼å®¹æ€§
        self.templates['decision_tree'] = self.templates['decision_tree_standard']

        # Ridge - å¿«é€Ÿæ¨¡æ¿
        self.templates['ridge_fast'] = ExperimentConfig(
            name="ridge_fast",
            description="Ridgeå¿«é€Ÿè®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="ridge",
                hyperparameters={'alpha': 1.0}
            ),
            training=TrainingConfig(n_folds=10)
        )

        # Ridge - æ ‡å‡†æ¨¡æ¿
        self.templates['ridge_standard'] = ExperimentConfig(
            name="ridge_standard",
            description="Ridgeæ ‡å‡†è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="ridge",
                hyperparameters={'alpha': 0.5}
            ),
            training=TrainingConfig(n_folds=10)
        )

        # Ridge - å¤§å‹æ¨¡æ¿
        self.templates['ridge_large'] = ExperimentConfig(
            name="ridge_large",
            description="Ridgeå¤§å‹è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="ridge",
                hyperparameters={'alpha': 0.1}
            ),
            training=TrainingConfig(n_folds=10)
        )

        # ä¿ç•™åŸå§‹æ¨¡æ¿ä»¥ä¿æŒå…¼å®¹æ€§
        self.templates['ridge'] = self.templates['ridge_standard']

        # Lasso - å¿«é€Ÿæ¨¡æ¿
        self.templates['lasso_fast'] = ExperimentConfig(
            name="lasso_fast",
            description="Lassoå¿«é€Ÿè®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="lasso",
                hyperparameters={'alpha': 0.1}
            ),
            training=TrainingConfig(n_folds=10)
        )

        # Lasso - æ ‡å‡†æ¨¡æ¿
        self.templates['lasso_standard'] = ExperimentConfig(
            name="lasso_standard",
            description="Lassoæ ‡å‡†è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="lasso",
                hyperparameters={'alpha': 0.05}
            ),
            training=TrainingConfig(n_folds=10)
        )

        # Lasso - å¤§å‹æ¨¡æ¿
        self.templates['lasso_large'] = ExperimentConfig(
            name="lasso_large",
            description="Lassoå¤§å‹è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="lasso",
                hyperparameters={'alpha': 0.01}
            ),
            training=TrainingConfig(n_folds=10)
        )

        # ä¿ç•™åŸå§‹æ¨¡æ¿ä»¥ä¿æŒå…¼å®¹æ€§
        self.templates['lasso'] = self.templates['lasso_standard']

        # ElasticNet - å¿«é€Ÿæ¨¡æ¿
        self.templates['elasticnet_fast'] = ExperimentConfig(
            name="elasticnet_fast",
            description="ElasticNetå¿«é€Ÿè®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="elastic_net",
                hyperparameters={
                    'alpha': 0.5,
                    'l1_ratio': 0.5
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # ElasticNet - æ ‡å‡†æ¨¡æ¿
        self.templates['elasticnet_standard'] = ExperimentConfig(
            name="elasticnet_standard",
            description="ElasticNetæ ‡å‡†è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="elastic_net",
                hyperparameters={
                    'alpha': 0.1,
                    'l1_ratio': 0.7
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # ElasticNet - å¤§å‹æ¨¡æ¿
        self.templates['elasticnet_large'] = ExperimentConfig(
            name="elasticnet_large",
            description="ElasticNetå¤§å‹è®­ç»ƒæ¨¡æ¿",
            model=ModelConfig(
                model_type="elastic_net",
                hyperparameters={
                    'alpha': 0.01,
                    'l1_ratio': 0.5
                }
            ),
            training=TrainingConfig(n_folds=10)
        )

        # ä¿ç•™åŸå§‹æ¨¡æ¿ä»¥ä¿æŒå…¼å®¹æ€§
        self.templates['elastic_net'] = self.templates['elasticnet_standard']
    
    def get_template(self, template_name: str) -> ExperimentConfig:
        """
        è·å–æ¨¡æ¿é…ç½®
        
        Args:
            template_name: æ¨¡æ¿åç§°
        
        Returns:
            é…ç½®å¯¹è±¡
        """
        if template_name not in self.templates:
            raise ValueError(f"æ¨¡æ¿ä¸å­˜åœ¨: {template_name}. å¯ç”¨æ¨¡æ¿: {list(self.templates.keys())}")
        return self.templates[template_name].copy()
    
    def list_templates(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡æ¿"""
        return list(self.templates.keys())
    
    def save_config(self, config: ExperimentConfig, filename: str, format: str = "yaml"):
        """
        ä¿å­˜é…ç½®æ–‡ä»¶
        
        Args:
            config: é…ç½®å¯¹è±¡
            filename: æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            format: æ ¼å¼ (yaml/json)
        """
        if format == "yaml":
            path = self.config_dir / f"{filename}.yaml"
            config.to_yaml(str(path))
        elif format == "json":
            path = self.config_dir / f"{filename}.json"
            config.to_json(str(path))
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format}")
        
        print(f"é…ç½®å·²ä¿å­˜: {path}")
        return path
    
    def load_config(self, filename: str) -> ExperimentConfig:
        """
        åŠ è½½é…ç½®æ–‡ä»¶
        
        Args:
            filename: æ–‡ä»¶åæˆ–è·¯å¾„
        
        Returns:
            é…ç½®å¯¹è±¡
        """
        # å°è¯•ä¸åŒçš„è·¯å¾„å’Œæ ¼å¼
        paths_to_try = [
            Path(filename),
            self.config_dir / filename,
            self.config_dir / f"{filename}.yaml",
            self.config_dir / f"{filename}.json"
        ]
        
        for path in paths_to_try:
            if path.exists():
                if path.suffix == '.yaml' or path.suffix == '.yml':
                    return ExperimentConfig.from_yaml(str(path))
                elif path.suffix == '.json':
                    return ExperimentConfig.from_json(str(path))
        
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
    
    def create_from_wizard(self) -> ExperimentConfig:
        """é€šè¿‡å‘å¯¼åˆ›å»ºé…ç½®"""
        print("\nğŸ”§ é…ç½®å‘å¯¼")
        print("=" * 50)
        
        # é€‰æ‹©æ¨¡æ¿
        print("\nå¯ç”¨æ¨¡æ¿:")
        for i, template in enumerate(self.templates.keys(), 1):
            desc = self.templates[template].description
            print(f"  {i}. {template}: {desc}")
        
        choice = input("\né€‰æ‹©æ¨¡æ¿ (è¾“å…¥ç¼–å·æˆ–åç§°ï¼Œç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤): ").strip()
        
        if choice.isdigit():
            template_name = list(self.templates.keys())[int(choice) - 1]
        elif choice in self.templates:
            template_name = choice
        else:
            template_name = 'xgboost_quick'
        
        config = self.get_template(template_name)
        
        # è‡ªå®šä¹‰é…ç½®
        name = input(f"å®éªŒåç§° [{config.name}]: ").strip() or config.name
        config.name = name
        
        description = input(f"å®éªŒæè¿° [{config.description}]: ").strip() or config.description
        config.description = description
        
        # æ¨¡å‹å‚æ•°
        n_folds = input(f"äº¤å‰éªŒè¯æŠ˜æ•° [{config.training.n_folds}]: ").strip()
        if n_folds.isdigit():
            config.training.n_folds = int(n_folds)
        
        # ç‰¹å¾ç±»å‹
        feature_type = input(f"ç‰¹å¾ç±»å‹ (morgan/descriptors/combined) [{config.feature.feature_type}]: ").strip()
        if feature_type in ["morgan", "descriptors", "combined"]:
            config.feature.feature_type = feature_type
        
        print("\nâœ… é…ç½®åˆ›å»ºå®Œæˆ!")
        return config


# ========================================
#           æ‰¹é‡å®éªŒé…ç½®
# ========================================

@dataclass
class BatchExperimentConfig:
    """æ‰¹é‡å®éªŒé…ç½®"""
    base_config: ExperimentConfig
    experiments: List[Dict[str, Any]] = field(default_factory=list)
    
    def add_experiment(self, name: str, updates: Dict):
        """
        æ·»åŠ å®éªŒ
        
        Args:
            name: å®éªŒåç§°
            updates: é…ç½®æ›´æ–°
        """
        self.experiments.append({
            'name': name,
            'updates': updates
        })
    
    def generate_configs(self) -> List[ExperimentConfig]:
        """ç”Ÿæˆæ‰€æœ‰å®éªŒé…ç½®"""
        configs = []
        for exp in self.experiments:
            config = self.base_config.copy()
            config.name = exp['name']
            config = config.update(exp['updates'])
            configs.append(config)
        return configs
    
    @classmethod
    def create_grid_search(cls, 
                          base_config: ExperimentConfig,
                          param_grid: Dict[str, List]) -> 'BatchExperimentConfig':
        """
        åˆ›å»ºç½‘æ ¼æœç´¢é…ç½®
        
        Args:
            base_config: åŸºç¡€é…ç½®
            param_grid: å‚æ•°ç½‘æ ¼
        
        Returns:
            æ‰¹é‡å®éªŒé…ç½®
        """
        batch = cls(base_config=base_config)
        
        # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
        import itertools
        
        keys = param_grid.keys()
        values = param_grid.values()
        
        for i, combination in enumerate(itertools.product(*values)):
            updates = dict(zip(keys, combination))
            name = f"{base_config.name}_grid_{i+1}"
            batch.add_experiment(name, updates)
        
        return batch


# ========================================
#           é…ç½®éªŒè¯å™¨
# ========================================

class ConfigValidator:
    """é…ç½®éªŒè¯å™¨"""
    
    @staticmethod
    def validate_file_exists(config: ExperimentConfig) -> bool:
        """éªŒè¯æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        data_path = Path(config.data.data_path)
        if not data_path.exists():
            print(f"âš ï¸ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            return False
        return True
    
    @staticmethod
    def validate_dependencies(config: ExperimentConfig) -> bool:
        """éªŒè¯ä¾èµ–æ˜¯å¦å®‰è£…"""
        base_packages = ['pandas', 'numpy', 'sklearn', 'matplotlib', 'seaborn']
        model_packages = {
            'xgboost': ['xgboost'],
            'lightgbm': ['lightgbm'],
            'catboost': ['catboost'],
            'random_forest': []
        }
        feature_requires_rdkit = getattr(config.feature, 'feature_type', None) in ['morgan', 'descriptors', 'combined']
        packages_to_check = list(base_packages)
        packages_to_check.extend(model_packages.get(config.model.model_type, []))
        if feature_requires_rdkit:
            packages_to_check.append('rdkit')
        missing = []
        for package in packages_to_check:
            try:
                __import__(package)
            except ImportError:
                missing.append(package)
        if missing:
            print(f"âš ï¸ ç¼ºå°‘ä¾èµ–åŒ…: {missing}")
            return False
        return True
    
    @staticmethod
    def validate_all(config: ExperimentConfig) -> bool:
        """æ‰§è¡Œæ‰€æœ‰éªŒè¯"""
        try:
            # é…ç½®å†…éƒ¨éªŒè¯
            config.validate()
            
            # æ–‡ä»¶éªŒè¯
            if not ConfigValidator.validate_file_exists(config):
                return False
            
            # ä¾èµ–éªŒè¯
            if not ConfigValidator.validate_dependencies(config):
                return False
            
            print("âœ… é…ç½®éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            print(f"âŒ é…ç½®éªŒè¯å¤±è´¥: {e}")
            return False


# ========================================
#           ä¾¿æ·å‡½æ•°
# ========================================

def create_default_config(model_type: str = "xgboost") -> ExperimentConfig:
    """åˆ›å»ºé»˜è®¤é…ç½®"""
    return ExperimentConfig(
        name=f"{model_type}_experiment",
        model=ModelConfig(model_type=model_type)
    )


def load_config(path: str) -> ExperimentConfig:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if path.endswith('.yaml') or path.endswith('.yml'):
        return ExperimentConfig.from_yaml(path)
    elif path.endswith('.json'):
        return ExperimentConfig.from_json(path)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„é…ç½®æ–‡ä»¶æ ¼å¼: {path}")


def save_config(config: ExperimentConfig, path: str):
    """ä¿å­˜é…ç½®æ–‡ä»¶"""
    if path.endswith('.yaml') or path.endswith('.yml'):
        config.to_yaml(path)
    elif path.endswith('.json'):
        config.to_json(path)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„é…ç½®æ–‡ä»¶æ ¼å¼: {path}")


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("é…ç½®ç³»ç»Ÿæµ‹è¯•")
    print("=" * 50)
    
    # åˆ›å»ºé…ç½®ç®¡ç†å™¨
    manager = ConfigManager()
    
    # åˆ—å‡ºæ¨¡æ¿
    print("\nå¯ç”¨æ¨¡æ¿:")
    for template in manager.list_templates():
        print(f"  - {template}")
    
    # è·å–æ¨¡æ¿
    config = manager.get_template('xgboost_full')
    
    # ä¿å­˜é…ç½®
    manager.save_config(config, "test_config", "yaml")
    
    # åŠ è½½é…ç½®
    loaded_config = manager.load_config("test_config")
    
    # éªŒè¯é…ç½®
    ConfigValidator.validate_all(loaded_config)
    
    print("\nâœ… é…ç½®ç³»ç»Ÿæµ‹è¯•å®Œæˆ")
